{"pages":[{"title":"","text":"google-site-verification: google39024f504cd1475b.html","link":"/google39024f504cd1475b.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"一步步透視 GBDT Regression Tree","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x) $ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 結論說完了，想看數學原理請移駕 背後的數學，想看例子從無到有生出 GBDT 的請到 Algorithm - step by step ，想離開得請按上一頁。 GBDT 簡介GBDT-regression tree 簡單來說，訓練時依序建立 trees $\\{ f_1(x), f_2(x), …. , f_M(x)\\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。 所以 GBDT - regression tree 的訓練是 sequentially ，無法以並行訓練加速。 我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下: F_m(x) = F_0(x) + \\nu\\sum^m_{i=1}f_i(x) = F_{m-1}(x) + \\nu f_m(x) $\\nu$ 為 learning rate GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。 差多遠即 residual 的概念： \\textit{residual = observed - predicted}因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual GBDT Algorithm - step by stepAlgorithm 參考了 statQuest 對 GBDT 的講解，連結放在 reference，必看！ GBDT-regression tree 擬合 algorithm： Input Data and Loss Function input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data 接下來都用這組簡單的數據，其中 Target $y_i$ : weight which is what we want to predict $x_i$: features 的組成有 身高，喜歡的顏色，性別 目標是用 $x_i$ 的 height, favorite color, gender 來預測 $y_i$ 的 weight loss function 為 square error L(y_i, F(x)) = \\cfrac{1}{2}(\\textit{observed - predicted}) ^2 = \\cfrac{1}{2}(y_i^2 - F(x))^2 square error commonly use in Regression with Gradient Boost $\\textit{observed - predicted}$ is called $residual$ $y_i$ are observed value $F(x)$: the function which give us the predicted value 也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出 F_m(x) = F_{m-1}(x) + \\nu f_m(x)F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x) Step 1 Initialize Model with a Constant Value初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可 取 weight 平均得到 $F_0(x) = 71.2 = \\textit{average weight}$ Step 2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At epoch m=1 (A) Calculate Residuals of $F_{0}(x)$epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals ​ residuals = \\textit{observed weight - predicted weight} 而 $F_0(x) = \\textit{average weight = 71.17}$ 計算 residual 後: epoch_0_prediction 表 $F_0(x)$ 輸出 epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$ (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual 用 columns $\\textit{height, favorite, color, gender}$ 預測 $residuals$ 來建新樹 $f_1(x)$ 建樹的過程為一般的 regression tree building 過程，target 就是 residuals。 假設我們找到分支結構是 綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$ 特定的 leaf node 下。 epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值 直覺地對每個 leaf node 內的 data sample $x$ weight 值取平均，得到輸出值 epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值 (D) Update $F_1(x)$ with New Tree $f_1(x)$此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$ 現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \\textit{learning rate } \\times f_1(x)$ 假設 $\\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下 epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值 At epoch m=2(A) Calculate Residuals of $F_{1}(x)$m = 2 新的 residual between observed weight and $F_1(x)$如下 epoch_1_prediction 為 $F_1(x)$ 的輸出 epoch_2_residual 為 observed weight 與 predicted weight $F_1(x)$ 的 residual (B) Construct New Regression Tree $f_2(x)$建一顆新樹擬合 epoch 2 (A) 得出的 residual epoch_2_residual 為 $f_2(x)$ 要擬合的 target 假設 $f_2(x)$ 擬合後樹結構長這樣 epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_2(x)$決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個 leaf node 下的 data sample $x$ 取平均 epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是 residual epoch_2_residual (D) Update $F_2(x)$ with New Tree $f_2(x)$到目前為止我們建立了兩顆 $tree$ $f_1(x), f_2(x)$，假設 $\\textit{learning rate = 0.1}$，則 $F_2(x)$ 為 F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))每個 data sample 在 $F_2(x)$ 的 predict 值如下圖： weight: out target value epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值 epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual epoch_2_prediction: $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出 Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ Step 3 輸出模型 $F_M(x)$，把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$ 與上一步 predicted value $\\hat{y}$ $F_{m-1}(x)$ 的 $residual$ GBDT Regression 背後的數學Q: Loss function 為什麼用 mean square error ?​ 選擇 $\\cfrac{1}{2}(\\textit{observed - predicted}) ^2$ 當作 loss function 的好處是對 $F(X)$ 微分的形式簡潔 \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\\textit{negative residual}$ ​ $-(y_i - F(X)) = \\textit{-(observed - predicted) = negative residual } $ 而我們知道 $F(X)$ 在 loss function $L(y_i, F(X))$ 的 gradient 就是 $\\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$ Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？問題來自 step 1 我們要找的是能使 cost function $\\sum^n_{i=1}L(y_i, \\gamma)$ 最小的那個輸出值 $\\gamma$ 做為 $F_0(X)$。 $F_0(x) = argmin_r \\sum^n_{i=1} L(y_i,\\gamma)$ $F_0(x)$ 初始化的 function，其值是常數 $\\gamma$ refers to the predicted values $n$ 是 data sample 數 Proof: 已知 ​ \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X)) 所以 cost function 對 $\\gamma $ 微分後，是所有 sample data 跟 $\\gamma$ 的 negative residual 和 為 0 ​ \\sum^n_{i=1}L(y_i, \\gamma) = -(y_1 - \\gamma) - (y_2 - \\gamma)- ... -(y_n - \\gamma) = 0 移項得到 $\\gamma$ ​ \\gamma = \\cfrac{y_1 + y_2 + .... + y_n}{n} 正是所有 target value 的 mean，故得證 ​ F_0(x) = \\gamma = \\textit{the average of targets value} Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？問題來自 step 2A 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum \\cfrac{1}{2}(y_i^2 - F(x))^2我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x)\\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} \\ L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - F_{m-1}(x))) \\\\ &= -(-(observed - predicted )) \\\\ &= - \\textit{negative residual} \\\\ & = residual \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual ​ $f_m(x)$ = $\\textit{gradient decent}$ = $\\textit{negative gradient}$ = $residual$ GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\\textit{gradient decent}$ 的方向得到新的 $F(x)$ 這也正是為什麼叫做 gradient boost 。 by the way，step 2-(A) compute residuals: r_{im} = -[\\cfrac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \\ \\textit{for i = 1,....,n} $i$: sample number $m$: the tree we are trying to build Q：個別 leaf node 的輸出為什麼是取平均 ？問題來自 step 2C 在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\\gamma$ 使 cost function 最小 $\\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma)$ $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合 $F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$ 上的輸出值為 $\\gamma_{j,m}$ \\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2直接對 $\\gamma$ 微分 \\begin{aligned}\\cfrac{d}{d \\gamma} \\ \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2 = \\ \\sum_{x_i \\in R_{j,m}}-(y_i - F_{m-1}(x_i) - \\gamma) = 0\\end{aligned}移項 \\gamma = \\cfrac{1}{n_{jm}} \\sum_{x_i \\in R_{j,m}}y_i - F_{m-1}(x_i) $n_{jm}$ is the number of data sample in leaf node $j$ at step $m$ 白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均 事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $ 都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。 Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？問題來自 step 2D \\begin{aligned}F_m(x) & = F_{m-1}(x) + \\nu f_m(x) \\\\ &= F_{m-1}(x) + \\nu \\sum^{J_m}_{j=1}\\gamma_{jm}I(x \\in R_{jm})\\end{aligned} $F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x)$ $\\nu$ learning rate $J_m$ m 步 的 leaf node 總數 $\\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出 $F_m(x)$ 展開來就是 F_m(x) = F_0(x) + \\nu(f_1(x) + f_2(x)+ ...+f_m(t))寫在最後 seeing is believing 太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙 人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶 learning by doing it 所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的 1234567891011import pandas as pddata = [ (1.6, 'Blue', 'Male', 88), (1.6, 'Greem', 'Female', 76), (1.5, 'Blue', 'Female', 56), (1.8, 'Red', 'Male', 73), (1.5, 'Green', 'Male', 77), (1.4, 'Blue', 'Female', 57) ]columns = ['height', 'favorite_color', 'gender', 'weight']df = pd.DataFrame.from_records(data, index=None, columns=columns) always get your hands dirty ReferenceMain Gradient Boost Part 1 (of 4): Regression Main Ideas Gradient Boost Part 2 (of 4): Regression Details ccd comment: 上面兩個必看 A Step by Step Gradient Boosting Decision Tree Example https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/ 真 step by step Gradient Boosting Decision Tree Algorithm Explained https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4 including sklearn 實作 Other Gradient Boosting Decision Tree http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法 提升树算法理论详解 https://hexinlin.top/2020/03/01/GBDT/ 梯度提升树(GBDT)原理小结 https://www.cnblogs.com/pinard/p/6140514.html","link":"/GBDT-Rregression-Tree-Step-by-Step/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/ckkb4qj5300030ce221fmb5r8/"},{"title":"Hierarchical Softmax 背後的數學","text":"以 CBOW 為例 $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ $\\text{v}’_j$ 表 $W’$ 中 j-th columns vector 在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是 leaf node ，跟 column vector $\\text{v}’_j$ 與 word $w_i$ 不是直接對應的關係 Hierarchical Softmax 優化了原始 softmax 分母 normalization 項需要對整個 vocabulary set 內的所有 word 內積，原始 softmax 如下 p(w | c) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_w})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$，is average of input context words’ vector representation in $W$ Hierarchical Softmax build a full binary tree to avoid computation over all vocabulary，示意圖： 每個 leaf node 代表一個 word $w_i$ Matrix $W^{‘}$ 就是所有 non-leaf node $n$ 代表的 vector $\\text{v}^{‘}_n$ 集合，與 word $w_i$ 無一對一對應關係 Binary tree 中每個 node 分岔的 probability 是個 binary classification problem $p(n, \\text{left}) = \\sigma({\\text{v}’_n}^{\\top} h)$ $p(n, \\text{righ}) = 1 - p(\\text{left},n) = \\sigma(-{\\text{v}’_n}^{\\top} h)$ $\\text{v}^{‘}_{n}$ 代表 node $n$ 的 vector 則每個 word $w_O = w_i$ 的 generate probability $p(w_i)$ 為其從 root node 分岔到 lead node 的 probability $p(w_i = w_O) = \\prod^{L(w_O)-1}_{j=1} \\sigma(\\mathbb{I}_{\\text{turn}}(n(w_O, j), n(w_O, j + 1) \\cdot {v^{‘}_{n(w_O, j)}}^{\\top}h)$ $w_O$ 表 output word 的意思 $L(w_O)$ is the depth of the path leading to the output word $w_O$ $\\mathbb{I}_{turn}$ is a specially indicator function 1 if $n(w_O, k+1)$ is the left child of $n(w_O, k)$ -1 if $n(w_O, k+1)$ is the right child of $n(w_O, k)$ $n(w, j)$ means the $j$ th unit on the path from root to the word $w$ $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$ average of all context word vector 簡單的例子Ex 1: Following the path from the root to leaf node of $w_2$, we can compute the probability of $w_2$ $p(w_2)$: Ex 2: $\\sum^{V}_{i=1} p(w_i = w_O) = 1$ probability $p(\\text{cat}| context)$, 是由 $ node1 \\stackrel{\\text{left}}{\\to} node \\stackrel{\\text{right}}{\\to} node 5 \\stackrel{\\text{right}}{\\to} cat $ 這條路徑組成 其中 context words 經過 hidden layer 後的輸出為 $h(\\text{context words})$ 為什麼 Hierarchical Softmax 可以減少 Time Complexity?透過 Hierarchical Softmax ， 原本計算 $p(w|c)$ 需要求所有 word $w_i$ 的 vector $\\text{v}_{w_i}$對 $h$ 的內積，現在只需要計算路徑上的節點數 $L(w_i) -1$，又 HS 是 full binary tree ，其最大深度為 $\\log_2|V|$ So we only need to evaluate at most $log_2|V|$ Hierarchical Softmax 如何 update 參數Error Funtion of Hierarchical SoftmaxError function $E$ is negative log likelihood $L(w_i) -1$ 表 從 root node 到 leaf node of $w_i$ 的 node number $[ \\cdot ]$表分岔判斷 $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$ average of all context word vector And we use gradient decent to update $\\text{v}^{‘}_j$ and $h$ in $W’$ and $W’$ Calculate the Derivate $E$ with Regard to $\\text{v}^{‘\\top}_jh$先求 total loss 對 $\\text{v}^{‘\\top}_jh$ 的 gradient $\\sigma^{‘}(x) = \\sigma(x)[1 - \\sigma(x)]$ $[\\log\\sigma(x)]^{‘} = 1 - \\sigma(x)$ ⇒ $[log(1 - \\sigma(x)]^{‘} = -\\sigma(x)$ Calculate the Derivate $E$ with Regard to $\\text{v}^{‘}_j$根據 chain rule 可以求出 total loss 對 huffman tree node vector $\\text{v}^{‘}_j$ 的 gradient Update Equation Calculate the Derivate $E$ with Regard to $h$ 最後求 total loss 對 hidden layer outpot $h$ 的 gradient $EH$: an N-dim vector, is the sum of the output vector of all words in the vocabulary, weighted by their prediction error Update EquationBecause hidden vector $h$ is composed with all the context word $w_{I,c}$ $\\text{v}_{w_{I,c}}$ is the input vector of c-th word in input context ImplementCBOW + HS 實現 [todo] Reference Language Models, Word2Vec, and Efficient Softmax Approximations https://rohanvarma.me/Word2Vec/ Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722","link":"/hierarchical-softmax-in-word2vec/"},{"title":"NLP Language Model","text":"General Form p(w_1 , \\cdots , w_T) = \\prod\\limits_i p(w_i \\: | \\: w_{i-1} , \\cdots , w_1)展開來後 $P(w_1, w_2, w_3,…,w_T) = P(w_1)P(x_2|w_1)P(w_3|w_2, w_1)…P(w_T|w_1,…w_{T-1})$ EX Ngram Model根據 Markov assumption， word $i$ 只跟其包含 $i$ 的連續 $n$ 個 word 有關, 即前面 $n -1$ 個 word p(w_1 , \\cdots , w_T) = \\prod\\limits_i p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-n+1})其中 conditional probability 統計 corpus 內的 word frequency，可以看到 ngram 只跟前面 $n-1$ 個 word 有關 p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-n+1}) = \\dfrac{count(w_{i-n+1}, \\cdots , w_{i-1},w_o)}{count({w_{i-n+1}, \\cdots , w_{i-1}})}如果 $k=2$ 則稱為 bigram model : p(w_i|w_1, w_2, ... w_{i-1}) \\approx p(w_i|w_{i-1})最簡單的實作直接統計 corpus 內的 word frequency 得到 conditional probability: 但通常泛化性不足，所以用 neural network 與 softmax 來泛化 n gram conditional probability $p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-i+1})$ Neural Network ImplementationIn neural network, we achieve the same objective using the softmax layer $p(w_t \\: | \\: w_{t-1} , \\cdots , w_{t-n+1}) = \\dfrac{\\text{exp}({h^\\top v’_{w_t}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top v’_{w_i}})}$ $h$ is the output vector of the penultimate network layer $v^{‘}_{w}$ is the output embedding of word $w$ the inner product $h^\\top v’_{w_t}$ computes the unnormalized log probability of word $w_t$ the denominator normalizes log probability by sum of the log-probabilities of all word in $V$ Implement Ngram model with PytorchCreating Corpus and Training Pairs1234567891011121314151617181920212223import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimtest_sentence = &quot;&quot;&quot;When forty winters shall besiege thy brow,And dig deep trenches in thy beauty's field,Thy youth's proud livery so gazed on now,Will be a totter'd weed of small worth held:Then being asked, where all thy beauty lies,Where all the treasure of thy lusty days;To say, within thine own deep sunken eyes,Were an all-eating shame, and thriftless praise.How much more praise deserv'd thy beauty's use,If thou couldst answer 'This fair child of mineShall sum my count, and make my old excuse,'Proving his beauty by succession thine!This were to be new made when thou art old,And see thy blood warm when thou feel'st it cold.&quot;&quot;&quot;.split()trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)]vocab = set(test_sentence)word_to_idx = {word: i for i, word in enumerate(vocab)} Define N Gram Model123456789101112131415class NGramLanguageModel(nn.Module): def __init__(self, vocab_size, embedding_dim, context_size): super(NGramLanguageModel, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(context_size * embedding_dim, 128) self.linear2 = nn.Linear(128, vocab_size) def forward(self, inputs): embeds = self.embeddings(inputs).view(1, -1) out = F.relu(self.linear1(embeds)) out = self.linear2(out) log_probs = F.log_softmax(out, dim=1) return log_probs Training1234567891011121314151617181920CONTEXT_SIZE = 2EMBEDDING_DIM = 10loss_function = nn.NLLLoss()net = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)optimizer = optim.SGD(net.parameters(), lr=0.001)losses = []for epoch in range(10): total_loss = 0 for context, target in trigrams: context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long) net.zero_grad() log_probs = net(context_idxs) loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long)) loss.backward() optimizer.step() total_loss += loss.data print(&quot;epcoh {} loss {}&quot;.format(epoch, total_loss)) losses.append(total_loss) Fetch Embedding1emb = net.embeddings(torch.tensor([i for i in range(len(vocab))])).detach().numpy() Reference on word embeddings https://ruder.io/word-embeddings-1/ https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html","link":"/NLP-language-model/"},{"title":"word2Vec 從原理到實現","text":"這篇是在 notion 整理的筆記大綱，只提供綱要性的說明 預備知識 language model： NLP 語言模型 參閱 NLP Language Model huffman tree 簡介兩種網路結構 Continuous bag of words (CBOW) &amp; Softmax CBOW feeds $n$ words around the target word $w_t$ at each step P(center|context;\\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ CBOW 的 Objective Function$J_\\theta = \\frac{1}{V}\\sum\\limits_{t=1}^V\\ \\text{log} \\space p(w_t \\: | \\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n})$ 其中 p(w_t \\: | \\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n}) = \\cfrac{\\exp(h^\\top \\text{v}^{'}_{w_{t}})}{\\sum_{w_i \\in V}\\exp(h^\\top \\text{v}'_{w_i})} $n$ 表 window size $w_t$ 表 CBOW target center word $w_i$ 表 word $i$ in vocabulary $V$ $\\text{v}_{w_i}$ 表 matrix $W$ 中，代表 $w_i$ 的那個 row vector $\\text{v}’_{w_i}$ 表 matrix $W’$ 中，代表 $w_i$ 的那個 column vector $h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\\cfrac{1}{C}(\\text{v}_{w_1} + \\text{v}_{w_2}+ …+ \\text{v}_{w_C})^T$ Because there are multiple contextual words, we construct the input vector by averaging each words’s distribution representation Skipgram &amp; Softmax skipgram uses the centre word to predict the surrounding words instead of using the surrounding words to predict the centre word P(context|center; \\theta) Skipgram 的 Objective Function J_\\theta = \\frac{1}{V}\\sum\\limits_{t=1}^V\\ \\sum\\limits_{-n \\leq j \\leq n , \\neq 0} \\text{log} \\space p(w_{t+j} \\: | \\: w_t)其中 p(w_{t+j} \\: | \\: w_t ) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_{w_{t+j}}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} $n$ 為 window size $w_{t+j}$ 表 skipgram target 第 j 個 context word $w_t$ 為 skipgram input 的 center word skip-gram 有兩個獨立的 embedding matrix $W$ and $W^{‘}$ $W$ : $V \\times N$ , $V$ is vocabulary size; N is vector dimension output matrix $W^{‘}$: $N \\times V$, encoding the meaning of context $\\text{v}^{‘}_{w_i}$ is column vector of word $w_i$ in $Ｗ^{‘}$ $h$ is the hidden layer’s output 事實上，skip-gram 沒有 hidden layer, 因爲 input 只有一個 word, $h$ 就是 word embedding $\\text{v}_{w_t}$of the word $w_t$ in $W$。 所以 $p(w_{t+j} \\: | \\: w_t ) = \\dfrac{\\text{exp}({\\text{v}^\\top_{w_t} \\text{v}’_{w_{t+j}}})}{\\sum_{w_i \\in V} \\text{exp}({\\text{v}^\\top_{w_t} \\text{v}’_{w_i}})}$ 兩種 loss function 優化原始 softmax 作為 objective function， 分母必須對所有 word $w_i$ in vocabulary 與 vector $ h$ 內積，造成運算瓶頸 p(w_O | w_I) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_{w_{O}}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})}所以 word2Vec 論文中採用兩種 objective function 的優化方案，Hierarchical Softmax 與 Negatvie Sampling Hierarchical Softmax原理推導請參閱 Hierarchical Softmax 背後的數學 Hierarchical softmax build a full binary tree to avoid computation over all vocabulary Negative Sampling原理推導請參閱 Negative Sampling 背後的數學 negative sampling did further simplification because it focuses on learning high-quality word embedding rather than modeling the likelihood of correct word in natural language. In implement，negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them 實現 WordVec skip gram + softmax [todo] CBOW + softmax [todo] CBOW + hierarchical softmax [todo] CBOW + negatove sampling skip gram + hierarchical softmax skip gram + negative sampling [todo] ConclusionSkip gram 與 CBOW 實際上都 train 了兩個 embedding matrix $W$ and $W’$ $W:$ 在 C implement 稱作 $syn0$。 $W’$: 若採用 hierarchical softmax 稱為 $syn1$ 若採用 negative sampling 叫 $syn1neg$ 根據性質，$syn0$ 與 $syn1neg$ 是一體兩面的，本身都可以代表 word embedding，皆可使用。 而代表 $W’$ 的 $syn1$ 因爲連結的是 huffman tree 的 non leaf node，所以其本身對 word $w_i$ 沒直接意義。 Reference On word embeddings - Part 1 https://ruder.io/word-embeddings-1/ On word embeddings - Part 2: Approximating the Softmax https://ruder.io/word-embeddings-softmax/ Learning Word Embedding https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy other Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from http://arxiv.org/abs/1411.2738 Language Models, Word2Vec, and Efficient Softmax Approximations https://rohanvarma.me/Word2Vec/ Word2Vec Tutorial - The Skip-Gram Model word2vec原理(一) CBOW与Skip-Gram模型基础 The Illustrated Word2vec Word2vec数学原理全家桶 http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ Word2Vec-知其然知其所以然 https://www.zybuluo.com/Dounm/note/591752#word2vec-知其然知其所以然 C source code Word2Vec源码解析 https://www.cnblogs.com/neopenx/p/4571996.html 應用 小白看Word2Vec的正确打开姿势|全部理解和应用 https://zhuanlan.zhihu.com/p/120148300 推荐系统从零单排系列(六)—Word2Vec优化策略层次Softmax与负采样 [https://zhuanlan.zhihu.com/p/66417229","link":"/word2vec-from-theory-2-implement/"},{"title":"一步步透視 GBDT Classifier","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x)$ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ GBDT classifier 常用的 loss function 為 cross entropy classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$ 跟 $probability$ 有關，得將 $F(x)$ 通過 $\\textit{sigmoid function }$ 獲得 probability $p = \\sigma(F(x))$ GBDT 簡介在 一步步透視 GBDT Regression Tree 直接進入正題吧 GBDT Algorithm - step by stepGBDT classification tree algorithm 跟 regression tree 並無不同 Input Dat and Loss Function Input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data target $y_i$: who loves Troll2 features of $x_i$: “likes popcorn”, “Age”, “favorite” Our goal is using $x_i$ to predict someone like Trolls 2 or not loss function 為 cross entropy \\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​值得注意的是，GBDT - classifier $F(x)$ 輸出的是 $log(odds)$ 而不是 $probability$ 要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\\textit{sigmoide function}$ \\textit{The probability of Loving Troll 2 } = \\sigma(F(x)) = p $log(odds)$ 轉換成 $probability$ 公式 p = \\cfrac{\\exp^{log(odds)}}{1 + exp^{log(odds)}} Step 1 Initial model with a constant value $F_0(X)$ 初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可 計算完，得到 $F_0(x) = 0.69$，每個 data point 的初始 prediction 都一樣就是 $F_0(x)$。 $F_0(x)$ 是 $\\log(odds)$ 若要計算 probability of loving Troll 2 呢？ $\\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。 ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$ ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving Troll 2 Step2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At Epoch m = 1(A) Calculate Residuals of $F_{0}(x)$classification 問題中 residual 為 predicted probability 與 observed label $y$ 之間的差距 $residual = observed - \\textit{predicted probability}$ true label 為 1 false label 為 0 注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$ 輸出的是 $log(odds)$ 計算各 data sample 的 residual 後： ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$ ep_0_prob 表 $F_0(x)$ predicted probability，$\\sigma(F_0(x))$ ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$ 建樹為一般 fit regression tree 的過程，criterion 為 mean square error，假設找到的樹結構為 可以看到綠色為 leaf node，所有的 data sample $x$ 都被歸到特定 leaf node 下 ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下 \\cfrac{\\sum residual_i}{\\sum [\\textit{previous probability} \\times \\textit{(1 - previous probability)}]} 分子是 each leaf node 下的 data sample $x$ 的 residual 和 分母的 previous probability 為 $m -1$ 步 GBDT 輸出的 probability $p = \\sigma(F(x))$ 。在這個 epoch 是指 $F_0(x)$ 經過計算後，每個 leaf node 輸出 ep_0_prob 表 $\\sigma(F_0(x))$ 計算出的 probability of loving Troll2 ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob ep_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值 (D) update $F_1(x)$ with new tree $f_1(x)$現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 F_1(x) = F_0(x) + \\textit{learning rate} \\times f_1(x)輸出的 probability 為 $\\sigma(F_1(x))$ 令 $\\textit{learnign rate = 0.8}$，得到 epoch 2 每個 data sample 的 $\\log(odds)$ prediction 與 probability prediction ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$ ep_1_prob 為 $F_1(x)$ 輸出的 probability $\\sigma(F_1(x))$ At Epoch m = 2(A) Calculate Residuals of $F_1(x)$計算上一步 $\\textit{residual of } F_1(X)$ residual = observed - \\textit{predicted probability} ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\\sigma(F_1(x))$ ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$ (B) Construct New Regression Tree $f_2(x)$用 data sample x 的 columns “like_popcor”, “age”, “favorite_color” 擬合 ep_2_residual build a new tree $f_2(x)$ 假設得到 $f_2(x)$ 的樹結構： 每個 data sample 對應的 leaf index ep_2_leaf_index 表 data sample 對應到 $f_2(x)$ 上的 leaf node index (D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$計算 $f_2(x)$ 下每個 leaf node 的輸出: 對應到 data sample 上: ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node 下會有一樣的值 Update $F_2(x)$ with New Tree $f_2(x)$到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為 $F_2(x) = F_0(x) + \\nu(f_1(x) + f_2(x))$ $\\nu$ 為 learning rate，假設為 0.8 GBDT 輸出的 probability 為 $\\sigma(F_2(x))$，計算 epoch 2 的 prediction of probability of loving troll2: love_toll2: our target ep_0_pre 表 $F_0(x)$ ep_1_leaf_output 表 data sample x​ 在第一顆樹 $f_1(x)$ 的輸出值 ep_2_leaf_output 表 data sample x 在第二顆樹 $f_2(x)$ 的輸出值 ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$ ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\\sigma(F_2(x))$ Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ 把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 的每棵樹 $f_m(x)$ 都是去 fit $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$ residual = observed - \\textit{predicted probability}所以 $F_m(x)$ 又可以寫成 F_m(x) = F_{m-1}(x) + \\nu f_m(x)這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住： $F_m(x)$ 輸出 $log(odds)$ $residual$ 的計算與 probability 有關 GBDT Classifier 背後的數學Q: 為什麼用 cross entropy 做為 loss function ?在分類問題上，我們預測的是 $\\textit{The probability of loving Troll 2}$ $P(Y|x)$，$\\textit{}$ 以 $maximize$ $\\textit{log likelihood}$ 來解 $P(Y|x)$。 令 GBDT - classification tree 的 probability prediction 為 $P(Y| x) = \\sigma(F(x))$，則 objective function 為 \\textit{log (likelihood of the obersved data given the prediction) } \\\\= \\sum_{i=1}^N [y_i log(p) + (1-y_i)log(1-p)] $p = P(Y=1|x)$，表 the probability of loving movie Troll 2 $y_i$ : observation of data sample $x_i$ loving Troll 2 or not $y \\in \\{1, 0\\}$ 而 $\\textit{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成 \\textit{objective function} = - \\sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)所以 $\\textit{loss function} = -[y log(p) + (1-y)log(1-p)]$ 把 loss function 用 $odds$ 表示： \\begin{aligned} -[y log(p) + (1-y)log(1-p)] & = -ylog(p)-(1-y)log(1-p) \\\\ &= -ylog(p)-log(1-p) + ylog(1-p) \\\\ &= -y[log(p) - log(1-p)] - log(1-p) \\\\ & = -ylog(odds) - log(1-p) \\\\ &= -ylog(odds) + log(1 + \\exp^{log(odds)}) \\end{aligned} 第三個等號 到 第四個等號用到 $odds=\\cfrac{p}{1-p}$ 第四個等號 到 第五個等號用到 $p = \\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}$ 這個結論 $log(1-p) = log(1- \\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}) = log(\\cfrac{1}{1 + \\exp^{log(odds)}}) = -log(1 + \\exp^{log(odds)})$ 把 loss function 表示成 odds 的好處是， $-ylog(odds) + log(1 + \\exp^{log(odds)})$ 對 $log(odds)$ 微分形式很簡潔 \\cfrac{d}{d \\ log(odds)} -ylog(odds) + log(1 + \\exp^{log(odds)}) = -y -\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}} = -y + ploss function 對 $log(odds)$ 的微分，既可以以 $log(odds)$ 表示，也可以以 probability $p$ 表示 以 $log(odds)$ 表示： $\\cfrac{d}{d log(odds)}L(y_i, p) = -y -\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}$ 以 $p$ 表示：$\\cfrac{d}{d log(odds)}L(y_i, p) = -y + p$ 用 $p$ 表示時，loss function 對 $log(odds)$ 的微分 -y + p = \\textit{ -(observed - predicted) = negative residual}Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\\cfrac{count(true)}{count(false)})$ ? 來自 Step 1 的問題 根據選定的 loss function \\textit{loss function} = -[y log(p) + (1-y)log(1-p)] $P(Y=1|x) = p$ 為出現正類的 probability $y \\in \\{1, 0\\}$ 將 loss function 以 $log(odds)$ 表示 -[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \\exp^{log(odds)})]$F_0(x)$ 為能使 $\\textit{cost function}$ 最小的 $log(odds): \\gamma$ F_0(x) = argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma) = argmin_\\gamma \\sum^n_{i=1} -[y_ilog(odds) + log(1 + \\exp^{log(odds)})] $n$ 為 number of data sample $x$ 令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$ cost function 對 $log(odds)$ 微分取極值： \\begin{aligned}& \\cfrac{d}{d log(odds)}\\sum^n_{i=1} -[y_ilog(odds) + log(1 + \\exp^{log(odds)})] \\\\ & = \\cfrac{d}{d log(odds)}\\sum^{n^{(1)}}_i -(log(odds) + log(1 + exp^{log(odds)})) - \\sum^{n^{(0)}}_j (0 * log(odds) + log(1 + \\exp^{log(odds)})) \\\\& = \\cfrac{d}{dlog(odds)} -n^{(1)} \\times (log(odds) + log(1 + exp^{log(odds)})) - n^{(0)} \\times log(1 + \\exp^{log(odds)}) \\\\ & =0\\end{aligned} \\begin{aligned} & n^{(1)} \\times(-1 + \\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}) + n^{(0)} \\times(\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}) \\\\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\\\ & = - n^{(1)} + n p \\\\ &= 0 \\end{aligned}移項得到 $p$ p = \\cfrac{n^{(1)}}{n} log(odds) = \\cfrac{p}{1-p} = \\cfrac{n^{(1)}}{n^{(0)}}故得證，給定 $\\textit{loss function } = -[y log(p) + (1-y)log(1-p)]$， 能使 $argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma)$ 的 $\\gamma$ 為 log(odds)= \\cfrac{n^{(1)}}{n^{(0)}} \\therefore F_0(x) = \\cfrac{n^{(1)}}{n^{(0)}}Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？ 問題來自 Step 2 - (A) 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum -[y log(p) + (1-y)log(1-p)] $y$ 為 target label $p = P(Y=1|x)$ 表正類的 probability 注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy 我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x) \\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - p)) \\\\ &= -(-(observed - \\textit{predict probability})) \\\\ &= - \\textit{negative residual} \\\\ & = residual \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual Q: leaf node 的輸出公式怎麼來的？ 問題來自 Step 2-(C) 在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\\gamma_{jm}$，使的 cost function 最小 \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma) \\\\ &= argmin_\\gamma \\sum_{x_i \\in R_{j, m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + \\exp^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $j$ 表 leaf node index $m$ 表第 $m$ 步 $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合 將 loss function 以 $log(odds)$ 表示後的 objective function \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + \\exp^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \\exp^{log(odds)})]$ $F_{m-1}(x)$ 輸出為 $log(odds)$ cost function 對 $\\gamma$ 微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近 loss function 處理 \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + \\exp^{F_{m-1}(x_i) + \\gamma }) \\end{aligned}讓 2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \\gamma)$ 在 $F_{m-1}(x)$ 處展開 L(y_i, F_{m-1}(x_i) + \\gamma) \\approx L(y_i, F_{m-1}(x_i) ) + \\cfrac{d}{d (F_{m-1}(x))} L(y_i, F_{m-1}(x_i))\\gamma + \\cfrac{1}{2} \\cfrac{d^2}{d (F_{m-1}(x) )^2}L(y_i, F_{m-1}(x_i))\\gamma^2將 cost function 對 $\\gamma$ 微分取極值，求 $\\gamma_{j,m}$ \\sum_{x_i \\in R_{jm}} \\cfrac{d}{d\\gamma} L(y_i, F_{m-1}(x_i), \\gamma) \\approx \\sum_{x_i \\in R_{jm}} (\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\\gamma) = 0移項得到 $\\gamma$ \\gamma = \\cfrac{\\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))}分子是 derivative of Loss function ; 分母是 second derivative of loss function 分子部分: \\begin{aligned} & \\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\\\& = \\sum \\cfrac{d}{d(F_{m-1}(x_i))} \\ y_i \\times [F_{m-1}(x_i) ] - log(1 + \\exp^{F_{m-1}(x_i) }) \\\\ &= \\sum (y_i - \\cfrac{\\exp^{^{F_{m-1}(x_i) }}}{1 + \\exp^{^{F_{m-1}(x_i) }}} ）\\\\& = \\sum_{x_i \\in R_{jm}} (y_i -p_i) \\end{aligned} $F_{m-1}(x_i)$ 是 $m-1$ 步時 $classifier$ 輸出的 $log(odds)$ 分子部分為 $\\textit{summation of residual}$ 分母部分 \\begin{aligned}& \\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\\\ & = \\sum_{x_i \\in R_{jm} } \\cfrac{d^2}{d \\, \\ (F_{m-1}(x_i))^2} \\, -[y_i \\times F_{m-1}(x_i) - log(1 + \\exp^{F_{m-1}(x_i)})] \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)}-[y_i - \\cfrac{\\exp^{^{F_{m-1}(x_i) }}}{1 + \\exp^{^{F_{m-1}(x_i) }}}] \\\\ & =\\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \\exp^{F_{m-1}(x_i)})^{-1} \\times \\exp^{F_{m-1}(x_i)}] \\\\ & = \\sum_{x_i \\in R_{jm} }-[(1 + \\exp^{F_{m-1}(x_i)})^{-2} \\exp^{F_{m-1}(x_i)}\\times \\exp^{F_{m-1}(x_i)} - (1+ \\exp^{F_{m-1}(x_i)})^{-1} \\times \\exp^{F_{m-1}(x_i)} ] \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} \\quad + \\quad \\cfrac{\\exp^{F_{m-1}(x_i)}}{1+ exp^{F_{m-1}(x_i)}} = \\sum_{x_i \\in R_{jm} }\\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} + \\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} \\times \\cfrac{(1 + \\exp^{F_{m-1}(x_i)})}{1 + \\exp^{F_{m-1}(x_i)}} \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} + \\cfrac{\\exp^{F_{m-1}(x_i)} + \\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} = \\sum_{x_i \\in R_{jm} }\\cfrac{\\exp^{F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp^{F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})} \\times \\cfrac{1}{(1 + \\exp^{F_{m-1}(x_i)})} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp^{log(odds)_i}}{1 + \\exp^{log(odds)_i}} \\times \\cfrac{1}{1 + \\exp^{log(odds)_i}} \\\\ &= \\sum_{x_i \\in R_{jm} } p_i \\times (1-p_i) \\end{aligned}綜合分子分母，能使 $F_m(x)$ cost function 最小化的 tree $f_m(x)$ 第 $j$ 個 leaf node 輸出為 \\gamma_{jm}= \\cfrac{\\sum_{x_i \\in R_{jm})} (y_i - p_i)}{\\sum_{x_i \\in R_{jm} }(p_i \\times (1- p_i))} = \\cfrac{\\textit{summation of residuals }}{\\textit{summantion of (previous probability $\\times$ (1 - previoous probability))}}寫在最後Data Sample learning by doing it 123456789101112import pandas as pddata = [ (True, 12, 'blue', True), (True, 87, 'gree', True), (False, 44, 'blue', False), (True, 19, 'red', False), (False, 32, 'green', True), (False, 14, 'blue', True)]columns = ['like_popcorn', 'age', 'favorite_color', 'love_troll2']target = 'love_troll2'df = pd.DataFrame.from_records(data, index=None, columns=columns) Reference Gradient Boost Part 3 (of 4): Classification Gradient Boost Part 4 (of 4): Classification Details Gradient Boosting In Classification: Not a Black Box Anymore! https://blog.paperspace.com/gradient-boosting-for-classification/ statquest 整理 StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s The Logit and Sigmoid Functions https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/ Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979","link":"/GBDT-Classifier-step-by-step/"},{"title":"Negative Sampling 背後的數學","text":"以下用 Skip-gram 為例 $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ Noise Contrastive Estimation (NCE) NCE attempts to approximately maximize the log probability of the softmax output The Noise Contractive Estimation metric intends to differentiate the target word from noise sample using a logistic regression classifier 從 cross entropy 說起 True label $y_i$ is 1 only when $w_i$ is the output word: \\mathcal{L}_\\theta = - \\sum_{i=1}^V y_i \\log p(w_i | w_I) = - \\log p(w_O \\vert w_I)又 $p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ $\\text{v}^{‘}_{w_i}$ is vector of word $w_i$ in $W^{‘}$ $w_O$ is the output word in $V$ $w_I$ is the input word in $V$ 代入後 \\mathcal{L}_{\\theta} = - \\log \\frac{\\exp({\\text{v}'_{w_O}}^{\\top}{\\text{v}_{w_I}})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top}{\\text{v}_{w_I} })} = - {\\text{v}'_{w_O}}^{\\top}{\\text{v}_{w_I} } + \\log \\sum_{i=1}^V \\exp({\\text{v}'_{w_i} }^{\\top}{\\text{v}_{w_I}})Compute gradient of loss function w.s.t mode’s parameter $\\theta$，令 $z_{IO} = {\\text{v}’_{w_O}}^{\\top}{\\text{v}_{w_I}}$ ; $z_{Ii} = {\\text{v}’_{w_i}}^{\\top}{\\text{v}_{w_I}}$ \\begin{aligned} \\nabla_\\theta \\mathcal{L}_{\\theta} &= \\nabla_\\theta\\big( - z_{IO} + \\log \\sum_{i=1}^V e^{z_{Ii}} \\big) \\\\ &= - \\nabla_\\theta z_{IO} + \\nabla_\\theta \\big( \\log \\sum_{i=1}^V e^{z_{Ii}} \\big) \\\\ &= - \\nabla_\\theta z_{IO} + \\frac{1}{\\sum_{i=1}^V e^{z_{Ii}}} \\sum_{i=1}^V e^{z_{Ii}} \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\sum_{i=1}^V \\frac{e^{z_{Ii}}}{\\sum_{i=1}^V e^{z_{Ii}}} \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\sum_{i=1}^V p(w_i \\vert w_I) \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\mathbb{E}_{w_i \\sim Q(\\tilde{w})} \\nabla_\\theta z_{Ii} \\end{aligned}可以看出，gradient $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$是由兩部分組成 : a positive reinforcement for the target word $w_O$, $\\nabla_{\\theta}z_{O}$ a negative reinforcement for all other words $w_i$, which weighted by their probability, $\\sum_{i=1}^V p(w_i \\vert w_I) \\nabla_\\theta z_{Ii}$ Second term actually is just the expectation of the gradient $\\nabla_{\\theta}z_{Ii}$ for all words $w_i$ in $V$。 And probability distribution $Q(\\tilde{w})$ could see as the distribution of noise samples NCE sample 原理According to gradient of loss function $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$ \\nabla_{\\theta}\\mathcal{L}_{\\theta} =- \\nabla_\\theta z_{IO} + \\mathbb{E}_{w_i \\sim Q(\\tilde{w})} \\nabla_\\theta z_{Ii}Given an input word $w_I$, the correct output word is known as $w$，我們同時可以從 noise sample distribution $Q$ sample 出 $M$ 個 samples $\\tilde{w}_1, \\tilde{w}_2, \\dots, \\tilde{w}_M \\sim Q$ 來近似 cross entropy gradient 的後半部分 現在我們手上有個 correct output word $w$ 和 $M$ 個從 $Q$ sample 出來的 word $\\tilde{w}$ ， 假設我們有一個 binary classifier 負責告訴我們，手上的 word $w$，哪個是 correct $p(d=1 | w, w_I)$，哪個是 negative $p(d=0 | \\tilde{w}, w_I)$ 於是 loss function 改寫成： \\mathcal{L}_\\theta = - [ \\log p(d=1 \\vert w, w_I) + \\sum_{i=1, \\tilde{w}_i \\sim Q}^M \\log p(d=0|\\tilde{w}_i, w_I) ]According to the law of large numbers $E_{p(x)} [ f(x)] \\approx \\frac{1}{n} \\sum^{n}_{i=1}f(x_i)$，we could simplify: \\mathcal{L}_\\theta = - [ \\log p(d=1 \\vert w, w_I) + M \\mathbb{E}_{\\tilde{w}_i \\sim Q} \\log p(d=0|\\tilde{w}_i, w_I)]$p(d |w, w_I)$ 可以從 joint probability $p(d, w|w_I)$ 求 $p(d, w | w_I) =\\begin{cases} \\frac{1}{M+1} p(w \\vert w_I) &amp; \\text{if } d=1 \\\\ \\frac{M}{M+1} q(\\tilde{w}) &amp; \\text{if } d=0 \\end{cases}$ $d$ is binary value $M$ is number of samples，we have a correct output word $w$ and sample M word from distribution $Q$ 因爲 $p(d| w, w_I) = \\frac{p(d, w, w_I)}{p(w, w_I)} = \\frac{p(d, w | w_I) p(w_I)}{p(w|w_I)p(w_I)} = \\frac{p(d, w| w_I)}{\\sum_dp(d,w| w_I)}$ 可以得出 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(d=1, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned} \\begin{aligned} p(d=0 \\vert w, w_I) &= \\frac{p(d=0, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{Mq(\\tilde{w})}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned} $q(\\tilde{w})$ 表從 distribution $Q$ sample 出 word $\\tilde{w}$ 的 probability 最終 loss function of NCE \\begin{aligned} \\mathcal{L}_\\theta & = - [ \\log p(d=1 \\vert w, w_I) + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log p(d=0|\\tilde{w}_i, w_I)] \\\\ & = - [ \\log \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{p(w \\vert w_I) + Mq(\\tilde{w}_i)}] \\end{aligned}$p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 代入 $\\mathcal{L}_{\\theta}$ \\begin{aligned}\\mathcal{L}_{\\theta} &= -[log\\frac{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})}}{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})+ Mq(\\tilde{w})}} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i\\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})}+ Mq(\\tilde{w}_i)}]\\end{aligned}可以看到 normalizer $Z(w) = {\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 依然要 sum up the entire vocabulary，實務上通常會假設 $Z(w)\\approx1$，所以 $\\mathcal{L}_\\theta$ 簡化成: \\mathcal{L}_\\theta = - [ \\log \\frac{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}})}{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}}) + Mq(\\tilde{w})} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}}) + Mq(\\tilde{w}_i)}]關於 Noise distribution $Q$關於 noise distribution $Q$，在設計的時候通常會考慮 it should intuitively be very similar to the real data distribution. it should be easy to sample from. Negative Sampling (NEG)Negative sampling can be seen as an approximation to NCE Noise Contrastive Estimation attempts to approximately maximize the log probability of the softmax output The objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modeling 從 NCE 說起NCE $p(d|w, w_I)$ equation，$p(d=1 |w, w_I)$ 代表 word $w$ 是 output word 的機率; $p(d=0|w, w_I)$ 表 sample 出 noise word 的機率 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(d=1, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned}\\begin{aligned} p(d=0 \\vert w, w_I) &= \\frac{p(d=0, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{Mq(\\tilde{w})}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned}NCE 假設 $p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 中的分母 $Z(w) = {\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})} = 1$，所以簡化成 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} &= \\frac{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})}{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) + Mq(\\tilde{w})} \\end{aligned}NEG 繼續化簡NEG 繼續假設 $Nq(\\tilde{w}) = 1$ 式子變成 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})}{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) + 1} &= \\frac{1}{1 +\\exp(-{\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})} = \\sigma({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) \\end{aligned}p(d=0|w, w_I) = 1 - \\sigma({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) = \\sigma(-{\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})最終得到 loss function \\mathcal{L}_\\theta = - [ \\log \\sigma({\\text{v}'_{w}}^\\top \\text{v}_{w_I}) + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\sigma(-{\\text{v}'_{\\tilde{w}_i}}^\\top \\text{v}_{w_I})]前項是 positive sample $p(d=1 \\vert w, w_I)$，後項是 M 個 negative samples $p(d=0|w, w_I) $ 在 skipgram with negative sampling 上 $\\text{v}_{w_I}$ 表 input 的 center word $w_I$ 的 vector，來自 $W$ $\\text{v}’_{w}$ 表 output side 的一個 context word $w$ 的 vector， 來自 $W’$ 實作上 skipgram 一個單位的 data sample 只會包含 一個 center word 與 一個 context word 的 pair 對 $(w_I, w_{C,j})$ 參閱 [todo] 結論 NEG 實際上目的跟 Hierarchical Softmax 不一樣，並不直接學 likelihood of correct word，而是直接學 words 的 embedding，也就是更好的 word distribution representation Reference Learning Word Embedding https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions 此篇從 skip gram 講解 negative sampling On word embeddings - Part 2: Approximating the Softmax https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches 此篇從 CBOW 講解 negative sampling Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722 Word2Vec Tutorial Part 2 - Negative Sampling [http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/","link":"/negative-sampling-in-word2vec/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"word embedding","slug":"word-embedding","link":"/tags/word-embedding/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}