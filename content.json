{"pages":[{"title":"","text":"google-site-verification: google39024f504cd1475b.html","link":"/google39024f504cd1475b.html"},{"title":"歡迎來到 404 號房","text":"不好意思，你沒有房間鑰匙預計將在約 5 秒後返回大廳。如果你很急著開房，你可以 點這裡 返回櫃檯。 let countTime = 5; function count() { document.getElementById('timeout').textContent = countTime; countTime -= 1; if(countTime === 0){ location.href = 'https://seed9d.github.io/'; // 記得改成自己網址 Url } setTimeout(() => { count(); }, 1000); } count();","link":"/404.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"一步步透視 GBDT Regression Tree","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x) $ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 結論說完了，想看數學原理請移駕 背後的數學，想看例子從無到有生出 GBDT 的請到 Algorithm - step by step ，想離開得請按上一頁。 GBDT 簡介GBDT-regression tree 簡單來說，訓練時依序建立 trees $\\{ f_1(x), f_2(x), …. , f_M(x)\\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。 所以 GBDT - regression tree 的訓練是 sequentially ，無法以並行訓練加速。 我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下: F_m(x) = F_0(x) + \\nu\\sum^m_{i=1}f_i(x) = F_{m-1}(x) + \\nu f_m(x) $\\nu$ 為 learning rate GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。 差多遠即 residual 的概念： \\textit{residual = observed - predicted}因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual GBDT Algorithm - step by stepAlgorithm 參考了 statQuest 對 GBDT 的講解，連結放在 reference，必看！ GBDT-regression tree 擬合 algorithm： Input Data and Loss Function input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data 接下來都用這組簡單的數據，其中 Target $y_i$ : weight which is what we want to predict $x_i$: features 的組成有 身高，喜歡的顏色，性別 目標是用 $x_i$ 的 height, favorite color, gender 來預測 $y_i$ 的 weight loss function 為 square error L(y_i, F(x)) = \\cfrac{1}{2}(\\textit{observed - predicted}) ^2 = \\cfrac{1}{2}(y_i^2 - F(x))^2 square error commonly use in Regression with Gradient Boost $\\textit{observed - predicted}$ is called $residual$ $y_i$ are observed value $F(x)$: the function which give us the predicted value 也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出 F_m(x) = F_{m-1}(x) + \\nu f_m(x)F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x) Step 1 Initialize Model with a Constant Value初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可 取 weight 平均得到 $F_0(x) = 71.2 = \\textit{average weight}$ Step 2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At epoch m=1 (A) Calculate Residuals of $F_{0}(x)$epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals ​ residuals = \\textit{observed weight - predicted weight} 而 $F_0(x) = \\textit{average weight = 71.17}$ 計算 residual 後: epoch_0_prediction 表 $F_0(x)$ 輸出 epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$ (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual 用 columns $\\textit{height, favorite, color, gender}$ 預測 $residuals$ 來建新樹 $f_1(x)$ 建樹的過程為一般的 regression tree building 過程，target 就是 residuals。 假設我們找到分支結構是 綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$ 特定的 leaf node 下。 epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值 直覺地對每個 leaf node 內的 data sample $x$ weight 值取平均，得到輸出值 epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值 (D) Update $F_1(x)$ with New Tree $f_1(x)$此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$ 現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \\textit{learning rate } \\times f_1(x)$ 假設 $\\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下 epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值 At epoch m=2(A) Calculate Residuals of $F_{1}(x)$m = 2 新的 residual between observed weight and $F_1(x)$如下 epoch_1_prediction 為 $F_1(x)$ 的輸出 epoch_2_residual 為 observed weight 與 predicted weight $F_1(x)$ 的 residual (B) Construct New Regression Tree $f_2(x)$建一顆新樹擬合 epoch 2 (A) 得出的 residual epoch_2_residual 為 $f_2(x)$ 要擬合的 target 假設 $f_2(x)$ 擬合後樹結構長這樣 epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_2(x)$決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個 leaf node 下的 data sample $x$ 取平均 epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是 residual epoch_2_residual (D) Update $F_2(x)$ with New Tree $f_2(x)$到目前為止我們建立了兩顆 $tree$ $f_1(x), f_2(x)$，假設 $\\textit{learning rate = 0.1}$，則 $F_2(x)$ 為 F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))每個 data sample 在 $F_2(x)$ 的 predict 值如下圖： weight: out target value epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值 epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual epoch_2_prediction: $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出 Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ Step 3 輸出模型 $F_M(x)$，把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$ 與上一步 predicted value $\\hat{y}$ $F_{m-1}(x)$ 的 $residual$ GBDT Regression 背後的數學Q: Loss function 為什麼用 mean square error ?​ 選擇 $\\cfrac{1}{2}(\\textit{observed - predicted}) ^2$ 當作 loss function 的好處是對 $F(X)$ 微分的形式簡潔 \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\\textit{negative residual}$ ​ $-(y_i - F(X)) = \\textit{-(observed - predicted) = negative residual } $ 而我們知道 $F(X)$ 在 loss function $L(y_i, F(X))$ 的 gradient 就是 $\\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$ Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？問題來自 step 1 我們要找的是能使 cost function $\\sum^n_{i=1}L(y_i, \\gamma)$ 最小的那個輸出值 $\\gamma$ 做為 $F_0(X)$。 $F_0(x) = argmin_r \\sum^n_{i=1} L(y_i,\\gamma)$ $F_0(x)$ 初始化的 function，其值是常數 $\\gamma$ refers to the predicted values $n$ 是 data sample 數 Proof: 已知 ​ \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X)) 所以 cost function 對 $\\gamma $ 微分後，是所有 sample data 跟 $\\gamma$ 的 negative residual 和 為 0 ​ \\sum^n_{i=1}L(y_i, \\gamma) = -(y_1 - \\gamma) - (y_2 - \\gamma)- ... -(y_n - \\gamma) = 0 移項得到 $\\gamma$ ​ \\gamma = \\cfrac{y_1 + y_2 + .... + y_n}{n} 正是所有 target value 的 mean，故得證 ​ F_0(x) = \\gamma = \\textit{the average of targets value} Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？問題來自 step 2A 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum \\cfrac{1}{2}(y_i^2 - F(x))^2我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x)\\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} \\ L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - F_{m-1}(x))) \\\\ &= -(-(observed - predicted )) \\\\ &= - \\textit{negative residual} \\\\ & = residual \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual ​ $f_m(x)$ = $\\textit{gradient decent}$ = $\\textit{negative gradient}$ = $residual$ GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\\textit{gradient decent}$ 的方向得到新的 $F(x)$ 這也正是為什麼叫做 gradient boost 。 by the way，step 2-(A) compute residuals: r_{im} = -[\\cfrac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \\ \\textit{for i = 1,....,n} $i$: sample number $m$: the tree we are trying to build Q：個別 leaf node 的輸出為什麼是取平均 ？問題來自 step 2C 在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\\gamma$ 使 cost function 最小 $\\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma)$ $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合 $F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$ 上的輸出值為 $\\gamma_{j,m}$ \\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2直接對 $\\gamma$ 微分 \\begin{aligned}\\cfrac{d}{d \\gamma} \\ \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2 = \\ \\sum_{x_i \\in R_{j,m}}-(y_i - F_{m-1}(x_i) - \\gamma) = 0\\end{aligned}移項 \\gamma = \\cfrac{1}{n_{jm}} \\sum_{x_i \\in R_{j,m}}y_i - F_{m-1}(x_i) $n_{jm}$ is the number of data sample in leaf node $j$ at step $m$ 白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均 事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $ 都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。 Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？問題來自 step 2D \\begin{aligned}F_m(x) & = F_{m-1}(x) + \\nu f_m(x) \\\\ &= F_{m-1}(x) + \\nu \\sum^{J_m}_{j=1}\\gamma_{jm}I(x \\in R_{jm})\\end{aligned} $F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x)$ $\\nu$ learning rate $J_m$ m 步 的 leaf node 總數 $\\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出 $F_m(x)$ 展開來就是 F_m(x) = F_0(x) + \\nu(f_1(x) + f_2(x)+ ...+f_m(t))寫在最後 seeing is believing 太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙 人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶 learning by doing it 所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的 1234567891011import pandas as pddata = [ (1.6, 'Blue', 'Male', 88), (1.6, 'Greem', 'Female', 76), (1.5, 'Blue', 'Female', 56), (1.8, 'Red', 'Male', 73), (1.5, 'Green', 'Male', 77), (1.4, 'Blue', 'Female', 57) ]columns = ['height', 'favorite_color', 'gender', 'weight']df = pd.DataFrame.from_records(data, index=None, columns=columns) always get your hands dirty ReferenceMain Gradient Boost Part 1 (of 4): Regression Main Ideas Gradient Boost Part 2 (of 4): Regression Details ccd comment: 上面兩個必看 A Step by Step Gradient Boosting Decision Tree Example https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/ 真 step by step Gradient Boosting Decision Tree Algorithm Explained https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4 including sklearn 實作 Other Gradient Boosting Decision Tree http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法 提升树算法理论详解 https://hexinlin.top/2020/03/01/GBDT/ 梯度提升树(GBDT)原理小结 https://www.cnblogs.com/pinard/p/6140514.html","link":"/GBDT-Rregression-Tree-Step-by-Step/"},{"title":"Thompson Sampling 推薦系統中簡單實用的 Exploring Strategy","text":"Exploring and ExploitingExploring and Exploiting (EE) 是推薦系統中歷久不衰的議題，如何幫助用戶發現更多感興趣的 entity 以及基於已有對用戶的認知推薦他感興趣的 entity，在推薦系統的實務上都得考慮。 具象化這個問題：在推薦系統中有$\\text{}$ $\\text{category A, category B, category C, category D, category E}$ 等五大類的 entity 集合，今天有個新用戶 $U$來了，我們要如何 知道他對哪個種類的 entity 比較感興趣？ 人的興趣可以分成長期興趣跟短期興趣，在電商場景中，用戶短期興趣指他有立即需求的商品，我們如何快速抓到他的意圖，調整推薦系統的響應？ 推薦哪些類目能帶給他意料之外的驚喜 ? 那些他沒預期，但我們推薦給他，能讓他感到滿意的 category。 Multi-armed bandit problem, K-armed bandit problem (MAP) 中的 Thompson Sampling，簡單又實用 推薦系統相關文章 Thompson Sampling Thompson Sampling 利用了 beta distribution 是 bernoulli distribution 的 conjugacy prior， 來更新 entity 被選中的 posterior probability distribution 從 Beta distribution 說起 Beta(p|\\alpha, \\beta) \\triangleq \\cfrac{1}{B(\\alpha, \\beta)} \\ p^{\\alpha -1} \\ (1-p)^{\\beta - 1} beta function $B(\\alpha, \\beta)$ is a normalization term ，其作用是使 $\\int^1_0 \\cfrac{1}{B(\\alpha, \\beta)} \\ p^{\\alpha -1} \\ (1-p)^{\\beta - 1} dp = 1$ $B(\\alpha, \\beta) = \\int^1_0 p^{\\alpha -1} (1-p)^{\\beta -1} dp$ Beta distribution $beta(\\alpha, \\beta)$ 的期望值很簡潔 E[p] = \\cfrac{\\alpha}{\\alpha + \\beta}我們知道期望值本身是結果的加權平均，如果把 $\\alpha$ 視為成功次數， $\\beta$ 視為失敗次數，那不就是平均成功率了嗎？ 更神奇的是，平均成功率還可以隨著試驗的失敗跟成功次數變動，依然還是 beta distribution \\cfrac{\\alpha + n^{(1)}}{\\alpha + \\beta + n^{(1)} + n^{(0)}} $n^{(1)}$：表新增成功次數 $n^{(0)}$: 表新增失敗次數 也因為這個直覺的特性，Beta distribution 非常適合用在估計 打擊率, 點擊率, 命中率 …等等 binary problem 以推薦系統 click through rate (CTR) 當例子在推薦系統中，category $A$ 在用戶的點擊率 (ctr) 統計中，所有用戶對 category $A$ : $\\text{average ctr} = 0.33$ $\\text{ctr variacne} = 0.00147$ 以 $\\text{mean=0.33 variacne=0.00147}$ 算出 $\\alpha \\approx50$ $\\beta \\approx100$， $\\alpha =50$ $\\beta=100$ 在本推薦系統中的意義是，$\\text{category A}$ 平均每 150 次 impression ($\\alpha + \\beta$) 能產生 50 次 click ($\\alpha$)，100 次 看了不點 ($\\beta$)。 畫出 PDF 圖中 PDF curve 的意義是，有個人叫做 “平均用戶”，”平均用戶” 對 $\\text{category A}$ 最有可能的點擊率是 $0.33$，但不一定是 0.33, 可能比 0.33 高，可能比 0.33 低，但產生 0.33 這個點擊率的 likelihood $L(\\theta| X=0.33)$ 最高 下圖是對 $beta(50, 100)$ sample 500 次，可以看出 $X=0.33$ 附近被 sample 到的次數的確較高 今天來了個新用戶 $U$，我們不知道他對 $\\text{category A}$ 的喜好程度怎麼樣，但我們可以利用前面的 “平均用戶” 做為先驗： 150 impression 產生 50 次 click ($\\alpha=50 \\ , \\beta=100$ )，再透過他後續跟 $\\text{category A}$ 的互動修正出 for $\\text{user U}$ 的 $\\alpha_U \\ \\beta_U$。 假設我們給 $U$ 展示 $\\text{category A}$ 100 次後， 他 click了 60 次，看了不點 40 次，那他的 beta distribution 變成 $beta(50 + 60, 100 + 40 ) = beta(110, 140)$ 可以發現橘線變得更尖，且往右移，此時 $mean =0.44$，表示 $user \\ U$ 比＂平均用戶＂更加偏好 $\\text{category A}$。 總結以上，一開始我們對於新用戶 $U$ 一無所知，不知道他對 $\\text{category A}$ 的偏好，但我們透過已有的先驗，結合他跟推薦系統的互動，慢慢修正對他的認知： \\cfrac{\\alpha + n^{(1)}}{\\alpha + \\beta + n^{(1)} + n^{(0)}} = \\cfrac{50 + 60}{50 + 100 + 60 + 40} = 0.44 $n^{(1)}$：對 $\\text{category A}$ 新的點擊行為 $n^{(0)}$: 對 $\\text{category A}$ 新的＂看了未點＂的行為 於是，ctr 從原本 “最有可能” 0.33 修正到 “最有可能” 0.44 。 “最有可能”: 因爲一切都是 distribution 阿 這個神奇又簡潔的現象背後的數學原理，正是 beta distribution 的 conjugacy 特性。 Conjugate prior &amp; Bayesian inference prior $p(\\theta)$ is conjugate to the likelihood function $p(X|\\theta)$ when the posterior $p(\\theta|X)$ has the same function form as the prior p(\\theta|X) = \\cfrac{p(X|\\theta) p(\\theta)}{p(X)} \\Leftrightarrow \\text{posterior} = \\cfrac{\\text{likelihood} \\cdot \\text{prior}}{\\text{evidence}} $p(X)$ is the normalization term $p(X) = \\int_{\\theta\\in \\Theta}p(X|\\theta)p(\\theta)d\\theta$ 即是 prior $p(\\theta)$ 為 beta distribution $Beta(\\theta|\\alpha, \\beta) = \\cfrac{1}{B(\\alpha, \\beta)} \\ \\theta^{\\alpha -1} \\ (1-\\theta)^{\\beta - 1}$ likelihood function $p(X|\\theta)$ 為 bernoulli distribution $Bern(c|\\theta) = \\theta^c(1-\\theta)^{1-c}$ beta distribution 與 bernoulli distribution 都有類似的 form: $\\theta^m(1-\\theta)^n$ ，同時 posterior distribution $p(\\theta|X)$ 也是 beta distribution posterior $p(\\theta|X)$ 也是 beta distribution 證明如下 Proof假設 推薦系統中，對 $category \\ A$ 曝光 $N$ 次，用戶 $U$ 點擊次數 $n^{(1)}$，未點擊次數 $n^{(0)}$，本質上是個 $N \\ bernoulli \\ trail$ ， 所以其 likelihood function： $p(C|p) =\\prod_{i=1}^n p(C=c_i|p)= p^{n^{(1)}}(1-p)^{n^{(0)}}$ (忽略係數) $C$ 是 outcome, $c=1$ for positive ; $c=0$ for negative $prior$ $p(p)$ 為 beta distribution : p(p|\\alpha, \\beta) = Beta(p|\\alpha, \\beta) = \\cfrac{1}{B(\\alpha, \\beta)} \\ p^{\\alpha -1} \\ (1-p)^{\\beta - 1} 則 $\\text{posterior}$ $p(p|C,\\alpha, \\beta) = \\cfrac{ p(C|p) \\ p(p|\\alpha, \\beta)}{\\int^1_0 p(C|p) \\ p(p|\\alpha, \\beta) \\ dp}$ 分母項 $\\int^1_0 p(C|p) \\ p(p|\\alpha, \\beta) \\ dp$ 作用為 normalize the distribution，通常用 $Z$ 代表： \\begin{aligned} p(p|C,\\alpha, \\beta) &= \\cfrac{ p(C|p) p(p|\\alpha, \\beta)}{\\int^1_0 p(C|p) p(p|\\alpha, \\beta) dp} \\\\ &= \\cfrac{p^{n^{(1)}} (1-p)^{n^{(0)}} \\cfrac{1}{B(\\alpha,\\beta)} p^{\\alpha -1} (1-p)^{\\beta -1}}{Z} \\\\ &= \\cfrac{p^{[n^{(1)} + \\alpha] -1} (1 - p) ^{[n^{(0)} + \\beta]-1}}{B(\\alpha, \\beta) Z} \\end{aligned} $Z =\\cfrac{1}{B(\\alpha,\\beta)} \\int^1_0 p^{n^{(1)}} (1-p)^{n^{(0)}} p^{\\alpha -1} (1-p)^{\\beta -1} dp$ 分母要 normalize 整個 probability distribution 使 $\\int p(p|C,\\alpha, \\beta) dp= 1$ 而新的 normalization 項為 B(\\alpha,\\beta)Z = \\int^1_0 p^{[n^{(1)} + \\alpha] -1} (1 - p) ^{[n^{(0)} + \\beta]-1}dp這不正是另一個 Beta function: $B(n^{(1)} + \\alpha , n^{(0) } +\\beta)$ ？？ 所以 $p(p|C,\\alpha, \\beta)$ 最終化簡成 \\begin{aligned} p(p|C,\\alpha, \\beta) &= \\cfrac{p^{[n^{(1)} + \\alpha] -1} (1 - p) ^{[n^{(0)} + \\beta]-1}}{B(n^{(1)} + \\alpha , n^{(0) } +\\beta)} \\\\ &= Beta (p|n^{(1)} +\\alpha, n^{(0)} + \\beta)\\end{aligned}故得證 $\\text{posterior}$ $p(p|C,\\alpha, \\beta)$ 也是 $\\text{Beta distribution}$ Implement一個簡單的實作方式是 先在線下計算好每個 category 的 ctr mean 跟 variance。 在實時推薦時，拿回某用戶近期對每個 category 交互數據 impression 與 click ，計算出新的 $\\alpha \\ \\beta$。 有了每個類目的 $\\alpha \\ \\beta$ 後，對每個類目的 $beta(\\alpha, \\beta)$ sampling，接著取出 sample 後 top K 的類目即可。 C2I 召回 ….. 當然，你也可以不基於 category 維度計算 beta distribution，而是基於每一個 entity。不過如果 entity 數量上百萬，這顯然不切實際。 線下統計每個 category CTR 的 variance and mean Spark snippets 1234567def calAvgAndVar(input: Dataset[Row], categoryCol: String): Dataset[Row] = input.select(categoryCol, ctrCol) .groupBy(categoryCol).agg( fn.avg(fn.col(ctrCol)).as(&quot;ctr_avg&quot;), fn.variance(ctrCol).alias(&quot;ctr_var&quot;)) .na.drop .withColumnRenamed(categoryCol, &quot;categoryId&quot;) 線上 計算每個 category 的初始 $\\alpha_0 \\ \\beta_0$ 12345ImmutablePair&lt;Double, Double&gt; calAlphaAndBeta(double ctrMean, double ctrVar) { double alpha = (((1 - ctrMean) / (ctrMean)) - 1 / ctrMean) * Math.pow(ctrMean, 2); double beta = alpha * ((1 / ctrMean) - 1); return ImmutablePair.of(alpha, beta); } \\begin{aligned} \\alpha &=\\left(\\frac{1-\\mu}{\\sigma^2}-\\frac{1}{\\mu}\\right)\\mu^2 \\\\ \\beta &= \\alpha\\left(\\frac{1}{\\mu}-1\\right) \\end{aligned} 取回用戶的近期 category 交互行為 impression and click，並計算新的 $\\alpha_t,\\ \\beta_t$ 123456789101112/* left: alpha, right: beta */ImmutablePair&lt;Double, Double&gt; prior = calAlphaAndBeta(double ctrMean, double ctrVar);/* left: impression, right: click */ImmutablePair&lt;Integer, Integer&gt; posteriorPair = posteriorData.getOrDefault(cateId, ImmutablePair.of(0, 0));int clickCount = posteriorPair.getRight();int impressionCount = posteriorPair.getLeft();int impressionWithoutClick = (impressionCount - clickCount) &gt; 0 ? (impressionCount - clickCount) : impressionCount;double newAlpha = prior.getLeft() + clickCount;double newBeta = prior.getRight() + impressionWithoutClick; 對每個 category 的 beta distribution $beta(\\alpha, \\beta)$ sampling 123456import org.apache.commons.math3.distribution.BetaDistribution;double calBetaProbability(double alpha, double beta) { BetaDistribution betaDistribution = new BetaDistribution(alpha, beta); double rand = Math.random(); return betaDistribution.inverseCumulativeProbability(rand);} sampling 利用 beta distribution 的 inverse cumulative distribution function (inverse CDF) sampling 出 random variable 參考 https://en.wikipedia.org/wiki/Inverse_transform_sampling Reference Multi-Armed Bandit With Thompson Sampling https://predictivehacks.com/multi-armed-bandit-with-thompson-sampling/ Conjugacy in Bayesian Inference http://gregorygundersen.com/blog/2019/03/16/conjugacy/ Understanding the beta distribution (using baseball statistics) http://varianceexplained.org/statistics/beta_distribution_and_baseball/ 中文翻譯 : 如何通俗理解 beta 分布？ - 小杰的回答 - 知乎 https://www.zhihu.com/question/30269898/answer/123261564 https://en.wikipedia.org/wiki/Beta_distribution Heinrich, G. (2005). Parameter estimation for text analysis 雖然是講 LDA，但前面從 ML MAP 一路推導到 Bayesian inference ，很詳細","link":"/Implement-Thompson-Sampling-in-Recommendation-System/"},{"title":"Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax","text":"CBOW with Hierarchical SoftmaxCBOW 的思想是用兩側 context words 去預測中間的 center word P(center|context;\\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ $\\text{v}’_j$ 表 $W’$ 中 j-th columns vector 在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是 leaf node ，跟 column vector $\\text{v}’_j$ 與 word $w_i$ 不是直接對應的關係 Objective Function Huffman Tree 令 $w_{I,j}$ 表 input 的 第 $j$ 個 context word; $w_O$ 表 target 的 center word 則 Hierarchical Softmax 下的 objective function \\begin{aligned} & -\\log p(w_O| w_I) = -\\log \\dfrac{\\text{exp}({h^\\top \\text{v}'_O})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} \\\\ & = - \\sum^{L(w)-1}_{l=1} \\log\\sigma([\\cdot] h^\\top \\text{v}^{'}_l) \\end{aligned} $L(w_i) -1$ 表 huffman tree 中從 root node 到 leaf node of $w_i$ 的 node number $[\\cdot]$表 huffman tree 的分岔判斷 $[\\cdot] = 1$ 表 turn left $[\\cdot ] = -1$ 表 turn right $h = \\frac {1}{C} \\sum^{C}_{j=1}\\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$ 詳細推導請見 Word2Vec (2):Hierarchical Softmax 背後的數學 透過 Hierarchical Softmax，因爲 huffman tree 為 full binary tree， time complexity 降成 $\\log_2|V|$ Pytorch CBOW with Hierarchical SoftmaxBuilding Huffman TreeHuffman Tree 建樹過程 HuffmanTree >folded12123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124class HuffmanTree: def __init__(self, fre_dict): self.root = None freq_dict = sorted(fre_dict.items(), key=lambda x:x[1], reverse=True) self.vocab_size = len(freq_dict) self.node_dict = {} self._build_tree(freq_dict) def _build_tree(self, freq_dict): ''' freq_dict is in decent order node_list: two part: [leaf node :: internal node] leaf node is sorting by frequency in decent order; ''' node_list = [HuffmanNode(is_leaf=True, value=w, fre=fre) for w, fre in freq_dict] # create leaf node node_list += [HuffmanNode(is_leaf=False, fre=1e10) for i in range(self.vocab_size)] # create non-leaf node parentNode = [0] * (self.vocab_size * 2) # only 2 * vocab_size - 2 be used binary = [0] * (self.vocab_size * 2) # recording turning left or turning right ''' pos1 points to currently processing leaf node at left side of node_list pos2 points to currently processing non-leaf node at right side of node_list ''' pos1 = self.vocab_size - 1 pos2 = self.vocab_size ''' each iteration picks two node from node_list the first pick assigns to min1i the second pick assigns to min2i min2i's frequency is always larger than min1i ''' min1i = 0 min2i = 0 ''' the main process of building huffman tree ''' for a in range(self.vocab_size - 1): ''' first pick assigns to min1i ''' if pos1 &gt;= 0: if node_list[pos1].fre &lt; node_list[pos2].fre: min1i = pos1 pos1 -= 1 else: min1i = pos2 pos2 += 1 else: min1i = pos2 pos2 += 1 ''' second pick assigns to min2i ''' if pos1 &gt;= 0: if node_list[pos1].fre &lt; node_list[pos2].fre: min2i = pos1 pos1 -= 1 else: min2i = pos2 pos2 += 1 else: min2i = pos2 pos2 += 1 ''' fill information of non leaf node ''' node_list[self.vocab_size + a].fre = node_list[min1i].fre + node_list[min2i].fre node_list[self.vocab_size + a].left = node_list[min1i] node_list[self.vocab_size + a].right = node_list[min2i] ''' the parent node always is non leaf node assigen lead child (min2i) and right child (min1i) to parent node ''' parentNode[min1i] = self.vocab_size + a # max index = 2 * vocab_size - 2 parentNode[min2i] = self.vocab_size + a binary[min2i] = 1 '''generate huffman code of each leaf node ''' for a in range(self.vocab_size): b = a i = 0 code = [] point = [] ''' backtrace path from current node until root node. (bottom up) 'root node index' in node_list is 2 * vocab_size - 2 ''' while b != self.vocab_size * 2 - 2: code.append(binary[b]) b = parentNode[b] # point recording the path index from leaf node to root, the length of point is less 1 than the length of code point.append(b) ''' huffman code should be top down, so we reverse it. ''' node_list[a].code_len = len(code) node_list[a].code = list(reversed(code)) ''' 1. Recording the path from root to leaf node (top down). 2.The actual index value should be shifted by self.vocab_size, because we need the index starting from zero to mapping non-leaf node 3. In case of full binary tree, the number of non leaf node always equals to vocab_size - 1. The index of BST root node in node_list is 2 * vocab_size - 2, and we shift vocab_size to get the actual index of root node: vocab_size - 2 ''' node_list[a].node_path = list(reversed([p - self.vocab_size for p in point])) self.node_dict[node_list[a].value] = node_list[a] self.root = node_list[2 * vocab_size - 2] 建樹過程參考 Word2Vec 作者 Tomas Mikolov 的 c code，思路如下： 建一個 Array，左半邊放 leaf node ，右半邊放 non leaf node leaf node 按照 frequency 降序排列 bottom up building tree 從 Array 中間位置向右半邊填 non leaf node each iteration 都從 leaf node 跟 已填完的 non leaf node 找兩個 frequency 最小的 node，做為 child node 填入當下 non leaf node Hierarchical Softmax用 huffman tree 實作 Hierarchical Softmax 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class HierarchicalSoftmaxLayer(nn.Module): def __init__(self, vocab_size, embedding_dim, freq_dict): super().__init__() ## in w2v c implement, syn1 initial with all zero self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.syn1 = nn.Embedding( num_embeddings=vocab_size + 1, embedding_dim=embedding_dim, padding_idx=vocab_size ) torch.nn.init.constant_(self.syn1.weight.data, val=0) self.huffman_tree = HuffmanTree(freq_dict) def forward(self, neu1, target): # neu1: [b_size, embedding_dim] # target: [b_size, 1] # turns:[b_size, max_code_len_in_batch] # paths: [b_size, max_code_len_in_batch] turns, paths = self._get_turns_and_paths(target) paths_emb = self.syn1(paths) # [b_size, max_code_len_in_batch, embedding_dim] loss = -F.logsigmoid( (turns.unsqueeze(2) * paths_emb * neu1.unsqueeze(1)).sum(2)).sum(1).mean() return loss def _get_turns_and_paths(self, target): turns = [] # turn right(1) or turn left(-1) in huffman tree paths = [] max_len = 0 for n in target: n = n.item() node = self.huffman_tree.node_dict[n] code = target.new_tensor(node.code).int() # in code, left node is 0; right node is 1 turn = torch.where(code == 1, code, -torch.ones_like(code)) turns.append(turn) paths.append(target.new_tensor(node.node_path)) if node.code_len &gt; max_len: max_len = node.code_len turns = [F.pad(t, pad=(0, max_len - len(t)), mode='constant', value=0) for t in turns] paths = [F.pad(p, pad=(0, max_len - p.shape[0]), mode='constant', value=net.hs.vocab_size) for p in paths] return torch.stack(turns).int(), torch.stack(paths).long() syn1 表 $W’$ 裡面的 vector 對應到 huffman tree non leaf node 的 vector 實作上 $W’$ row vector 才有意義 neu1 即 $\\text{h}$ 為 hidden layer 的輸出 target 為 center word $w_O$ function _get_turns_and_paths 中 實作時 -1 表 turn left ; 1 表 turn right ，其實兩者只要相反就好，因爲對於 binary classification $p(\\text{true}) = \\sigma(x)$ ⇒ $p(\\text{false}) = 1- \\sigma(x) = \\sigma(-x)$ 只是 $\\sigma$ 裡的正負號對換而已 CBOW + Hierarchical Softmax12345678910111213class CBOWHierarchicalSoftmax(nn.Module): def __init__(self, vocab_size, embedding_dim, fre_dict): super().__init__() self.syn0 = nn.Embedding(vocab_size, embedding_dim) self.hs = HierarchicalSoftmaxLayer(vocab_size, embedding_dim, fre_dict) torch.nn.init.xavier_uniform_(self.syn0.weight.data) def forward(self, context, target): # context: [b_size, 2 * window_size] # target: [b_size] neu1 = self.syn0(context.long()).mean(dim=1) # [b_size, embedding_dim] loss = self.hs(neu1, target.long()) return loss neu1 為 average of context words’ vector Training訓練過程省略，有興趣請見 notebook seed9D/hands-on-machine-learning Evaluation訓練語料是聖經，看看 jesus 跟 christ 的相近詞 In : 12cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)cosinSim.get_synonym('christ') out: 12345678910[('christ', 1.0), ('hope', 0.78780156), ('gospel', 0.7656436), ('jesus', 0.74575657), ('faith', 0.7190881), ('godliness', 0.7005944), ('offences', 0.70045626), ('grace', 0.6946964), ('dear', 0.666232), ('willing', 0.66131693)] In 1cosinSim.get_synonym('jesus') Out 12345678910[('jesus', 0.9999999), ('gospel', 0.8051339), ('grace', 0.75879383), ('church', 0.7542972), ('christ', 0.74575657), ('manifest', 0.7415799), ('believed', 0.7215627), ('faith', 0.7198993), ('godliness', 0.7091305), ('john', 0.7015951)] In 1cosinSim.get_synonym('god') Out 12345678910[('jesus', 0.9999999), ('gospel', 0.8051339), ('grace', 0.75879383), ('church', 0.7542972), ('christ', 0.74575657), ('manifest', 0.7415799), ('believed', 0.7215627), ('faith', 0.7198993), ('godliness', 0.7091305), ('john', 0.7015951)] Reference https://github.com/tmikolov/word2vec c code 基于Numpy实现Word2Vec Hierarchical Softmax CBOW and SkipGram模型 http://ziyangluo.tech/2020/02/29/W2VHierarchical/ https://github.com/ilyakhov/pytorch-word2vec https://github.com/weberrr/pytorch_word2vec other Binary Tree: Intro(簡介) http://alrightchiu.github.io/SecondRound/binary-tree-introjian-jie.html#fullcomplete","link":"/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/"},{"title":"Word2Vec (4):Pytorch 實作 Word2Vec with Softmax","text":"用 pytorch 實現最簡單版本的 CBOW 與 skipgram，objective function 採用 minimize negative log likelihood with softmax CBOWCBOW 的思想是用兩側 context 詞預測中間 center 詞，context 詞有數個，視 window size 大小而定 P(center|context;\\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ Condition probability $P(center | context; \\theta)$ 中 variable $\\textit{center word}$ 有限，所以是個 descrete probability，可以轉化成多分類問題來解 令 $w_O$ 表 center word, $w_I$ 表 input 的 context word，則 P(center|context;\\theta) = P(w_O|w_I; \\theta) = \\cfrac{\\exp(h^\\top \\text{v}^{'}_{w_{O}})}{\\sum_{w_ \\in V}\\exp(h^\\top \\text{v}'_{w_i})} $h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\\cfrac{1}{C}(\\text{v}_{w_1} + \\text{v}_{w_2}+ …+ \\text{v}_{w_C})^T$ 訓練過程 $\\text{maximize log of condition probability } P(w_O|w_I; \\theta$ \\begin{aligned} & \\text{maxmize}_\\theta \\ \\log P(w_O|w_I; \\theta) \\\\& = \\text{minimize}_\\theta \\ -\\log \\ P(w_O|w_I; \\theta) \\\\& = \\text{minimize}_\\theta \\ - \\log \\cfrac{\\exp(h^\\top \\text{v}^{'}_{w_{O}})}{\\sum_{w_i \\in V} \\exp(h^\\top \\text{v}^{'}_{w_i})} \\end{aligned}Pytorch CBOW + softmaxCBOW + softmax 模型定義123456789101112131415class CBOWSoftmax(nn.Module): def __init__(self, vocab_size, embedding_dim): super().__init__() self.syn0 = nn.Embedding(vocab_size, embedding_dim) self.syn1 = nn.Linear(embedding_dim, vocab_size) def forward(self, context, center): # context: [b_size, windows_size] # center: [b_size, 1] embds = self.syn0(context).mean(dim=1) # [b_size, embedding_dim] out = self.syn1(embds) log_probs = F.log_softmax(out, dim=1) loss = F.nll_loss(log_probs, center.view(-1), reduction='mean') return loss syn0 對應到 input 側的 embedding matrix $W$ syn1 對應到 output 側的 embedding matrix $W’$ loss 的計算 $- log \\cfrac{\\exp(h^\\top \\text{v}^{‘}_{w_{O}})}{\\sum_{w_i \\in V} \\exp(h^\\top \\text{v}^{‘}_{w_i})}$ input: context 跟 center 內容都是將 word index 化 因爲 context 是由 windows size N 個 words 組成，所以總共有 N 個 word embedding ，常規操作是 sum or mean Training Stage訓練過程省略，有興趣的可以去 github 看 notebook seed9D/hands-on-machine-learning 取出 Embedding創建一個衡量 cosine similarity的 class 12345678910111213141516171819class CosineSimilarity: def __init__(self, word_embedding, idx_to_word_dict, word_to_idx_dict): self.word_embedding = word_embedding # normed already self.idx_to_word_dict = idx_to_word_dict self.word_to_idx_dict = word_to_idx_dict def get_synonym(self, word, topK=10): idx = self.word_to_idx_dict[word] embed = self.word_embedding[idx] cos_similairty = w2v_embedding @ embed topK_index = np.argsort(-cos_similairty)[:topK] pairs = [] for i in topK_index: w = self.idx_to_word_dict[i]# pairs[w] = cos_similairty[i] pairs.append((w, cos_similairty[i])) return pairs 僅使用 syn0 做為 embedding，記得 L2 norm 123456syn0 = model.syn0.weight.dataw2v_embedding = syn0 w2v_embedding = w2v_embedding.numpy()l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)w2v_embedding = w2v_embedding / l2norm 訓練的 corpus 是聖經，所以簡單看下 jesus 與 christ 兩個 word 的相似詞，效果不予置評 Skipgramskipgram 的思想是用中心詞 center word 去預測兩側的 context words P(context|center; \\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ 令 $w_I$ 表 input 的 center word， $w_{O,j}$ 表 target 的 第 $j$ 個 context word ，則 condition probability P(context|center;\\theta) = P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) = \\prod^C_{c=1 }\\cfrac{\\exp(h^\\top \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(h^\\top \\text{v}'_{w_i})} $h$ 表 hidden layer 的輸出，在 skipgram 實際上就是 $\\text{v}_{w_I}$ Skipgram 的 objective function \\begin{aligned} & -\\log P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) \\\\ & = -\\log \\prod^C_{c=1}\\cfrac{\\exp(h^\\top \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(h^{\\top} \\text{v}'_{w_i})} \\\\ & = -\\log \\prod^C_{c=1}\\cfrac{\\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_i})} \\\\& = -\\sum^C_{c=1}\\log \\cfrac{\\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_i})} \\end{aligned}Pytorch skipgram + softmax模型12345678910111213141516class SkipgramSoftmax(nn.Module): def __init__(self, vocab_size, embedding_dim): super().__init__() self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.syn0 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K| self.syn1 = nn.Linear(embedding_dim, vocab_size) # |K| x |V| def forward(self, center, context): # center: [b_size, 1] # context: [b_size, 1] embds = self.syn0(center.view(-1)) out = self.syn1(embds) log_probs = F.log_softmax(out, dim=1) loss = F.nll_loss(log_probs, context.view(-1), reduction='mean') return loss syn0 對應到 input 側的 embedding matrix $W$ syn1 對應到 output 側的 embedding matrix $W’$ 實際上，skipgram 每筆 training data 只需要 (a center word, a context word) 的 pair 即可 所以 loss function 實現上非常簡單 -\\log \\cfrac{\\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_i})}Training Stage訓練過程省略，有興趣的可以去 github 看 notebook seed9D/hands-on-machine-learning Evaluation取出 embedding，這次 embedding 嘗試 $(W + W’)/2$ 1234567syn0 = model.syn0.weight.datasyn1 = model.syn1.weight.dataw2v_embedding = (syn0 + syn1) / 2w2v_embedding = w2v_embedding.numpy()l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)w2v_embedding = w2v_embedding / l2norm 一樣看 jesus 跟 christ 的相似詞，感覺似乎比 CBOW 好一點 Reference https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb 基于PyTorch实现word2vec模型 https://lonepatient.top/2019/01/18/Pytorch-word2vec.htm Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from http://arxiv.org/abs/1411.2738 https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py","link":"/Pytorch-Implement-Naive-Word2Vec-with-Softmax/"},{"title":"Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling","text":"Skipgram with Negative Samplingskipgram 的思想是用中心詞 center word 去預測兩側的 context words P(context|center; \\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ Objective Function 令 $w_I$ 表 input 的 center word ; $w_{O,j}$ 表 target 的 第 $j$ 個 context word。 則 Negative Sampling 下的 objective function \\mathcal{L}_\\theta = - [ \\log \\sigma(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,j}}) + \\sum^M_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}\\sigma(-\\text{v}^\\top_{w_I} \\text{v}'_{\\tilde{w}_i})] $\\tilde{w}_i$ 為從 distribution $Q$ sample 出的 word M 為 從 $Q$ sample 出的 $\\tilde{w}$ 數量 第一項為 input center word $w_I$ 與 target context word $w_{O,j}$ 產生的 loss 第二項為 negative sample 產生的 loss ，共 sample 出 $M$ 個 word 有興趣看從 softmax 推導到 NEG的，參閱 Word2Vec (3):Negative Sampling 背後的數學 Negative Sample (NEG)目標是從一個分佈 $Q$ sample 出 word $\\tilde{w}$ 實作上從 vocabulary $V$ sample 出 ${w}_i$ 的 probability $P(w_i)$ 為 P(w_i) = \\cfrac{f(w_i)^{\\alpha}}{\\sum^M_{j=0}(f(w_j)^\\alpha)} $f(w_i)$ 為 $w_i$ 在 corpus 的 frequency count $\\alpha$ 為 factor, 通常設為 $0.75$，其作用是 increase the probability for less frequency words and decrease the probability for more frequent words 每個 word $w_i$ 都有個被 sample 出的 probability $P(w_i)$， 目的是從 $P(w)$ sample 出 $M$ 個 word 做為 negative 項 網路上常見的實現方法是調用 1np.random.multinomial(sample_size, pvals) 此法應該是透過 inverse CDF 來 sample word，每筆 training data 都調用一次的話運算效率不高 Word2Vec 作者 Tomas Mikolov 在他的 c code 中，採用了一種近似方式，其思想是在極大的抽樣次數下 $M = 1e8$，word 的 probability 越高代表其 frequency 越大，也就是在 M 中所占份額 shares 越多。 例如 yellow 的 probability 最大，理應在 M=30 中佔據較多的份額。 $P(\\text{blue}) = \\frac{2}{30}$ $P(\\text{green}) = \\frac{6}{30}$ $P(\\text{yellow}) = \\frac{10}{30}$ $P(\\text{red}) = \\frac{5}{30}$ $P(\\text{gray}) = \\frac{7}{30}$ 所以事先準備好一張 size 夠大的 table ($M = 1e8$)，根據 word frequency 給予相應的 shares ，真正要 sample word 的時候，只要從 $M$ 中 uniform random 出一個 index $m$ ， index $m$ 對應到的 word 就是被 sample 出的 word $\\tilde{w}$，是個以空間換取時間的做法。 Seeing is Believing做了一下測試 ，10000 次迭代，每次取 6 個 negatvie sample 的情景下，Tomas Mikolov 的近似思路比較有效率，而且是碾壓性的 但在 一次 sample 較多 word 的時候，multinomial 較有效率，可能 numpy 內部有做平行化的關係 Pytorch Skipgram with Negative SamplingNegative Sample1234567891011121314151617181920212223242526272829class NegativeSampler: def __init__(self, corpus, sample_ratio=0.75): self.sample_ratio = sample_ratio self.sample_table = self.__build_sample_table(corpus) self.table_size = len(self.sample_table) def __build_sample_table(self, corpus): counter = dict(Counter(list(itertools.chain.from_iterable(corpus)))) words = np.array(list(counter.keys())) probs = np.power(np.array(list(counter.values())), self.sample_ratio) normalizing_factor = probs.sum() probs = np.divide(probs, normalizing_factor) sample_table = [] table_size = 1e8 word_share_list = np.round(probs * table_size) ''' the higher prob, the more shares in sample_table ''' for w_idx, w_fre in enumerate(word_share_list): sample_table += [words[w_idx]] * int(w_fre)# sample_table = np.array(sample_table) // too slow return sample_table def generate(self, sample_size=6): negatvie_samples = [self.sample_table[idx] for idx in np.random.randint(0, self.table_size, sample_size)] return np.array(negatvie_samples) In: 123sampler = NegativeSampler(corpus)sampler.generate() Out: 12array(['visiting', 'defiled', 'thieves', 'beyond', 'lord', 'fill'], dtype='&lt;U18') Skipgram + NEG123456789101112131415161718192021222324class SkipGramNEG(nn.Module): def __init__(self, vocab_size, embedding_dim): super().__init__() self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.syn0 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K| self.neg_syn1 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K| torch.nn.init.constant_(self.neg_syn1.weight.data, val=0) def forward(self, center: torch.Tensor, context: torch.Tensor, negative_samples: torch.Tensor): # center : [b_size, 1] # context: [b_size, 1] # negative_sample: [b_size, negative_sample_num] embd_center = self.syn0(center) # [b_size, 1, embedding_dim] embd_context = self.neg_syn1(context) # [b_size, 1, embedding_dim] embd_negative_sample = self.neg_syn1(negative_samples) # [b_size, negative_sample_num, embedding_dim] prod_p = (embd_center * embd_context).sum(dim=1).squeeze() # [b_size] loss_p = F.logsigmoid(prod_p).mean() # 1 prod_n = (embd_center * embd_negative_sample).sum(dim=2) # [b_size, negative_sample_num] loss_n = F.logsigmoid(-prod_n).sum(dim=1).mean() # 1 return -(loss_p + loss_n) syn0 對應到 input side 的 matrix $W$ neg_syn1 對應到 output side 的 matrix $W’$ Tomas Mikolov 在 WordVec c code 初始化為 0 loss function loss_p 對應到 $\\log \\sigma(\\text{v}^\\top_{w_I} \\text{v}’_{w_{O,j}})$ loos_n 對應到 $\\sum^M_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}\\exp(\\text{v}^\\top_{w_I} \\text{v}’_{\\tilde{w}_i})$ Training Skipgram + Negative Sampling訓練過程省略，參閱 notebook seed9D/hands-on-machine-learning Evaluation取回 embedding簡單的把 syn0 跟 neg_syn1 平均 1234567syn0 = model.syn0.weight.dataneg_syn1 = model.neg_syn1.weight.dataw2v_embedding = (syn0 + neg_syn1) / 2w2v_embedding = w2v_embedding.numpy()l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)w2v_embedding = w2v_embedding / l2norm Cosine similarity123456789101112131415161718class CosineSimilarity: def __init__(self, word_embedding, idx_to_word_dict, word_to_idx_dict): self.word_embedding = word_embedding # normed already self.idx_to_word_dict = idx_to_word_dict self.word_to_idx_dict = word_to_idx_dict def get_synonym(self, word, topK=10): idx = self.word_to_idx_dict[word] embed = self.word_embedding[idx] cos_similairty = w2v_embedding @ embed topK_index = np.argsort(-cos_similairty)[:topK] pairs = [] for i in topK_index: w = self.idx_to_word_dict[i] pairs.append((w, cos_similairty[i])) return pairs 訓練語料是聖經，看看 jesus 跟 christ 的相近詞 In: 12cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)cosinSim.get_synonym('christ') Out: 12345678910[('christ', 1.0), ('jesus', 0.7170907), ('gospel', 0.4621805), ('peter', 0.39412546), ('disciples', 0.3873747), ('noise', 0.28152165), ('asleep', 0.26372147), ('taught', 0.2422184), ('zarhites', 0.24168596), ('nobles', 0.23950878)] In: 1cosinSim.get_synonym('jesus') out: 12345678910[('jesus', 1.0), ('christ', 0.7170907), ('gospel', 0.5360588), ('peter', 0.3603956), ('disciples', 0.3460646), ('church', 0.2755898), ('passed', 0.24744174), ('noise', 0.23768528), ('preach', 0.23454829), ('send', 0.2337867)] Reference https://github.com/tmikolov/word2vec c code word2vec的PyTorch实现 https://samaelchen.github.io/word2vec_pytorch/ CBOW + NEG https://rguigoures.github.io/word2vec_pytorch/ CBOW + NEG https://github.com/ilyakhov/pytorch-word2vec Word2Vec Tutorial Part 2 - Negative Sampling http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ 基于PyTorch实现word2vec模型 https://lonepatient.top/2019/01/18/Pytorch-word2vec.html other https://github.com/Adoni/word2vec_pytorch/blob/master/model.py medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4","link":"/Pytorch-Implement-Skipgram-with-Negative-Sampling/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/ckkyy83g1000foje2454l7b5j/"},{"title":"Word2Vec (2):Hierarchical Softmax 背後的數學","text":"以 CBOW 為例 $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ $\\text{v}’_j$ 表 $W’$ 中 j-th columns vector 在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是 leaf node ，跟 column vector $\\text{v}’_j$ 與 word $w_i$ 不是直接對應的關係 Hierarchical Softmax 優化了原始 softmax 分母 normalization 項需要對整個 vocabulary set 內的所有 word 內積，原始 softmax 如下 p(w | c) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_w})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$，is average of input context words’ vector representation in $W$ Hierarchical Softmax build a full binary tree to avoid computation over all vocabulary，示意圖： 每個 leaf node 代表一個 word $w_i$ Matrix $W^{‘}$ 就是所有 non-leaf node $n$ 代表的 vector $\\text{v}^{‘}_n$ 集合，與 word $w_i$ 無一對一對應關係 Binary tree 中每個 node 分岔的 probability 是個 binary classification problem $p(n, \\text{left}) = \\sigma({\\text{v}’_n}^{\\top} h)$ $p(n, \\text{righ}) = 1 - p(\\text{left},n) = \\sigma(-{\\text{v}’_n}^{\\top} h)$ $\\text{v}^{‘}_{n}$ 代表 node $n$ 的 vector 則每個 word $w_O = w_i$ 的 generate probability $p(w_i)$ 為其從 root node 分岔到 lead node 的 probability $p(w_i = w_O) = \\prod^{L(w_O)-1}_{j=1} \\sigma(\\mathbb{I}_{\\text{turn}}(n(w_O, j), n(w_O, j + 1) \\cdot {v^{‘}_{n(w_O, j)}}^{\\top}h)$ $w_O$ 表 output word 的意思 $L(w_O)$ is the depth of the path leading to the output word $w_O$ $\\mathbb{I}_{turn}$ is a specially indicator function 1 if $n(w_O, k+1)$ is the left child of $n(w_O, k)$ -1 if $n(w_O, k+1)$ is the right child of $n(w_O, k)$ $n(w, j)$ means the $j$ th unit on the path from root to the word $w$ $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$ average of all context word vector 簡單的例子Ex 1: Following the path from the root to leaf node of $w_2$, we can compute the probability of $w_2$ $p(w_2)$: Ex 2: $\\sum^{V}_{i=1} p(w_i = w_O) = 1$ probability $p(\\text{cat}| context)$, 是由 $ node1 \\stackrel{\\text{left}}{\\to} node \\stackrel{\\text{right}}{\\to} node 5 \\stackrel{\\text{right}}{\\to} cat $ 這條路徑組成 其中 context words 經過 hidden layer 後的輸出為 $h(\\text{context words})$ 為什麼 Hierarchical Softmax 可以減少 Time Complexity?透過 Hierarchical Softmax ， 原本計算 $p(w|c)$ 需要求所有 word $w_i$ 的 vector $\\text{v}_{w_i}$對 $h$ 的內積，現在只需要計算路徑上的節點數 $L(w_i) -1$，又 HS 是 full binary tree ，其最大深度為 $\\log_2|V|$ So we only need to evaluate at most $log_2|V|$ Hierarchical Softmax 如何 update 參數Error Funtion of Hierarchical SoftmaxError function $E$ is negative log likelihood $L(w_i) -1$ 表 從 root node 到 leaf node of $w_i$ 的 node number $[ \\cdot ]$表分岔判斷 $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$ average of all context word vector And we use gradient decent to update $\\text{v}^{‘}_j$ and $h$ in $W’$ and $W’$ Calculate the Derivate $E$ with Regard to $\\text{v}^{‘\\top}_jh$先求 total loss 對 $\\text{v}^{‘\\top}_jh$ 的 gradient $\\sigma^{‘}(x) = \\sigma(x)[1 - \\sigma(x)]$ $[\\log\\sigma(x)]^{‘} = 1 - \\sigma(x)$ ⇒ $[log(1 - \\sigma(x)]^{‘} = -\\sigma(x)$ Calculate the Derivate $E$ with Regard to $\\text{v}^{‘}_j$根據 chain rule 可以求出 total loss 對 huffman tree node vector $\\text{v}^{‘}_j$ 的 gradient Update Equation Calculate the Derivate $E$ with Regard to $h$ 最後求 total loss 對 hidden layer outpot $h$ 的 gradient $EH$: an N-dim vector, is the sum of the output vector of all words in the vocabulary, weighted by their prediction error Update EquationBecause hidden vector $h$ is composed with all the context word $w_{I,c}$ $\\text{v}_{w_{I,c}}$ is the input vector of c-th word in input context ImplementCBOW + HS 實現 Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax Reference Language Models, Word2Vec, and Efficient Softmax Approximations https://rohanvarma.me/Word2Vec/ Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722","link":"/hierarchical-softmax-in-word2vec/"},{"title":"Word2Vec (1):NLP Language Model","text":"General Form p(w_1 , \\cdots , w_T) = \\prod\\limits_i p(w_i \\: | \\: w_{i-1} , \\cdots , w_1)展開來後 $P(w_1, w_2, w_3,…,w_T) = P(w_1)P(x_2|w_1)P(w_3|w_2, w_1)…P(w_T|w_1,…w_{T-1})$ EX Ngram Model根據 Markov assumption， word $i$ 只跟其包含 $i$ 的連續 $n$ 個 word 有關, 即前面 $n -1$ 個 word p(w_1 , \\cdots , w_T) = \\prod\\limits_i p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-n+1})其中 conditional probability 統計 corpus 內的 word frequency，可以看到 ngram 只跟前面 $n-1$ 個 word 有關 p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-n+1}) = \\dfrac{count(w_{i-n+1}, \\cdots , w_{i-1},w_o)}{count({w_{i-n+1}, \\cdots , w_{i-1}})}如果 $k=2$ 則稱為 bigram model : p(w_i|w_1, w_2, ... w_{i-1}) \\approx p(w_i|w_{i-1})最簡單的實作直接統計 corpus 內的 word frequency 得到 conditional probability: 但通常泛化性不足，所以用 neural network 與 softmax 來泛化 n gram conditional probability $p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-i+1})$ Neural Network ImplementationIn neural network, we achieve the same objective using the softmax layer $p(w_t \\: | \\: w_{t-1} , \\cdots , w_{t-n+1}) = \\dfrac{\\text{exp}({h^\\top v’_{w_t}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top v’_{w_i}})}$ $h$ is the output vector of the penultimate network layer $v^{‘}_{w}$ is the output embedding of word $w$ the inner product $h^\\top v’_{w_t}$ computes the unnormalized log probability of word $w_t$ the denominator normalizes log probability by sum of the log-probabilities of all word in $V$ Implement Ngram model with PytorchCreating Corpus and Training Pairs1234567891011121314151617181920212223import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimtest_sentence = &quot;&quot;&quot;When forty winters shall besiege thy brow,And dig deep trenches in thy beauty's field,Thy youth's proud livery so gazed on now,Will be a totter'd weed of small worth held:Then being asked, where all thy beauty lies,Where all the treasure of thy lusty days;To say, within thine own deep sunken eyes,Were an all-eating shame, and thriftless praise.How much more praise deserv'd thy beauty's use,If thou couldst answer 'This fair child of mineShall sum my count, and make my old excuse,'Proving his beauty by succession thine!This were to be new made when thou art old,And see thy blood warm when thou feel'st it cold.&quot;&quot;&quot;.split()trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)]vocab = set(test_sentence)word_to_idx = {word: i for i, word in enumerate(vocab)} Define N Gram Model123456789101112131415class NGramLanguageModel(nn.Module): def __init__(self, vocab_size, embedding_dim, context_size): super(NGramLanguageModel, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(context_size * embedding_dim, 128) self.linear2 = nn.Linear(128, vocab_size) def forward(self, inputs): embeds = self.embeddings(inputs).view(1, -1) out = F.relu(self.linear1(embeds)) out = self.linear2(out) log_probs = F.log_softmax(out, dim=1) return log_probs Training1234567891011121314151617181920CONTEXT_SIZE = 2EMBEDDING_DIM = 10loss_function = nn.NLLLoss()net = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)optimizer = optim.SGD(net.parameters(), lr=0.001)losses = []for epoch in range(10): total_loss = 0 for context, target in trigrams: context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long) net.zero_grad() log_probs = net(context_idxs) loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long)) loss.backward() optimizer.step() total_loss += loss.data print(&quot;epcoh {} loss {}&quot;.format(epoch, total_loss)) losses.append(total_loss) Fetch Embedding1emb = net.embeddings(torch.tensor([i for i in range(len(vocab))])).detach().numpy() Reference on word embeddings https://ruder.io/word-embeddings-1/ https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html","link":"/NLP-language-model/"},{"title":"Word2Vec (3):Negative Sampling 背後的數學","text":"以下用 Skip-gram 為例 $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ Noise Contrastive Estimation (NCE) NCE attempts to approximately maximize the log probability of the softmax output The Noise Contractive Estimation metric intends to differentiate the target word from noise sample using a logistic regression classifier 從 cross entropy 說起 True label $y_i$ is 1 only when $w_i$ is the output word: \\mathcal{L}_\\theta = - \\sum_{i=1}^V y_i \\log p(w_i | w_I) = - \\log p(w_O \\vert w_I)又 $p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ $\\text{v}^{‘}_{w_i}$ is vector of word $w_i$ in $W^{‘}$ $w_O$ is the output word in $V$ $w_I$ is the input word in $V$ 代入後 \\mathcal{L}_{\\theta} = - \\log \\frac{\\exp({\\text{v}'_{w_O}}^{\\top}{\\text{v}_{w_I}})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top}{\\text{v}_{w_I} })} = - {\\text{v}'_{w_O}}^{\\top}{\\text{v}_{w_I} } + \\log \\sum_{i=1}^V \\exp({\\text{v}'_{w_i} }^{\\top}{\\text{v}_{w_I}})Compute gradient of loss function w.s.t mode’s parameter $\\theta$，令 $z_{IO} = {\\text{v}’_{w_O}}^{\\top}{\\text{v}_{w_I}}$ ; $z_{Ii} = {\\text{v}’_{w_i}}^{\\top}{\\text{v}_{w_I}}$ \\begin{aligned} \\nabla_\\theta \\mathcal{L}_{\\theta} &= \\nabla_\\theta\\big( - z_{IO} + \\log \\sum_{i=1}^V e^{z_{Ii}} \\big) \\\\ &= - \\nabla_\\theta z_{IO} + \\nabla_\\theta \\big( \\log \\sum_{i=1}^V e^{z_{Ii}} \\big) \\\\ &= - \\nabla_\\theta z_{IO} + \\frac{1}{\\sum_{i=1}^V e^{z_{Ii}}} \\sum_{i=1}^V e^{z_{Ii}} \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\sum_{i=1}^V \\frac{e^{z_{Ii}}}{\\sum_{i=1}^V e^{z_{Ii}}} \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\sum_{i=1}^V p(w_i \\vert w_I) \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\mathbb{E}_{w_i \\sim Q(\\tilde{w})} \\nabla_\\theta z_{Ii} \\end{aligned}可以看出，gradient $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$是由兩部分組成 : a positive reinforcement for the target word $w_O$, $\\nabla_{\\theta}z_{O}$ a negative reinforcement for all other words $w_i$, which weighted by their probability, $\\sum_{i=1}^V p(w_i \\vert w_I) \\nabla_\\theta z_{Ii}$ Second term actually is just the expectation of the gradient $\\nabla_{\\theta}z_{Ii}$ for all words $w_i$ in $V$。 And probability distribution $Q(\\tilde{w})$ could see as the distribution of noise samples NCE sample 原理According to gradient of loss function $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$ \\nabla_{\\theta}\\mathcal{L}_{\\theta} =- \\nabla_\\theta z_{IO} + \\mathbb{E}_{w_i \\sim Q(\\tilde{w})} \\nabla_\\theta z_{Ii}Given an input word $w_I$, the correct output word is known as $w$，我們同時可以從 noise sample distribution $Q$ sample 出 $M$ 個 samples $\\tilde{w}_1, \\tilde{w}_2, \\dots, \\tilde{w}_M \\sim Q$ 來近似 cross entropy gradient 的後半部分 現在我們手上有個 correct output word $w$ 和 $M$ 個從 $Q$ sample 出來的 word $\\tilde{w}$ ， 假設我們有一個 binary classifier 負責告訴我們，手上的 word $w$，哪個是 correct $p(d=1 | w, w_I)$，哪個是 negative $p(d=0 | \\tilde{w}, w_I)$ 於是 loss function 改寫成： \\mathcal{L}_\\theta = - [ \\log p(d=1 \\vert w, w_I) + \\sum_{i=1, \\tilde{w}_i \\sim Q}^M \\log p(d=0|\\tilde{w}_i, w_I) ]According to the law of large numbers $E_{p(x)} [ f(x)] \\approx \\frac{1}{n} \\sum^{n}_{i=1}f(x_i)$，we could simplify: \\mathcal{L}_\\theta = - [ \\log p(d=1 \\vert w, w_I) + M \\mathbb{E}_{\\tilde{w}_i \\sim Q} \\log p(d=0|\\tilde{w}_i, w_I)]$p(d |w, w_I)$ 可以從 joint probability $p(d, w|w_I)$ 求 $p(d, w | w_I) =\\begin{cases} \\frac{1}{M+1} p(w \\vert w_I) &amp; \\text{if } d=1 \\\\ \\frac{M}{M+1} q(\\tilde{w}) &amp; \\text{if } d=0 \\end{cases}$ $d$ is binary value $M$ is number of samples，we have a correct output word $w$ and sample M word from distribution $Q$ 因爲 $p(d| w, w_I) = \\frac{p(d, w, w_I)}{p(w, w_I)} = \\frac{p(d, w | w_I) p(w_I)}{p(w|w_I)p(w_I)} = \\frac{p(d, w| w_I)}{\\sum_dp(d,w| w_I)}$ 可以得出 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(d=1, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned} \\begin{aligned} p(d=0 \\vert w, w_I) &= \\frac{p(d=0, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{Mq(\\tilde{w})}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned} $q(\\tilde{w})$ 表從 distribution $Q$ sample 出 word $\\tilde{w}$ 的 probability 最終 loss function of NCE \\begin{aligned} \\mathcal{L}_\\theta & = - [ \\log p(d=1 \\vert w, w_I) + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log p(d=0|\\tilde{w}_i, w_I)] \\\\ & = - [ \\log \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{p(w \\vert w_I) + Mq(\\tilde{w}_i)}] \\end{aligned}$p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 代入 $\\mathcal{L}_{\\theta}$ \\begin{aligned}\\mathcal{L}_{\\theta} &= -[log\\frac{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})}}{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})+ Mq(\\tilde{w})}} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i\\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})}+ Mq(\\tilde{w}_i)}]\\end{aligned}可以看到 normalizer $Z(w) = {\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 依然要 sum up the entire vocabulary，實務上通常會假設 $Z(w)\\approx1$，所以 $\\mathcal{L}_\\theta$ 簡化成: \\mathcal{L}_\\theta = - [ \\log \\frac{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}})}{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}}) + Mq(\\tilde{w})} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}}) + Mq(\\tilde{w}_i)}]關於 Noise distribution $Q$關於 noise distribution $Q$，在設計的時候通常會考慮 it should intuitively be very similar to the real data distribution. it should be easy to sample from. Negative Sampling (NEG)Negative sampling can be seen as an approximation to NCE Noise Contrastive Estimation attempts to approximately maximize the log probability of the softmax output The objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modeling 從 NCE 說起NCE $p(d|w, w_I)$ equation，$p(d=1 |w, w_I)$ 代表 word $w$ 是 output word 的機率; $p(d=0|w, w_I)$ 表 sample 出 noise word 的機率 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(d=1, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned}\\begin{aligned} p(d=0 \\vert w, w_I) &= \\frac{p(d=0, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{Mq(\\tilde{w})}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned}NCE 假設 $p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 中的分母 $Z(w) = {\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})} = 1$，所以簡化成 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} &= \\frac{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})}{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) + Mq(\\tilde{w})} \\end{aligned}NEG 繼續化簡NEG 繼續假設 $Nq(\\tilde{w}) = 1$ 式子變成 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})}{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) + 1} &= \\frac{1}{1 +\\exp(-{\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})} = \\sigma({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) \\end{aligned}p(d=0|w, w_I) = 1 - \\sigma({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) = \\sigma(-{\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})最終得到 loss function \\mathcal{L}_\\theta = - [ \\log \\sigma({\\text{v}'_{w}}^\\top \\text{v}_{w_I}) + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\sigma(-{\\text{v}'_{\\tilde{w}_i}}^\\top \\text{v}_{w_I})]前項是 positive sample $p(d=1 \\vert w, w_I)$，後項是 M 個 negative samples $p(d=0|w, w_I) $ 在 skipgram with negative sampling 上 $\\text{v}_{w_I}$ 表 input 的 center word $w_I$ 的 vector，來自 $W$ $\\text{v}’_{w}$ 表 output side 的一個 context word $w$ 的 vector， 來自 $W’$ 實作上 skipgram 一個單位的 data sample 只會包含 一個 center word 與 一個 context word 的 pair 對 $(w_I, w_{C,j})$ 參閱 Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling 結論 NEG 實際上目的跟 Hierarchical Softmax 不一樣，並不直接學 likelihood of correct word，而是直接學 words 的 embedding，也就是更好的 word distribution representation Reference Learning Word Embedding https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions 此篇從 skip gram 講解 negative sampling On word embeddings - Part 2: Approximating the Softmax https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches 此篇從 CBOW 講解 negative sampling Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722 Word2Vec Tutorial Part 2 - Negative Sampling [http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/","link":"/negative-sampling-in-word2vec/"},{"title":"內容推薦 (2) Title Embedding with Keyword","text":"前言在前篇 內容推薦 (1) 關鍵詞識別 中，我們利用 entropy 從商品池的 title 中辨識出 product word &amp; label word 此篇，我們將利用已經辨識出的 product word &amp; label word 回頭對商品池中的商品 title 做 embedding 當然你也可以直接將所有 title 送進 Word2Vec 硬 train 一發，然後對 title 內的所有的 word vectors 取平均得到 title vector。 推薦系統相關文章 Weight Keyword Embedding假設我們有一個 title ，我們希望能根據 word 在 title 中的重要程度將他 embedding 化，要怎麼做？ 1'summer fisherman hat female outdoor sun hat sun hat japanese student basin hat watch travel fishing sun hat male' 從 CBOW 說起CBOW 的思想是用兩側 context words 去預測中間的 center word P(center|context;\\theta)換句話說，給定 context words 集合 $w_{I,C}$， word $w_j$ 是 center word $w_O$ 的 probability 越大 是否代表 $w_j$ 在 context $C$ 中越關鍵？ P(w_O = w_j |w_{I,C};\\theta)如果上面的推測成立的話，CBOW 在 Hierarchical Softmax 下的 objective function: negative log likelihood \\begin{aligned} & -\\log p(w_O| w_I) = -\\log \\dfrac{\\text{exp}({h^\\top \\text{v}'_O})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} \\\\ & = - \\sum^{L(w_O)-1}_{l=1} \\log\\sigma([ \\cdot] h^\\top \\text{v}^{'}_l) \\end{aligned} \\tag{1} CBOW with Hierarchical Softmax 有兩個 matrix $W$ and $W’$ $W$ 的 row vector 對應到 word $w_i$ 的 vector $W’$ 對應的是 huffman tree non-leaf node 的 vector 參見 Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax $\\text{v}’_j$ 表 output side matrix $W’$ 中 j-th columns vector，跟任何 word 沒一對一對應關係 $L(w_i) -1$ 表 huffman tree 中從 root node 到 leaf node of $w_i$ 的 node number $[ \\cdot ]$表 huffman tree 的分岔判斷 $[ \\cdot ] = 1$ 表 turn left $[ \\cdot ] = -1$ 表 turn right $h = \\frac {1}{C} \\sum^{C}_{j=1}\\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$ Score function $\\log p(w_O| w_I)$ (沒負號)， 本質上是對 output word $w_O$ 的打分。 先改寫一下 score function，等等會用到 \\begin{aligned} \\log p(w_O| w_I) &= \\sum^{L(w_O)-1}_{l=1}\\log\\sigma([ \\cdot] h^\\top \\text{v}^{'}_l) \\\\ &= \\sum^{L(w_O)-1}_{l=1} \\log(\\cfrac{1}{1+ \\exp^{- [ \\cdot] h^{\\top} v_l^{'}}}) \\\\ &= \\sum^{L(w_O)-1}_{l=1} [\\log(1) -\\log(1+ \\exp^{- [ \\cdot] h^{\\top} v_l^{'}}) ]\\\\ & = \\sum^{L(w_O)-1}_{l=1}-\\log(1 + \\exp^{- [ \\cdot] h^{\\top} v_l^{'}}) \\end{aligned} \\tag{2}有了 式(2) score function ，給定一 title words 集合 $w_{T}$ 只要對 title 裡的每個 word $w_j \\in w_{T}$ ，令 $\\log p(w_O=w_j|w_I = w_{T, \\lnot j})$，進行打分即可得到每個 word $w_j$ 在 title 裡的重要程度 \\text{weight}_j = \\log p(w_O=w_j|w_I = w_{T, \\lnot j}) \\tag{3}而我們要的 title embedding 即 weighted sum of words in title \\text{v}_{\\text{title}} = \\sum_{w_j \\in w_T}\\text{weight}_j \\times \\text{v}_{w_{j}} \\tag{4} $w_T$: 某 title 的 word 集合 $\\text{v}_{w_{j}}$: word $w_j$ 對應 matrix $W$ 中 row vector Gensim 實作Train CBOW + HS12345678910111213141516171819from gensim.models import Word2Vecw2v_model = Word2Vec( min_count=3, window=5, size=100, alpha=0.005, min_alpha=0.0007, hs=1, sg=0, workers=4, batch_words=100, cbow_mean = 1 )w2v_model.build_vocab(corpus) # build huffman treew2v_model.train( corpus, total_examples=w2v_model.corpus_count, epochs=50, report_delay=1) corpus 裡的每個 title，應該已經先行合併 bigram and trigram 的 product &amp; label 詞 ，最好可以去除無用詞，如下 sentence，某些 words 已合併 : 1'hyuna_style ins cute wild sunscreen female hand sleeves arm_guard ice_silk sleeves driving anti_ultraviolet ice gloves tide' Scoring Words in Title訓練完 model 後，對每個 title 內的 words 重要性進行評分: 12345def cal_log_probs(model, target_w, context_embd: np.ndarray)-&gt; np.ndarray: turns = (-1.0) ** target_w.code path_embd = model.trainables.syn1[target_w.point] log_probs = -np.logaddexp(0, -turns * np.dot(context_embd, path_embd.T)) return np.sum(log_probs) 為 式 (2) 的實現，實際上就是 gensim Word2Vec 內的 score_cbow_pair word 在 huffman tree 的 path code 是 0/1 code，使用時須轉換成 -1 or 1 model.trainables.syn1 即 $W’$ ，存放 huffman tree non-leaf node 的 vector 12345678910111213141516171819202122def _cal_keyword_score(model, sentence:List[str]) -&gt; Dict[str, float]: word_vocabs = [model.wv.vocab[w] for w in sentence if w in model.wv.vocab] word_importance = {} for pos_center, center_w in enumerate(word_vocabs): context_w_indices = [w.index for pos_w, w in enumerate(word_vocabs) if pos_center != pos_w] context_embed = np.mean(model.wv.vectors[context_w_indices], axis=0) log_probs = cal_log_probs(model, center_w, context_embed) center_w_term = w2v_model.wv.index2word[center_w.index] word_importance[center_w_term] = word_importance.get(center_w_term, 0) + log_probs return word_importancedef cal_keyword_score(model, sentence: List[str]) -&gt; np.ndarray: word_importance = _cal_keyword_score(model, sentence) ds = pd.Series(word_importance).sort_values(ascending=False) scalar = MinMaxScaler(feature_range=(0.1, 1)) array = ds.to_numpy() array = scalar.fit_transform(array.reshape(array.shape[0], -1)) ds = pd.Series(array.reshape(-1, ), index=ds.index) return ds model.wv.vectors 存放 $W$，即訓練完後每個 word 的 vector MinMaxScaler: 縮放到 0.1 到 1 是為了方便觀察 使用方式如下 In: 123sent = corpus_with_bigram_trigram[7676]ds = cal_keyword_score(w2v_model, sent)print(sent), print(ds) Out: 123456789101112131415['haining', 'leather', 'male', 'stand_collar', 'middle_aged', 'coat', 'fur', 'one', 'winter', 'cashmere', 'thick', 'money', 'father_loaded']coat 1.000000leather 0.874738fur 0.861750middle_aged 0.812752winter 0.773609male 0.734654stand_collar 0.699505thick 0.676800cashmere 0.642869one 0.546631haining 0.457806father_loaded 0.393533money 0.100000dtype: float64 Weighted Sum of Word Vectors從 w2v_model 中取出某 title 內所有 words 的 vector 做 weighed sum 123456def weighted_sum_w2v(w2v_model, ds: pd.Series) -&gt; np.ndarray: ds_ = ds.copy() / sum(ds) w2v = w2v_model.wv[ds_.index] weights = np.expand_dims(ds_.to_numpy(), 1) return np.sum((w2v * weights), axis=0) 得出的 title vector is un-normalized ，要使用前得先 L2-norm 參閱 bible 做的示例 notebook seed9D/hands-on-machine-learning Title Embedding 應用title embedding 最直覺的應用是 content I2I，用戶點擊了 商品 A，我們就可以透過 商品 A 的 title vector 召回 TopK 個最相似 title 的商品推薦給他。 而 title embedding with keyword weighting 中，我們將 product word 與 label word 在 title 中的重要程度進行 weighted sum，能更準確的表達 title 的意思，不再只是簡單的對 word vector 取平均，連一些無用詞的 vector 也混進去。 不過 title embedding 在推薦的效果不如利用用戶交互數據訓練出來的 embedding，但因爲每個商品一定會有 title， 很適合作為商品冷啟動召回策略之一使用。在我負責的推薦應用裡，也是利用 title embedding 關聯新商品到有交互數據的舊商品上後讓新商品取得曝光機會。 title embedding 結合 label 詞 &amp; product 詞的另一個業務應用就是卡片式的主題推薦，類似淘寶上的一個頁面就講一個購物主題，選定一個主題 (ex: 旅遊)與某樣你曾經互動過商品進行推薦 這個算法側的實作不難，留到下次說吧 Reference 【不可思议的Word2Vec】 3.提取关键词 https://spaces.ac.cn/archives/4316 以 skipgram 角度計算 my post Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax Word2Vec (2):Hierarchical Softmax 背後的數學 gensim https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html Gensim Word2Vec Tutorial [https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial","link":"/title-embedding-with-keywords/"},{"title":"Word2Vec (0):從原理到實現","text":"這篇是在 notion 整理的筆記大綱，只提供綱要性的說明 預備知識 language model： NLP 語言模型 參閱 Word2Vec (1):NLP Language Model huffman tree 簡介兩種網路結構 Continuous bag of words (CBOW) &amp; Softmax CBOW feeds $n$ words around the target word $w_t$ at each step P(center|context;\\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ CBOW 的 Objective Function$J_\\theta = \\frac{1}{V}\\sum\\limits_{t=1}^V\\ \\text{log} \\space p(w_t \\: | \\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n})$ 其中 p(w_t \\: | \\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n}) = \\cfrac{\\exp(h^\\top \\text{v}^{'}_{w_{t}})}{\\sum_{w_i \\in V}\\exp(h^\\top \\text{v}'_{w_i})} $n$ 表 window size $w_t$ 表 CBOW target center word $w_i$ 表 word $i$ in vocabulary $V$ $\\text{v}_{w_i}$ 表 matrix $W$ 中，代表 $w_i$ 的那個 row vector $\\text{v}’_{w_i}$ 表 matrix $W’$ 中，代表 $w_i$ 的那個 column vector $h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\\cfrac{1}{C}(\\text{v}_{w_1} + \\text{v}_{w_2}+ …+ \\text{v}_{w_C})^T$ Because there are multiple contextual words, we construct the input vector by averaging each words’s distribution representation Skipgram &amp; Softmax skipgram uses the centre word to predict the surrounding words instead of using the surrounding words to predict the centre word P(context|center; \\theta) Skipgram 的 Objective Function J_\\theta = \\frac{1}{V}\\sum\\limits_{t=1}^V\\ \\sum\\limits_{-n \\leq j \\leq n , \\neq 0} \\text{log} \\space p(w_{t+j} \\: | \\: w_t)其中 p(w_{t+j} \\: | \\: w_t ) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_{w_{t+j}}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} $n$ 為 window size $w_{t+j}$ 表 skipgram target 第 j 個 context word $w_t$ 為 skipgram input 的 center word skip-gram 有兩個獨立的 embedding matrix $W$ and $W^{‘}$ $W$ : $V \\times N$ , $V$ is vocabulary size; N is vector dimension output matrix $W^{‘}$: $N \\times V$, encoding the meaning of context $\\text{v}^{‘}_{w_i}$ is column vector of word $w_i$ in $Ｗ^{‘}$ $h$ is the hidden layer’s output 事實上，skip-gram 沒有 hidden layer, 因爲 input 只有一個 word, $h$ 就是 word embedding $\\text{v}_{w_t}$of the word $w_t$ in $W$。 所以 $p(w_{t+j} \\: | \\: w_t ) = \\dfrac{\\text{exp}({\\text{v}^\\top_{w_t} \\text{v}’_{w_{t+j}}})}{\\sum_{w_i \\in V} \\text{exp}({\\text{v}^\\top_{w_t} \\text{v}’_{w_i}})}$ 兩種 loss function 優化原始 softmax 作為 objective function， 分母必須對所有 word $w_i$ in vocabulary 與 vector $ h$ 內積，造成運算瓶頸 p(w_O | w_I) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_{w_{O}}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})}所以 word2Vec 論文中採用兩種 objective function 的優化方案，Hierarchical Softmax 與 Negatvie Sampling Hierarchical Softmax原理推導請參閱 Word2Vec (2):Hierarchical Softmax 背後的數學 Hierarchical softmax build a full binary tree to avoid computation over all vocabulary Negative Sampling原理推導請參閱 Word2Vec (3):Negative Sampling 背後的數學 negative sampling did further simplification because it focuses on learning high-quality word embedding rather than modeling the likelihood of correct word in natural language. In implement，negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them 實現 WordVec skip gram + softmax Word2Vec (4):Pytorch 實作 Word2Vec with Softmax CBOW + softmax Word2Vec (4):Pytorch 實作 Word2Vec with Softmax CBOW + hierarchical softmax Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax CBOW + negatove sampling skip gram + hierarchical softmax skip gram + negative sampling Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling ConclusionSkip gram 與 CBOW 實際上都 train 了兩個 embedding matrix $W$ and $W’$ $W:$ 在 C implement 稱作 $syn0$。 $W’$: 若採用 hierarchical softmax 稱為 $syn1$ 若採用 negative sampling 叫 $syn1neg$ 根據性質，$syn0$ 與 $syn1neg$ 是一體兩面的，本身都可以代表 word embedding，皆可使用。 而代表 $W’$ 的 $syn1$ 因爲連結的是 huffman tree 的 non leaf node，所以其本身對 word $w_i$ 沒直接意義。 Reference On word embeddings - Part 1 https://ruder.io/word-embeddings-1/ On word embeddings - Part 2: Approximating the Softmax https://ruder.io/word-embeddings-softmax/ Learning Word Embedding https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy other Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from http://arxiv.org/abs/1411.2738 Language Models, Word2Vec, and Efficient Softmax Approximations https://rohanvarma.me/Word2Vec/ Word2Vec Tutorial - The Skip-Gram Model word2vec原理(一) CBOW与Skip-Gram模型基础 The Illustrated Word2vec Word2vec数学原理全家桶 http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ Word2Vec-知其然知其所以然 https://www.zybuluo.com/Dounm/note/591752#word2vec-知其然知其所以然 C source code Word2Vec源码解析 https://www.cnblogs.com/neopenx/p/4571996.html 應用 小白看Word2Vec的正确打开姿势|全部理解和应用 https://zhuanlan.zhihu.com/p/120148300 推荐系统从零单排系列(六)—Word2Vec优化策略层次Softmax与负采样 [https://zhuanlan.zhihu.com/p/66417229","link":"/word2vec-from-theory-2-implement/"},{"title":"一步步透視 GBDT Classifier","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x)$ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ GBDT classifier 常用的 loss function 為 cross entropy classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$ 跟 $probability$ 有關，得將 $F(x)$ 通過 $\\textit{sigmoid function }$ 獲得 probability $p = \\sigma(F(x))$ GBDT 簡介在 一步步透視 GBDT Regression Tree 直接進入正題吧 GBDT Algorithm - step by stepGBDT classification tree algorithm 跟 regression tree 並無不同 Input Dat and Loss Function Input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data target $y_i$: who loves Troll2 features of $x_i$: “likes popcorn”, “Age”, “favorite” Our goal is using $x_i$ to predict someone like Trolls 2 or not loss function 為 cross entropy \\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​值得注意的是，GBDT - classifier $F(x)$ 輸出的是 $log(odds)$ 而不是 $probability$ 要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\\textit{sigmoide function}$ \\textit{The probability of Loving Troll 2 } = \\sigma(F(x)) = p $log(odds)$ 轉換成 $probability$ 公式 p = \\cfrac{e^{\\log(odds)}}{1 + e^{\\log(odds)}} Step 1 Initial model with a constant value $F_0(X)$ 初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可 計算完，得到 $F_0(x) = 0.69$，每個 data point 的初始 prediction 都一樣就是 $F_0(x)$。 $F_0(x)$ 是 $\\log(odds)$ 若要計算 probability of loving Troll 2 呢？ $\\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。 ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$ ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving Troll 2 Step2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At Epoch m = 1(A) Calculate Residuals of $F_{0}(x)$classification 問題中 residual 為 predicted probability 與 observed label $y$ 之間的差距 $residual = observed - \\textit{predicted probability}$ true label 為 1 false label 為 0 注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$ 輸出的是 $log(odds)$ 計算各 data sample 的 residual 後： ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$ ep_0_prob 表 $F_0(x)$ predicted probability，$\\sigma(F_0(x))$ ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$ 建樹為一般 fit regression tree 的過程，criterion 為 mean square error，假設找到的樹結構為 可以看到綠色為 leaf node，所有的 data sample $x$ 都被歸到特定 leaf node 下 ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下 \\cfrac{\\sum residual_i}{\\sum [\\textit{previous probability} \\times \\textit{(1 - previous probability)}]} 分子是 each leaf node 下的 data sample $x$ 的 residual 和 分母的 previous probability 為 $m -1$ 步 GBDT 輸出的 probability $p = \\sigma(F(x))$ 。在這個 epoch 是指 $F_0(x)$ 經過計算後，每個 leaf node 輸出 ep_0_prob 表 $\\sigma(F_0(x))$ 計算出的 probability of loving Troll2 ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob ep_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值 (D) update $F_1(x)$ with new tree $f_1(x)$現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 F_1(x) = F_0(x) + \\textit{learning rate} \\times f_1(x)輸出的 probability 為 $\\sigma(F_1(x))$ 令 $\\textit{learnign rate = 0.8}$，得到 epoch 2 每個 data sample 的 $\\log(odds)$ prediction 與 probability prediction ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$ ep_1_prob 為 $F_1(x)$ 輸出的 probability $\\sigma(F_1(x))$ At Epoch m = 2(A) Calculate Residuals of $F_1(x)$計算上一步 $\\textit{residual of } F_1(X)$ residual = observed - \\textit{predicted probability} ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\\sigma(F_1(x))$ ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$ (B) Construct New Regression Tree $f_2(x)$用 data sample x 的 columns “like_popcor”, “age”, “favorite_color” 擬合 ep_2_residual build a new tree $f_2(x)$ 假設得到 $f_2(x)$ 的樹結構： 每個 data sample 對應的 leaf index ep_2_leaf_index 表 data sample 對應到 $f_2(x)$ 上的 leaf node index (D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$計算 $f_2(x)$ 下每個 leaf node 的輸出: 對應到 data sample 上: ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node 下會有一樣的值 Update $F_2(x)$ with New Tree $f_2(x)$到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為 $F_2(x) = F_0(x) + \\nu(f_1(x) + f_2(x))$ $\\nu$ 為 learning rate，假設為 0.8 GBDT 輸出的 probability 為 $\\sigma(F_2(x))$，計算 epoch 2 的 prediction of probability of loving troll2: love_toll2: our target ep_0_pre 表 $F_0(x)$ ep_1_leaf_output 表 data sample x​ 在第一顆樹 $f_1(x)$ 的輸出值 ep_2_leaf_output 表 data sample x 在第二顆樹 $f_2(x)$ 的輸出值 ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$ ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\\sigma(F_2(x))$ Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ 把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 的每棵樹 $f_m(x)$ 都是去 fit $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$ residual = observed - \\textit{predicted probability}所以 $F_m(x)$ 又可以寫成 F_m(x) = F_{m-1}(x) + \\nu f_m(x)這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住： $F_m(x)$ 輸出 $log(odds)$ $residual$ 的計算與 probability 有關 GBDT Classifier 背後的數學Q: 為什麼用 cross entropy 做為 loss function ?在分類問題上，我們預測的是 $\\textit{The probability of loving Troll 2}$ $P(Y|x)$，$\\textit{}$ 以 $maximize$ $\\textit{log likelihood}$ 來解 $P(Y|x)$。 令 GBDT - classification tree 的 probability prediction 為 $P(Y| x) = \\sigma(F(x))$，則 objective function 為 \\text{log (likelihood of the obersved data given the prediction) } \\\\= \\sum_{i=1}^N [y_i \\log(p) + (1-y_i)\\log(1-p)] $p = P(Y=1|x)$，表 the probability of loving movie Troll 2 $y_i$ : observation of data sample $x_i$ loving Troll 2 or not $y \\in \\{1, 0\\}$ 而 $\\text{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成 \\text{objective function} = - \\sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)所以 $\\text{loss function} = -[y \\log(p) + (1-y)\\log(1-p)]$ 把 loss function 用 $odds$ 表示： \\begin{aligned} -[y \\log(p) + (1-y)\\log(1-p)] & = -y\\log(p)-(1-y)\\log(1-p) \\\\ &= -y\\log(p)-\\log(1-p) + y\\log(1-p) \\\\ &= -y[\\log(p) - \\log(1-p)] - \\log(1-p) \\\\ & = -y\\log(odds) - \\log(1-p) \\\\ &= -y\\log(odds) + \\log(1 + \\exp(\\log(odds))) \\end{aligned} 第三個等號 到 第四個等號用到 $odds=\\cfrac{p}{1-p}$ 第四個等號 到 第五個等號用到 $p = \\cfrac{\\exp(\\log(odds))}{1 + \\exp(\\log(odds))}$ 這個結論 $\\log(1-p) = \\log(1- \\cfrac{\\exp(\\log(odds))}{1 + \\exp(\\log(odds))}) = \\log(\\cfrac{1}{1 + \\exp(\\log(odds))}) = -\\log(1 + \\exp(\\log(odds)))$ 把 loss function 表示成 odds 的好處是， $ -y\\log(odds) + \\log(1 + \\exp(\\log(odds)))$ 對 $log(odds)$ 微分形式很簡潔 \\cfrac{d}{d \\ log(odds)} -ylog(odds) + log(1 + \\exp^{log(odds)}) = -y -\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}} = -y + ploss function 對 $log(odds)$ 的微分，既可以以 $\\log(odds)$ 表示，也可以以 probability $p$ 表示 以 $log(odds)$ 表示： $\\cfrac{d}{d \\log(odds)}L(y_i, p) = -y -\\cfrac{\\exp(\\log(odds))}{1 + \\exp(\\log(odds))}$ 以 $p$ 表示：$\\cfrac{d}{d \\log(odds)}L(y_i, p) = -y + p$ 用 $p$ 表示時，loss function 對 $log(odds)$ 的微分 -y + p = \\text{ -(observed - predicted) = negative residual}Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\\cfrac{count(true)}{count(false)})$ ? 來自 Step 1 的問題 根據選定的 loss function \\text{loss function} = -[y \\log(p) + (1-y)\\log(1-p)] $P(Y=1|x) = p$ 為出現正類的 probability $y \\in \\{1, 0\\}$ 將 loss function 以 $\\log(odds)$ 表示 -[y \\log(p) + (1-y)\\log(1-p)] = -[y\\log(odds) + \\log(1 + \\exp(log(odds)))]$F_0(x)$ 為能使 $\\textit{cost function}$ 最小的 $\\log(odds): \\gamma$ F_0(x) = argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma) = argmin_\\gamma \\sum^n_{i=1} -[y_i\\log(odds) + \\log(1 + \\exp(\\log(odds)))] $n$ 為 number of data sample $x$ 令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$ cost function 對 $log(odds)$ 微分取極值： \\begin{aligned}& \\cfrac{d}{d \\log(odds)}\\sum^n_{i=1} -[y_i\\log(odds) + \\log(1 + \\exp(log(odds)))] \\\\ & = \\cfrac{d}{d \\log(odds)}\\sum^{n^{(1)}}_i -(\\log(odds) + \\log(1 + exp(log(odds)))) - \\sum^{n^{(0)}}_j (0 * \\log(odds) + \\log(1 + \\exp(\\log(odds)))) \\\\& = \\cfrac{d}{d\\log(odds)} -n^{(1)} \\times (\\log(odds) + \\log(1 + \\exp(\\log(odds)))) - n^{(0)} \\times \\log(1 + \\exp(\\log(odds))) \\\\ & =0\\end{aligned} \\begin{aligned} & n^{(1)} \\times(-1 + \\cfrac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) + n^{(0)} \\times(\\cfrac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) \\\\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\\\ & = - n^{(1)} + n p \\\\ &= 0 \\end{aligned}移項得到 $p$ p = \\cfrac{n^{(1)}}{n} \\log(odds) = \\cfrac{p}{1-p} = \\cfrac{n^{(1)}}{n^{(0)}}故得證，給定 $\\text{loss function } = -[y \\log(p) + (1-y)\\log(1-p)]$， 能使 $argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma)$ 的 $\\gamma$ 為 log(odds)= \\cfrac{n^{(1)}}{n^{(0)}} \\therefore F_0(x) = \\cfrac{n^{(1)}}{n^{(0)}}Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？ 問題來自 Step 2 - (A) 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum -[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)] $y$ 為 target label $p = P(Y=1|x)$ 表正類的 probability 注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy 我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x) \\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - p)) \\\\ &= -(-(\\text{observed} - \\text{predict probability})) \\\\ &= - \\text{negative residual} \\\\ & = \\text{residual} \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual Q: leaf node 的輸出公式怎麼來的？ 問題來自 Step 2-(C) 在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\\gamma_{jm}$，使的 cost function 最小 \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma) \\\\ &= argmin_\\gamma \\sum_{x_i \\in R_{j, m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + e^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $j$ 表 leaf node index $m$ 表第 $m$ 步 $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合 將 loss function 以 $\\log(odds)$ 表示後的 objective function \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + e^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $-[y \\log(p) + (1-y)\\log(1-p)] = -[y\\log(odds) + \\log(1 + e^{\\log(odds)})]$ $F_{m-1}(x)$ 輸出為 $\\log(odds)$ cost function 對 $\\gamma$ 微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近 loss function 處理 f(x) \\approx f(a) + f'(a)(x-a) + \\cfrac{1}{2}f''(a)(x-a)^2讓 2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \\gamma)$ 在 $F_{m-1}(x)$ 處展開 L(y_i, F_{m-1}(x_i) + \\gamma) \\approx L(y_i, F_{m-1}(x_i) ) + \\cfrac{d}{d (F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))\\gamma + \\cfrac{1}{2} \\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\\gamma^2將 cost function 對 $\\gamma$ 微分取極值，求 $\\gamma_{j,m}$ \\sum_{x_i \\in R_{jm}} \\cfrac{d}{d\\gamma} L(y_i, F_{m-1}(x_i), \\gamma) \\approx \\sum_{x_i \\in R_{jm}} (\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\\gamma) = 0移項得到 $\\gamma$ \\gamma = \\cfrac{\\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))} = \\cfrac{ \\sum_{x_i \\in R_{jm} } g_i}{ \\sum_{x_i \\in R_{jm} } h_i}分子是 derivative of Loss function ; 分母是 second derivative of loss function 分子部分: \\begin{aligned} & \\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\\\& = \\sum \\cfrac{d}{d(F_{m-1}(x_i))} \\ y_i \\times [F_{m-1}(x_i) ] - \\log(1 + \\exp(F_{m-1}(x_i) )) \\\\ &= \\sum (y_i - \\cfrac{\\exp(F_{m-1}(x_i) )}{1 + \\exp(F_{m-1}(x_i) )} ）\\\\& = \\sum_{x_i \\in R_{jm}} (y_i -p_i) \\end{aligned} $F_{m-1}(x_i)$ 是 $m-1$ 步時 $classifier$ 輸出的 $\\log(odds)$ 分子部分為 $\\text{summation of residual}$ 分母部分 \\begin{aligned}& \\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\\\ & = \\sum_{x_i \\in R_{jm} } \\cfrac{d^2}{d \\, \\ (F_{m-1}(x_i))^2} \\, -[y_i \\times F_{m-1}(x_i) - \\log(1 + \\exp(F_{m-1}(x_i)))] \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)}-[y_i - \\cfrac{\\exp(F_{m-1}(x_i) )}{1 + \\exp(F_{m-1}(x_i) )}] \\\\ & =\\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \\exp(F_{m-1}(x_i)))^{-1} \\times \\exp(F_{m-1}(x_i))] \\\\ & = \\sum_{x_i \\in R_{jm} }-[(1 + e^{F_{m-1}(x_i)})^{-2} \\exp(F_{m-1}(x_i))\\times \\exp(F_{m-1}(x_i)) - (1+ \\exp(F_{m-1}(x_i)))^{-1} \\times \\exp(F_{m-1}(x_i)) ] \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\quad + \\quad \\cfrac{\\exp(F_{m-1}(x_i))}{1+ \\exp(F_{m-1}(x_i))} = \\sum_{x_i \\in R_{jm} }\\cfrac{-\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\ + \\ \\cfrac{-\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\times \\cfrac{(1 + \\exp(F_{m-1}(x_i)))}{1 + \\exp(F_{m-1}(x_i))} \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp(2 * F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} + \\cfrac{\\exp(F_{m-1}(x_i)) + \\exp(2 * F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} = \\sum_{x_i \\in R_{jm} }\\cfrac{\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))} \\times \\cfrac{1}{(1 + \\exp(F_{m-1}(x_i)))} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp(\\log(odds)_i)}{1 + \\exp(\\log(odds)_i)} \\times \\cfrac{1}{1 + \\exp(\\log(odds)_i)} \\\\ &= \\sum_{x_i \\in R_{jm} } p_i \\times (1-p_i) \\end{aligned}綜合分子分母，能使 $F_m(x)$ cost function 最小化的 tree $f_m(x)$ 第 $j$ 個 leaf node 輸出為 \\gamma_{jm}= \\cfrac{\\sum_{x_i \\in R_{jm})} (y_i - p_i)}{\\sum_{x_i \\in R_{jm} }(p_i \\times (1- p_i))} = \\cfrac{\\text{summation of residuals }}{\\text{summantion of (previous probability $\\times$ (1 - previoous probability))}}寫在最後Data Sample learning by doing it 123456789101112import pandas as pddata = [ (True, 12, 'blue', True), (True, 87, 'gree', True), (False, 44, 'blue', False), (True, 19, 'red', False), (False, 32, 'green', True), (False, 14, 'blue', True)]columns = ['like_popcorn', 'age', 'favorite_color', 'love_troll2']target = 'love_troll2'df = pd.DataFrame.from_records(data, index=None, columns=columns) Reference Gradient Boost Part 3 (of 4): Classification Gradient Boost Part 4 (of 4): Classification Details Gradient Boosting In Classification: Not a Black Box Anymore! https://blog.paperspace.com/gradient-boosting-for-classification/ statquest 整理 StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s The Logit and Sigmoid Functions https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/ Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979","link":"/GBDT-Classifier-step-by-step/"},{"title":"內容推薦 (1) 關鍵詞識別","text":"背景從內容召回說起電商推薦系統多路召回中，通常會有一路召回商品叫內容召回 $\\text{content I2I}$ content I2I 與其他根據用戶行為的召回不同， 最簡單的 content I2I 可以根據 A 商品 title 內容找出其他與 A 相似的 title 的商品，這給了他很強的推薦解釋性 對於商品冷啟動或者系統冷啟動來說，content I2I 可以在沒有用戶交互訊息時，就可以進行推薦，也不失為一種冷啟動方案。 在萬物皆可 Embedding 的今天，content I2I 只要把所有商品的 title 送進 word2Vec 硬 train 一發也就完事了 當然要是這麼簡單，也就不會有這篇了 推薦系統相關文章 一言難盡的商品 title我司的電商商品 title 為中翻英淘寶商品 title 而來，基本上毫無文法可言 如果用 word2Vec 硬做一發，再以 doc2vector 的思路融合成 sentence vector ，肯定會加入某些糟糕詞彙的 vector。 諸如此類的怪異詞彙： EX: “real time” (應該是實時發貨？) , liu haichang(劉海夾??), two yuan(兩元？), yiwu(義烏？) 為了讓 vector 能更好的表達句子 title，加上組內對於商品關鍵字有需求，於是就有了以下商品 title 挖掘出中 產品詞 與 標籤詞的識別任務 原理關鍵詞識別先解釋一下，什麼是產品詞，什麼是標籤詞 以下是我自己的定義： 所有商品都會有自己的 tilte，但肯定會有一個 “產品詞” 去描述這商品到底是賣什麼，他可以是單詞，稱為 unigram，也可以複合詞，bigram or trigram…… unigram:shirt , blouse bigram: apron dress, bermuda shorts trigram: buckle strap shoes, denim mini dress “ 標籤詞 “ (label words)，定義比較空泛， 狹義一點指那些可以用來形容商品或能凸顯商品特色的詞 unigram: denim, hipster bigram: chinese tunic, cotton padded trigram: deep v collar 廣義一點，也可以包含產品詞，最終還得看業務需求，標籤詞他還能在細分出 ＂屬性詞＂(propery) 領口：高領， 低領，V 領，深 V .. 材質：棉，麻 … 不管怎樣，如果沒有人工去蒐集出詞彙，那就得靠機器自己挖掘詞彙字典。 問題來了，在我們的商品池中的商品 title，基本上沒什文法可言，詞彙也是中翻英出來的。 如何找出有意義的詞彙組成的 ＂產品詞＂或＂標籤詞＂就是問題的核心了 EX:＂long＂,＂sleeved＂ 分別看沒什意義，但合起來變成 bigram words： ＂long sleeved” 就有意義 那要怎麼衡量 words 跟 words 之間的組合程度呢？ 就是 熵 entropy了 Entropy 識別關鍵字Entropy在 information theory 中 ，entropy 被用來衡量系統內的不確定性程度 H(X) = -\\sum_x p(x)\\log p(x)＂不確定性＂(uncertainty) 跟 information 豐富程度是一體兩面的 entropy 越高，代表不確定性越高，代表能提供的 information 也更多 舉例來說，如果明天會下雨的 probability 為 0.5，天氣系統的 entropy (以 2 為底)就是 -\\cfrac{1}{2} \\log_2\\cfrac{1}{2} - -\\cfrac{1}{2} \\log_2\\cfrac{1}{2} = 1如果明天會下雨的 probability 為 1，那天氣系統的 entropy 就是 -1log_21 = 0代表天氣系統完全沒有不確定性，明天肯定會下雨，對我們無法提供任何 information 我們可以利用 entropy 來衡量 : 內聚力：word 跟 word 之間的連結緊密程度，由互消息衡量之 豐富度：words 本身的自由運用程度，由 left entropy / right entropy 衡量之 互消息 (MI - Mutual Information)先上公式 I(X,Y) =\\sum_{y \\in Y}\\sum_{x \\in X} p(x,y) \\log(\\cfrac{p(x,y)}{p(x) p(y)})再上圖 看圖就可以直觀明白，$I(X,Y)$ 可以用來衡量兩個事件彼此的關聯性，直觀上互消息可以用來衡量兩個 word 之間的依賴程度。 $\\text{PMI}$ (point-wise mutual information) 也可以用來來衡量兩個 word 的相關性，他被視為簡化版的 $\\text{MI}$ PMI(x,y) = \\log \\cfrac{p(x,y)}{p(x)p(y)}word A 跟 word B 的 PMI(A， B) 或 MI(A，B）value 越高，代表 A, B 越相互依賴，組成一個 term 的可能性越越大 但從公式上不難看出，MI 是 weighted 過後的 PMI。在實務上，PMI 傾向給 “those word only occur together” 組成的 bigram 較高的分數 ; 而 MI 傾向給 high frequency bigram 更高的分數 EX:在商品 title 中有個詞 ＂small fresh＂ joint probability $\\text{p(“small”, “fresh”)} = 0.002$ $\\text{p(“small”)} = 0.0058$ $\\text{p(“fresh”)}= 0.0035$ $\\text{PMI(“small”, “fresh”)} = 4.59$ $\\text{MI(“small”, “fresh”)} =0.009$ 有另一個詞 ＂fresh loos＂ join probability $\\text{p(“fresh loose”)} = 0.0001$ $\\text{p(“fresh”) =0.0035}$ $\\text{p(“loose”)} = 0.0024$ $\\text{PMI(“fresh”, “loose”)} = 2.47$ $\\text{MI(“fresh”, “loose”)} =0.00024$ 顯然，對於 fresh 這個 word 而言，＂small fresh＂比 ＂fresh loose＂成詞程度較高。 左右熵左右熵代表了 word 本身可以自由運用的程度 我們知道，一個 word A 可以跟左邊的 word L，也可以跟右邊的 word R 組合，而左右熵就是來衡量 word A 組成 phase 的豐富程度 H_L(W) = -\\sum_{l \\in L}p(\\text{l::w | w}) \\ \\log_2 p(\\text{l::w| w}) \\\\ H_R(W) = -\\sum_{r \\in R}p(\\text{r::w | w}) \\ \\log_2 p(\\text{r::w| w})EX： 假設 ＂skirt ＂ 這個產品詞在池子中的鄰字組合計數如下 12345678910111213141516171819202122def cal_information(x): return - x * math.log(x)def cal_entropy(freqDict): total_count = sum(list(freqDict.values())) informations = [cal_information(fre / total_count) for fre in freqDict.values()] return sum(informations)skirt_left = { &quot;long skirt&quot;: 500, &quot;midi skirt&quot;: 1000, &quot;pegged skirt&quot;: 100, &quot;pleated skirt&quot;: 300, &quot;prairie skirt&quot;: 20 &quot;printed skirt&quot;: 400, &quot;sarong skirt&quot;: 80, &quot;trumpet skirt&quot;: 600 }skirt_right = { &quot;skirt suit&quot;: 500, &quot;skirt dress&quot;: 1000} 則 skirt 的 left entropy 為 E_L(\\text{\"skirt\"}) = 1.82right entropy為 E_R(\\text{\"skirt\"}) =0.63顯然對 “skirt” 而言，左側語境比右側豐富 Normalize Entropy &amp; PMI &amp; MIPMI, MI 與 entropy 的值域是個相對 unbound 的值，造成在使用時比較難拿捏 threshold，得來回比對數值決定成詞標準，解決方法是 normalize 值到固定範圍內: Normalizing PMI into (1, -1) \\text{PMI}_n(x, y) = (\\ln\\cfrac{p(x,y)}{p(x)p(y)}) / -\\ln p(x,y) Normalizing MI into (0, 1) \\text{MI}*n(X,Y) = \\cfrac{\\sum*{x,y} p(x,y) \\ln \\frac{p(x,y)}{p(x) p(y)}}{-\\sum_{x,y}p(x,y)\\ln p(x,y)} Normalizing entropy into (0, 1) H_n(X) = -\\sum_x \\frac{p(x) \\log p(x)}{\\log n}論文研究顯示 Normalized MI &amp; PMI 為對角線趨勢，但依然會有一定失真，所以在使用上得自行拿捏 一般來說 MI 偏向 high frequency，NMI 會稍微將高頻詞 push down ，低頻詞 pull up PMI 偏向 low frequency，NPMI 稍微降低低頻詞的 rank Score 成詞分數有了度量語境豐富度跟詞彙內聚力的工具後，得進一步定出一個 score 代表＂成詞程度＂，score 越高，代表這個詞成為有意義詞的可能性相對較高。 先定義一個 candidate phrase 的抽象表達，方便我們計算其成詞 score。 我們的 Candidate phrase 可以是以下這些組合： unigram candidate (special case) [unigram] , ex: [skirt] bigram candidate [unigram] :: [unigram], ex: [long :: skirt] trigram candidate [unigram] :: [bigram], ex: [casual] :: [long skirt] [bigram] :: [unigram] ex: [flower printed] :: [shirt] 拆分成 [Left] :: [Right] 的形式方便我們泛化處理 candidate phrase 接下來利用定義好的 candidate phrase 來計算成詞 score 這裡給出一個最簡單的 score 計算： \\text{score} = \\text{(PMI or MI)} - \\min(Right_{\\text{left_entropy}}, Left_{\\text{right_entropy}}) + \\min(\\text{right_entropy}, \\text{left_entropy}) $\\min(Right_{\\text{left_entropy}}, Left_{\\text{right_entropy}})$ 分別表示，Left side 與 Right side 各自的語境豐富程度，通常取 min 後的的值越大，代表 Left side 或 Right side 有一側傾向與其他詞結合，candidate 越不可能成詞 $\\min(\\text{right_entropy}, \\text{left_entropy})$ 表示 candidate 左右兩側語境豐富成度，越大代表 candidate 越可能成詞 Label Score &amp; Product Score有了以上的 background 是時候來說說產品詞跟標籤詞的特性了，大致上 產品詞在 candidate 會出現在 right side，其左側自由度較高: left_entropy &gt; right_entropy 標籤詞在 candidate 會出現在 left side，其右側自由度較高: right_entropy &gt; left_entropy 顯然只有成詞分數 score 不足以將產品詞和標籤詞分離，所以每個 candidate phrase，會針對 label 跟 product 特性計算 label score 跟 product score。 先上圖： Right Phrase，表 corpus 內出現在 candidate right side 的 phrase 集合 EX : short sleeve right side unigram 集合 Light Phrase，表 corpus 內出現在 candidate left side 的 phrase 集合 計算 Right Phrases 集合內每一個 phase 對 candidate phrase 的 sum of information，代表從所有 right phrases 的角度來看 candidate phrase 的豐富度，值越高代表 candidate 的 right phrases 組成越豐富，其成為 label 的機會越高。 我們希望單個 $\\text{phrase}_i$ 對 candidate phrase $C$ 的 conditional probability $p(\\text{phrase}_i::C|\\text{phrase}_i)$ 不要太高也不要太低，此時算出的 $\\text{information} = - p(\\text{phrase}_i::C|\\text{phrase}_i) \\log p(\\text{phrase}_i::C|\\text{phrase}_i)$ 恰好是最大 \\begin{aligned} I_{L,C} &= \\sum_{phase_i \\in C_{L}} - p(\\text{phrase}_i::C|\\text{phrase}_i) \\log p(\\text{phrase}\\_i::C|\\text{phrase}_i)\\\\ I_{R,C} &= \\sum_{phrase_j \\in C*{R}} - p(C::\\text{phrase}_j|\\text{phrase}_j) \\log p(C::\\text{phrase}_j|\\text{phrase}_j) \\end{aligned} $I_{L,C}$ 表 left phrases 對 candidate 的 sum of information $C_L$ 表 candidate 的 left phrase 集合 有了 left / right phrases information，一個簡單的 label score and product score 計算如下 \\begin{aligned} \\text{label score} &= (\\text{right_entropy - left_entropy}) + (I_{R,C} - I_{L,C}) \\\\ \\text{product score} & = (\\text{left_entropy - right_entropy}) + (I_{L,C} - I_{R,C})\\end{aligned}P.S. 上面分數計算只是提供一個計算思路，實際使用還是得資料分析 EngineeringData Structurecandidate phrase 的需要計算的值有 candidate 的 left entropy and right entropy candidate 的 PMI/MI 中的 joint probability / frequency left component 的 entropy ; right component 的 entropy right phrase information and left phrase information … etc 為了方便計算 candidate 需要兩種 Tries ，一個存 corpus 內所有 sentence 的 prefix tries，另一個存 corpus 內所有 reversed title 的 reversed tries (叫 suffix tries 怕有歧義) 以 title = &quot;Masks Scarf Cashmere Sweater Cap&quot; 為例 首先將 title 所有可能 ngram 取出: 123456789101112131415['Masks', 'Scarf', 'Cashmere', 'Sweater', 'Cap']['Masks', 'Scarf', 'Cashmere', 'Sweater']['Masks', 'Scarf', 'Cashmere']['Masks', 'Scarf']['Masks']['Scarf', 'Cashmere', 'Sweater', 'Cap']['Scarf', 'Cashmere', 'Sweater']['Scarf', 'Cashmere']['Scarf']['Cashmere', 'Sweater', 'Cap']['Cashmere', 'Sweater']['Cashmere']['Sweater', 'Cap']['Sweater']['Cap'] build prefix tries: 123456789101112131415[['Masks'], ['Masks', 'Scarf'], ['Masks', 'Scarf', 'Cashmere'], ['Masks', 'Scarf', 'Cashmere', 'Sweater'], ['Masks', 'Scarf', 'Cashmere', 'Sweater', 'Cap'], ['Scarf'], ['Scarf', 'Cashmere'], ['Scarf', 'Cashmere', 'Sweater'], ['Scarf', 'Cashmere', 'Sweater', 'Cap'], ['Cashmere'], ['Cashmere', 'Sweater'], ['Cashmere', 'Sweater', 'Cap'], ['Sweater'], ['Sweater', 'Cap'], ['Cap']] build reversed tries: 123456789101112131415[['Cap'], ['Cap', 'Sweater'], ['Cap', 'Sweater', 'Cashmere'], ['Cap', 'Sweater', 'Cashmere', 'Scarf'], ['Cap', 'Sweater', 'Cashmere', 'Scarf', 'Masks'], ['Sweater'], ['Sweater', 'Cashmere'], ['Sweater', 'Cashmere', 'Scarf'], ['Sweater', 'Cashmere', 'Scarf', 'Masks'], ['Cashmere'], ['Cashmere', 'Scarf'], ['Cashmere', 'Scarf', 'Masks'], ['Scarf'], ['Scarf', 'Masks'], ['Masks']] 當我們的 candidate phrase = [&quot;Cashmere&quot; :: &quot;Sweater&quot;] 時， 透過 prefix tries 即可找出 [&quot;Cashmere&quot; :: &quot;Sweater&quot;] 右側所有的 phrase node 透過 reversed tries 即可找出 [&quot;Cashmere&quot; :: &quot;Sweater&quot;] 左側所有的 phrase node 啟發式辨識流程 用 entropy 辨識產品詞與標籤詞本質上是 unsupervised learning 的做法，如果以 threshold 卡 score, label score, product score 判斷結果，肯定事倍工半。辨識過程中加入 clustering/grouping 輔助判斷，多迭代幾遍後就能搜集到高度置信的結果。 Grouping把 candidate phrase 當成 data sample $x$ 的話，其包含的特徵有 自身統計類：frequency ，probability … etc 自身 entropy related PMI/MI，NPMI/NMI，left entropy / right entropy left / right component related: PMI/MI ，entropy to left/entropy to right left phrase/right phrase related: sum of information ，deviation，diversity，total frequency，average frequency，total phrase …etc score 類：成詞 score ，label score，product score 挑出 data samples 裡有鑑別度的特徵丟進 cluster 算法中初步分成四群： group A: 獨立成詞 一些用法固定的詞彙 ex : “big code”(這應該是想表示大碼？)，”united state” group B: label 詞 符合 label 詞的特性，右側自由度高 ex: “short sleeved” group C: 右側 product 詞 符合 product 詞的特性，左側自由度高 ex: “lace blouse” group D: 無用詞 沒什意義的詞，本身成詞 score 不高 ex: “lace long” 然後在每個 group 中，分別挑出多個 high confidence and typical data samples ，跟其餘的 data sample 做 KNN/Kmean，來回個幾次做 semi-supervised 。 Exploration隨著置信的 data sample 越多，可以考慮訓練 decision tree，辨識新的 candidate phrases。 也可以利用 word2Vec 強大的相近詞搜索相似的 產品詞/標籤詞 挖掘辨識新的產品詞/標籤詞 這兩個做法建立在手頭上的詞彙已能很好區分出產品詞和標籤詞的情況下，例如用 word2Vec 找相近產品詞有奇效： In: 1w2v_model.wv.most_similar(&quot;flight_jacket&quot;, topn=10) out: 12345678910[('bomber_jacket', 0.8251528739929199), ('flight_suit', 0.8104218244552612), ('coach_jacket', 0.7058290243148804), ('workwear_jacket', 0.7037896513938904), ('jacket', 0.7035773992538452), ('ma_1', 0.6985215544700623), ('baseball_uniform', 0.6333736181259155), ('ma1_pilot', 0.6201080679893494), ('jackets', 0.608674168586731), ('denim_jacket', 0.6048851013183594)] Some Tips candidate phrase proposal：可以透過 TF-IDF, frequency, student-t, PMI 先行召回一批 candidate 再開始辨識 一次處理一種 ngram 辨識過程中加入字典輔助 product words 黑白字典 label words 黑白字典 善用 bigram / trigram 可以由其他 phrase 組合出，可以省去很多計算量 產品詞組成 unigram 產品詞 [unigram], ex： skirt bigram 產品詞 [unigram label] :: [unigram product] , ex: long skirt [unigram] :: [unigram], ex: phone shell (手機殼 …) trigram 產品詞 [bigram label] :: [unigram product], ex: long sleeved blouse [unigram label] :: [bigram product], ex: little black dress 標籤詞組成 unigram 標籤詞 [unigram], ex: slim bigram 標籤詞 [unigram] :: [unigram], ex: v neck, high cut trigram 標籤詞 [unigram label] :: [bigram label], ex: half high collar [bigram label] :: [unigram], ex: deep v collar Reference data mining basing on entropy Language Models – handling unseen sequences &amp; Information Theoryhttps://www3.cs.stonybrook.edu/~ychoi/cse628/lecture/03-ngram.pdf 反作弊基于左右信息熵和互信息的新词挖掘 https://zhuanlan.zhihu.com/p/25499358 基于互信息和左右信息熵的短语提取识别 http://www.hankcs.com/nlp/extraction-and-identification-of-mutual-information-about-the-phrase-based-on-information-entropy.html Normalization entropy Efficiency (normalized entropy) https://en.wikipedia.org/wiki/Entropy_(information_theory)#Efficiency_(normalized_entropy)#Efficiency_(normalized_entropy)) Bouma, G. (2009). Normalized ( Pointwise ) Mutual Information in Collocation Extraction. Proceedings of German Society for Computational Linguistics (GSCL 2009), 31–40. [https://math.stackexchange.com/questions/395121/how-entropy-scales-with-sample-size","link":"/recognize-keywords-by-entorpy/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"recommendation system","slug":"recommendation-system","link":"/tags/recommendation-system/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"word embedding","slug":"word-embedding","link":"/tags/word-embedding/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"recommendation system","slug":"recommendation-system","link":"/categories/recommendation-system/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}