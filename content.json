{"pages":[{"title":"","text":"google-site-verification: google39024f504cd1475b.html","link":"/google39024f504cd1475b.html"},{"title":"歡迎來到 404 號房","text":"不好意思，你沒有房間鑰匙預計將在約 5 秒後返回大廳。如果你很急著開房，你可以 點這裡 返回櫃檯。 let countTime = 5; function count() { document.getElementById('timeout').textContent = countTime; countTime -= 1; if(countTime === 0){ location.href = 'https://seed9d.github.io/'; // 記得改成自己網址 Url } setTimeout(() => { count(); }, 1000); } count();","link":"/404.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"一步步透視 GBDT Regression Tree","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x) $ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 結論說完了，想看數學原理請移駕 背後的數學，想看例子從無到有生出 GBDT 的請到 Algorithm - step by step ，想離開得請按上一頁。 GBDT 簡介GBDT-regression tree 簡單來說，訓練時依序建立 trees $\\{ f_1(x), f_2(x), …. , f_M(x)\\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。 所以 GBDT - regression tree 的訓練是 sequentially ，無法以並行訓練加速。 我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下: F_m(x) = F_0(x) + \\nu\\sum^m_{i=1}f_i(x) = F_{m-1}(x) + \\nu f_m(x) $\\nu$ 為 learning rate GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。 差多遠即 residual 的概念： \\textit{residual = observed - predicted}因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual GBDT Algorithm - step by stepAlgorithm 參考了 statQuest 對 GBDT 的講解，連結放在 reference，必看！ GBDT-regression tree 擬合 algorithm： Input Data and Loss Function input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data 接下來都用這組簡單的數據，其中 Target $y_i$ : weight which is what we want to predict $x_i$: features 的組成有 身高，喜歡的顏色，性別 目標是用 $x_i$ 的 height, favorite color, gender 來預測 $y_i$ 的 weight loss function 為 square error L(y_i, F(x)) = \\cfrac{1}{2}(\\textit{observed - predicted}) ^2 = \\cfrac{1}{2}(y_i^2 - F(x))^2 square error commonly use in Regression with Gradient Boost $\\textit{observed - predicted}$ is called $residual$ $y_i$ are observed value $F(x)$: the function which give us the predicted value 也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出 F_m(x) = F_{m-1}(x) + \\nu f_m(x)F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x) Step 1 Initialize Model with a Constant Value初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可 取 weight 平均得到 $F_0(x) = 71.2 = \\textit{average weight}$ Step 2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At epoch m=1 (A) Calculate Residuals of $F_{0}(x)$epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals ​ residuals = \\textit{observed weight - predicted weight} 而 $F_0(x) = \\textit{average weight = 71.17}$ 計算 residual 後: epoch_0_prediction 表 $F_0(x)$ 輸出 epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$ (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual 用 columns $\\textit{height, favorite, color, gender}$ 預測 $residuals$ 來建新樹 $f_1(x)$ 建樹的過程為一般的 regression tree building 過程，target 就是 residuals。 假設我們找到分支結構是 綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$ 特定的 leaf node 下。 epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值 直覺地對每個 leaf node 內的 data sample $x$ weight 值取平均，得到輸出值 epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值 (D) Update $F_1(x)$ with New Tree $f_1(x)$此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$ 現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \\textit{learning rate } \\times f_1(x)$ 假設 $\\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下 epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值 At epoch m=2(A) Calculate Residuals of $F_{1}(x)$m = 2 新的 residual between observed weight and $F_1(x)$如下 epoch_1_prediction 為 $F_1(x)$ 的輸出 epoch_2_residual 為 observed weight 與 predicted weight $F_1(x)$ 的 residual (B) Construct New Regression Tree $f_2(x)$建一顆新樹擬合 epoch 2 (A) 得出的 residual epoch_2_residual 為 $f_2(x)$ 要擬合的 target 假設 $f_2(x)$ 擬合後樹結構長這樣 epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_2(x)$決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個 leaf node 下的 data sample $x$ 取平均 epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是 residual epoch_2_residual (D) Update $F_2(x)$ with New Tree $f_2(x)$到目前為止我們建立了兩顆 $tree$ $f_1(x), f_2(x)$，假設 $\\textit{learning rate = 0.1}$，則 $F_2(x)$ 為 F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))每個 data sample 在 $F_2(x)$ 的 predict 值如下圖： weight: out target value epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值 epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual epoch_2_prediction: $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出 Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ Step 3 輸出模型 $F_M(x)$，把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$ 與上一步 predicted value $\\hat{y}$ $F_{m-1}(x)$ 的 $residual$ GBDT Regression 背後的數學Q: Loss function 為什麼用 mean square error ?​ 選擇 $\\cfrac{1}{2}(\\textit{observed - predicted}) ^2$ 當作 loss function 的好處是對 $F(X)$ 微分的形式簡潔 \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\\textit{negative residual}$ ​ $-(y_i - F(X)) = \\textit{-(observed - predicted) = negative residual } $ 而我們知道 $F(X)$ 在 loss function $L(y_i, F(X))$ 的 gradient 就是 $\\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$ Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？問題來自 step 1 我們要找的是能使 cost function $\\sum^n_{i=1}L(y_i, \\gamma)$ 最小的那個輸出值 $\\gamma$ 做為 $F_0(X)$。 $F_0(x) = argmin_r \\sum^n_{i=1} L(y_i,\\gamma)$ $F_0(x)$ 初始化的 function，其值是常數 $\\gamma$ refers to the predicted values $n$ 是 data sample 數 Proof: 已知 ​ \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X)) 所以 cost function 對 $\\gamma $ 微分後，是所有 sample data 跟 $\\gamma$ 的 negative residual 和 為 0 ​ \\sum^n_{i=1}L(y_i, \\gamma) = -(y_1 - \\gamma) - (y_2 - \\gamma)- ... -(y_n - \\gamma) = 0 移項得到 $\\gamma$ ​ \\gamma = \\cfrac{y_1 + y_2 + .... + y_n}{n} 正是所有 target value 的 mean，故得證 ​ F_0(x) = \\gamma = \\textit{the average of targets value} Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？問題來自 step 2A 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum \\cfrac{1}{2}(y_i^2 - F(x))^2我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x)\\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} \\ L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - F_{m-1}(x))) \\\\ &= -(-(observed - predicted )) \\\\ &= - \\textit{negative residual} \\\\ & = residual \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual ​ $f_m(x)$ = $\\textit{gradient decent}$ = $\\textit{negative gradient}$ = $residual$ GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\\textit{gradient decent}$ 的方向得到新的 $F(x)$ 這也正是為什麼叫做 gradient boost 。 by the way，step 2-(A) compute residuals: r_{im} = -[\\cfrac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \\ \\textit{for i = 1,....,n} $i$: sample number $m$: the tree we are trying to build Q：個別 leaf node 的輸出為什麼是取平均 ？問題來自 step 2C 在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\\gamma$ 使 cost function 最小 $\\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma)$ $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合 $F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$ 上的輸出值為 $\\gamma_{j,m}$ \\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2直接對 $\\gamma$ 微分 \\begin{aligned}\\cfrac{d}{d \\gamma} \\ \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2 = \\ \\sum_{x_i \\in R_{j,m}}-(y_i - F_{m-1}(x_i) - \\gamma) = 0\\end{aligned}移項 \\gamma = \\cfrac{1}{n_{jm}} \\sum_{x_i \\in R_{j,m}}y_i - F_{m-1}(x_i) $n_{jm}$ is the number of data sample in leaf node $j$ at step $m$ 白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均 事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $ 都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。 Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？問題來自 step 2D \\begin{aligned}F_m(x) & = F_{m-1}(x) + \\nu f_m(x) \\\\ &= F_{m-1}(x) + \\nu \\sum^{J_m}_{j=1}\\gamma_{jm}I(x \\in R_{jm})\\end{aligned} $F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x)$ $\\nu$ learning rate $J_m$ m 步 的 leaf node 總數 $\\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出 $F_m(x)$ 展開來就是 F_m(x) = F_0(x) + \\nu(f_1(x) + f_2(x)+ ...+f_m(t))寫在最後 seeing is believing 太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙 人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶 learning by doing it 所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的 1234567891011import pandas as pddata = [ (1.6, 'Blue', 'Male', 88), (1.6, 'Greem', 'Female', 76), (1.5, 'Blue', 'Female', 56), (1.8, 'Red', 'Male', 73), (1.5, 'Green', 'Male', 77), (1.4, 'Blue', 'Female', 57) ]columns = ['height', 'favorite_color', 'gender', 'weight']df = pd.DataFrame.from_records(data, index=None, columns=columns) always get your hands dirty ReferenceMain Gradient Boost Part 1 (of 4): Regression Main Ideas Gradient Boost Part 2 (of 4): Regression Details ccd comment: 上面兩個必看 A Step by Step Gradient Boosting Decision Tree Example https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/ 真 step by step Gradient Boosting Decision Tree Algorithm Explained https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4 including sklearn 實作 Other Gradient Boosting Decision Tree http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法 提升树算法理论详解 https://hexinlin.top/2020/03/01/GBDT/ 梯度提升树(GBDT)原理小结 https://www.cnblogs.com/pinard/p/6140514.html","link":"/GBDT-Rregression-Tree-Step-by-Step/"},{"title":"Thompson Sampling 推薦系統中簡單實用的 Exploring Strategy","text":"Exploring and ExploitingExploring and Exploiting (EE) 是推薦系統中歷久不衰的議題，如何幫助用戶發現更多感興趣的 entity 以及基於已有對用戶的認知推薦他感興趣的 entity，在推薦系統的實務上都得考慮。 具象化這個問題：在推薦系統中有$\\text{}$ $\\text{category A, category B, category C, category D, category E}$ 等五大類的 entity 集合，今天有個新用戶 $U$來了，我們要如何 知道他對哪個種類的 entity 比較感興趣？ 人的興趣可以分成長期興趣跟短期興趣，在電商場景中，用戶短期興趣指他有立即需求的商品，我們如何快速抓到他的意圖，調整推薦系統的響應？ 推薦哪些類目能帶給他意料之外的驚喜 ? 那些他沒預期，但我們推薦給他，能讓他感到滿意的 category。 Multi-armed bandit problem, K-armed bandit problem (MAP) 中的 Thompson Sampling，簡單又實用 推薦系統相關文章 Thompson Sampling Thompson Sampling 利用了 beta distribution 是 bernoulli distribution 的 conjugacy prior， 來更新 entity 被選中的 posterior probability distribution 從 Beta distribution 說起 Beta(p|\\alpha, \\beta) \\triangleq \\cfrac{1}{B(\\alpha, \\beta)} \\ p^{\\alpha -1} \\ (1-p)^{\\beta - 1} beta function $B(\\alpha, \\beta)$ is a normalization term ，其作用是使 $\\int^1_0 \\cfrac{1}{B(\\alpha, \\beta)} \\ p^{\\alpha -1} \\ (1-p)^{\\beta - 1} dp = 1$ $B(\\alpha, \\beta) = \\int^1_0 p^{\\alpha -1} (1-p)^{\\beta -1} dp$ Beta distribution $beta(\\alpha, \\beta)$ 的期望值很簡潔 E[p] = \\cfrac{\\alpha}{\\alpha + \\beta}我們知道期望值本身是結果的加權平均，如果把 $\\alpha$ 視為成功次數， $\\beta$ 視為失敗次數，那不就是平均成功率了嗎？ 更神奇的是，平均成功率還可以隨著試驗的失敗跟成功次數變動，依然還是 beta distribution \\cfrac{\\alpha + n^{(1)}}{\\alpha + \\beta + n^{(1)} + n^{(0)}} $n^{(1)}$：表新增成功次數 $n^{(0)}$: 表新增失敗次數 也因為這個直覺的特性，Beta distribution 非常適合用在估計 打擊率, 點擊率, 命中率 …等等 binary problem 以推薦系統 click through rate (CTR) 當例子在推薦系統中，category $A$ 在用戶的點擊率 (ctr) 統計中，所有用戶對 category $A$ : $\\text{average ctr} = 0.33$ $\\text{ctr variacne} = 0.00147$ 以 $\\text{mean=0.33 variacne=0.00147}$ 算出 $\\alpha \\approx50$ $\\beta \\approx100$， $\\alpha =50$ $\\beta=100$ 在本推薦系統中的意義是，$\\text{category A}$ 平均每 150 次 impression ($\\alpha + \\beta$) 能產生 50 次 click ($\\alpha$)，100 次 看了不點 ($\\beta$)。 畫出 PDF 圖中 PDF curve 的意義是，有個人叫做 “平均用戶”，”平均用戶” 對 $\\text{category A}$ 最有可能的點擊率是 $0.33$，但不一定是 0.33, 可能比 0.33 高，可能比 0.33 低，但產生 0.33 這個點擊率的 likelihood $L(\\theta| X=0.33)$ 最高 下圖是對 $beta(50, 100)$ sample 500 次，可以看出 $X=0.33$ 附近被 sample 到的次數的確較高 今天來了個新用戶 $U$，我們不知道他對 $\\text{category A}$ 的喜好程度怎麼樣，但我們可以利用前面的 “平均用戶” 做為先驗： 150 impression 產生 50 次 click ($\\alpha=50 \\ , \\beta=100$ )，再透過他後續跟 $\\text{category A}$ 的互動修正出 for $\\text{user U}$ 的 $\\alpha_U \\ \\beta_U$。 假設我們給 $U$ 展示 $\\text{category A}$ 100 次後， 他 click了 60 次，看了不點 40 次，那他的 beta distribution 變成 $beta(50 + 60, 100 + 40 ) = beta(110, 140)$ 可以發現橘線變得更尖，且往右移，此時 $mean =0.44$，表示 $user \\ U$ 比＂平均用戶＂更加偏好 $\\text{category A}$。 總結以上，一開始我們對於新用戶 $U$ 一無所知，不知道他對 $\\text{category A}$ 的偏好，但我們透過已有的先驗，結合他跟推薦系統的互動，慢慢修正對他的認知： \\cfrac{\\alpha + n^{(1)}}{\\alpha + \\beta + n^{(1)} + n^{(0)}} = \\cfrac{50 + 60}{50 + 100 + 60 + 40} = 0.44 $n^{(1)}$：對 $\\text{category A}$ 新的點擊行為 $n^{(0)}$: 對 $\\text{category A}$ 新的＂看了未點＂的行為 於是，ctr 從原本 “最有可能” 0.33 修正到 “最有可能” 0.44 。 “最有可能”: 因爲一切都是 distribution 阿 這個神奇又簡潔的現象背後的數學原理，正是 beta distribution 的 conjugacy 特性。 Conjugate prior &amp; Bayesian inference prior $p(\\theta)$ is conjugate to the likelihood function $p(X|\\theta)$ when the posterior $p(\\theta|X)$ has the same function form as the prior p(\\theta|X) = \\cfrac{p(X|\\theta) p(\\theta)}{p(X)} \\Leftrightarrow \\text{posterior} = \\cfrac{\\text{likelihood} \\cdot \\text{prior}}{\\text{evidence}} $p(X)$ is the normalization term $p(X) = \\int_{\\theta\\in \\Theta}p(X|\\theta)p(\\theta)d\\theta$ 即是 prior $p(\\theta)$ 為 beta distribution $Beta(\\theta|\\alpha, \\beta) = \\cfrac{1}{B(\\alpha, \\beta)} \\ \\theta^{\\alpha -1} \\ (1-\\theta)^{\\beta - 1}$ likelihood function $p(X|\\theta)$ 為 bernoulli distribution $Bern(c|\\theta) = \\theta^c(1-\\theta)^{1-c}$ beta distribution 與 bernoulli distribution 都有類似的 form: $\\theta^m(1-\\theta)^n$ ，同時 posterior distribution $p(\\theta|X)$ 也是 beta distribution posterior $p(\\theta|X)$ 也是 beta distribution 證明如下 Proof假設 推薦系統中，對 $category \\ A$ 曝光 $N$ 次，用戶 $U$ 點擊次數 $n^{(1)}$，未點擊次數 $n^{(0)}$，本質上是個 $N \\ bernoulli \\ trail$ ， 所以其 likelihood function： $p(C|p) =\\prod_{i=1}^n p(C=c_i|p)= p^{n^{(1)}}(1-p)^{n^{(0)}}$ (忽略係數) $C$ 是 outcome, $c=1$ for positive ; $c=0$ for negative $prior$ $p(p)$ 為 beta distribution : p(p|\\alpha, \\beta) = Beta(p|\\alpha, \\beta) = \\cfrac{1}{B(\\alpha, \\beta)} \\ p^{\\alpha -1} \\ (1-p)^{\\beta - 1} 則 $\\text{posterior}$ $p(p|C,\\alpha, \\beta) = \\cfrac{ p(C|p) \\ p(p|\\alpha, \\beta)}{\\int^1_0 p(C|p) \\ p(p|\\alpha, \\beta) \\ dp}$ 分母項 $\\int^1_0 p(C|p) \\ p(p|\\alpha, \\beta) \\ dp$ 作用為 normalize the distribution，通常用 $Z$ 代表： \\begin{aligned} p(p|C,\\alpha, \\beta) &= \\cfrac{ p(C|p) p(p|\\alpha, \\beta)}{\\int^1_0 p(C|p) p(p|\\alpha, \\beta) dp} \\\\ &= \\cfrac{p^{n^{(1)}} (1-p)^{n^{(0)}} \\cfrac{1}{B(\\alpha,\\beta)} p^{\\alpha -1} (1-p)^{\\beta -1}}{Z} \\\\ &= \\cfrac{p^{[n^{(1)} + \\alpha] -1} (1 - p) ^{[n^{(0)} + \\beta]-1}}{B(\\alpha, \\beta) Z} \\end{aligned} $Z =\\cfrac{1}{B(\\alpha,\\beta)} \\int^1_0 p^{n^{(1)}} (1-p)^{n^{(0)}} p^{\\alpha -1} (1-p)^{\\beta -1} dp$ 分母要 normalize 整個 probability distribution 使 $\\int p(p|C,\\alpha, \\beta) dp= 1$ 而新的 normalization 項為 B(\\alpha,\\beta)Z = \\int^1_0 p^{[n^{(1)} + \\alpha] -1} (1 - p) ^{[n^{(0)} + \\beta]-1}dp這不正是另一個 Beta function: $B(n^{(1)} + \\alpha , n^{(0) } +\\beta)$ ？？ 所以 $p(p|C,\\alpha, \\beta)$ 最終化簡成 \\begin{aligned} p(p|C,\\alpha, \\beta) &= \\cfrac{p^{[n^{(1)} + \\alpha] -1} (1 - p) ^{[n^{(0)} + \\beta]-1}}{B(n^{(1)} + \\alpha , n^{(0) } +\\beta)} \\\\ &= Beta (p|n^{(1)} +\\alpha, n^{(0)} + \\beta)\\end{aligned}故得證 $\\text{posterior}$ $p(p|C,\\alpha, \\beta)$ 也是 $\\text{Beta distribution}$ Implement一個簡單的實作方式是 先在線下計算好每個 category 的 ctr mean 跟 variance。 在實時推薦時，拿回某用戶近期對每個 category 交互數據 impression 與 click ，計算出新的 $\\alpha \\ \\beta$。 有了每個類目的 $\\alpha \\ \\beta$ 後，對每個類目的 $beta(\\alpha, \\beta)$ sampling，接著取出 sample 後 top K 的類目即可。 C2I 召回 ….. 當然，你也可以不基於 category 維度計算 beta distribution，而是基於每一個 entity。不過如果 entity 數量上百萬，這顯然不切實際。 線下統計每個 category CTR 的 variance and mean Spark snippets 1234567def calAvgAndVar(input: Dataset[Row], categoryCol: String): Dataset[Row] = input.select(categoryCol, ctrCol) .groupBy(categoryCol).agg( fn.avg(fn.col(ctrCol)).as(&quot;ctr_avg&quot;), fn.variance(ctrCol).alias(&quot;ctr_var&quot;)) .na.drop .withColumnRenamed(categoryCol, &quot;categoryId&quot;) 線上 計算每個 category 的初始 $\\alpha_0 \\ \\beta_0$ 12345ImmutablePair&lt;Double, Double&gt; calAlphaAndBeta(double ctrMean, double ctrVar) { double alpha = (((1 - ctrMean) / (ctrMean)) - 1 / ctrMean) * Math.pow(ctrMean, 2); double beta = alpha * ((1 / ctrMean) - 1); return ImmutablePair.of(alpha, beta); } \\begin{aligned} \\alpha &=\\left(\\frac{1-\\mu}{\\sigma^2}-\\frac{1}{\\mu}\\right)\\mu^2 \\\\ \\beta &= \\alpha\\left(\\frac{1}{\\mu}-1\\right) \\end{aligned} 取回用戶的近期 category 交互行為 impression and click，並計算新的 $\\alpha_t,\\ \\beta_t$ 123456789101112/* left: alpha, right: beta */ImmutablePair&lt;Double, Double&gt; prior = calAlphaAndBeta(double ctrMean, double ctrVar);/* left: impression, right: click */ImmutablePair&lt;Integer, Integer&gt; posteriorPair = posteriorData.getOrDefault(cateId, ImmutablePair.of(0, 0));int clickCount = posteriorPair.getRight();int impressionCount = posteriorPair.getLeft();int impressionWithoutClick = (impressionCount - clickCount) &gt; 0 ? (impressionCount - clickCount) : impressionCount;double newAlpha = prior.getLeft() + clickCount;double newBeta = prior.getRight() + impressionWithoutClick; 對每個 category 的 beta distribution $beta(\\alpha, \\beta)$ sampling 123456import org.apache.commons.math3.distribution.BetaDistribution;double calBetaProbability(double alpha, double beta) { BetaDistribution betaDistribution = new BetaDistribution(alpha, beta); double rand = Math.random(); return betaDistribution.inverseCumulativeProbability(rand);} sampling 利用 beta distribution 的 inverse cumulative distribution function (inverse CDF) sampling 出 random variable 參考 https://en.wikipedia.org/wiki/Inverse_transform_sampling Reference Multi-Armed Bandit With Thompson Sampling https://predictivehacks.com/multi-armed-bandit-with-thompson-sampling/ Conjugacy in Bayesian Inference http://gregorygundersen.com/blog/2019/03/16/conjugacy/ Understanding the beta distribution (using baseball statistics) http://varianceexplained.org/statistics/beta_distribution_and_baseball/ 中文翻譯 : 如何通俗理解 beta 分布？ - 小杰的回答 - 知乎 https://www.zhihu.com/question/30269898/answer/123261564 https://en.wikipedia.org/wiki/Beta_distribution Heinrich, G. (2005). Parameter estimation for text analysis 雖然是講 LDA，但前面從 ML MAP 一路推導到 Bayesian inference ，很詳細","link":"/Implement-Thompson-Sampling-in-Recommendation-System/"},{"title":"Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax","text":"CBOW with Hierarchical SoftmaxCBOW 的思想是用兩側 context words 去預測中間的 center word P(center|context;\\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ $\\text{v}’_j$ 表 $W’$ 中 j-th columns vector 在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是 leaf node ，跟 column vector $\\text{v}’_j$ 與 word $w_i$ 不是直接對應的關係 Objective Function Huffman Tree 令 $w_{I,j}$ 表 input 的 第 $j$ 個 context word; $w_O$ 表 target 的 center word 則 Hierarchical Softmax 下的 objective function \\begin{aligned} & -\\log p(w_O| w_I) = -\\log \\dfrac{\\text{exp}({h^\\top \\text{v}'_O})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} \\\\ & = - \\sum^{L(w)-1}_{l=1} \\log\\sigma([\\cdot] h^\\top \\text{v}^{'}_l) \\end{aligned} $L(w_i) -1$ 表 huffman tree 中從 root node 到 leaf node of $w_i$ 的 node number $[\\cdot]$表 huffman tree 的分岔判斷 $[\\cdot] = 1$ 表 turn left $[\\cdot ] = -1$ 表 turn right $h = \\frac {1}{C} \\sum^{C}_{j=1}\\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$ 詳細推導請見 Word2Vec (2):Hierarchical Softmax 背後的數學 透過 Hierarchical Softmax，因爲 huffman tree 為 full binary tree， time complexity 降成 $\\log_2|V|$ Pytorch CBOW with Hierarchical SoftmaxBuilding Huffman TreeHuffman Tree 建樹過程 HuffmanTree >folded12123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124class HuffmanTree: def __init__(self, fre_dict): self.root = None freq_dict = sorted(fre_dict.items(), key=lambda x:x[1], reverse=True) self.vocab_size = len(freq_dict) self.node_dict = {} self._build_tree(freq_dict) def _build_tree(self, freq_dict): ''' freq_dict is in decent order node_list: two part: [leaf node :: internal node] leaf node is sorting by frequency in decent order; ''' node_list = [HuffmanNode(is_leaf=True, value=w, fre=fre) for w, fre in freq_dict] # create leaf node node_list += [HuffmanNode(is_leaf=False, fre=1e10) for i in range(self.vocab_size)] # create non-leaf node parentNode = [0] * (self.vocab_size * 2) # only 2 * vocab_size - 2 be used binary = [0] * (self.vocab_size * 2) # recording turning left or turning right ''' pos1 points to currently processing leaf node at left side of node_list pos2 points to currently processing non-leaf node at right side of node_list ''' pos1 = self.vocab_size - 1 pos2 = self.vocab_size ''' each iteration picks two node from node_list the first pick assigns to min1i the second pick assigns to min2i min2i's frequency is always larger than min1i ''' min1i = 0 min2i = 0 ''' the main process of building huffman tree ''' for a in range(self.vocab_size - 1): ''' first pick assigns to min1i ''' if pos1 &gt;= 0: if node_list[pos1].fre &lt; node_list[pos2].fre: min1i = pos1 pos1 -= 1 else: min1i = pos2 pos2 += 1 else: min1i = pos2 pos2 += 1 ''' second pick assigns to min2i ''' if pos1 &gt;= 0: if node_list[pos1].fre &lt; node_list[pos2].fre: min2i = pos1 pos1 -= 1 else: min2i = pos2 pos2 += 1 else: min2i = pos2 pos2 += 1 ''' fill information of non leaf node ''' node_list[self.vocab_size + a].fre = node_list[min1i].fre + node_list[min2i].fre node_list[self.vocab_size + a].left = node_list[min1i] node_list[self.vocab_size + a].right = node_list[min2i] ''' the parent node always is non leaf node assigen lead child (min2i) and right child (min1i) to parent node ''' parentNode[min1i] = self.vocab_size + a # max index = 2 * vocab_size - 2 parentNode[min2i] = self.vocab_size + a binary[min2i] = 1 '''generate huffman code of each leaf node ''' for a in range(self.vocab_size): b = a i = 0 code = [] point = [] ''' backtrace path from current node until root node. (bottom up) 'root node index' in node_list is 2 * vocab_size - 2 ''' while b != self.vocab_size * 2 - 2: code.append(binary[b]) b = parentNode[b] # point recording the path index from leaf node to root, the length of point is less 1 than the length of code point.append(b) ''' huffman code should be top down, so we reverse it. ''' node_list[a].code_len = len(code) node_list[a].code = list(reversed(code)) ''' 1. Recording the path from root to leaf node (top down). 2.The actual index value should be shifted by self.vocab_size, because we need the index starting from zero to mapping non-leaf node 3. In case of full binary tree, the number of non leaf node always equals to vocab_size - 1. The index of BST root node in node_list is 2 * vocab_size - 2, and we shift vocab_size to get the actual index of root node: vocab_size - 2 ''' node_list[a].node_path = list(reversed([p - self.vocab_size for p in point])) self.node_dict[node_list[a].value] = node_list[a] self.root = node_list[2 * vocab_size - 2] 建樹過程參考 Word2Vec 作者 Tomas Mikolov 的 c code，思路如下： 建一個 Array，左半邊放 leaf node ，右半邊放 non leaf node leaf node 按照 frequency 降序排列 bottom up building tree 從 Array 中間位置向右半邊填 non leaf node each iteration 都從 leaf node 跟 已填完的 non leaf node 找兩個 frequency 最小的 node，做為 child node 填入當下 non leaf node Hierarchical Softmax用 huffman tree 實作 Hierarchical Softmax 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class HierarchicalSoftmaxLayer(nn.Module): def __init__(self, vocab_size, embedding_dim, freq_dict): super().__init__() ## in w2v c implement, syn1 initial with all zero self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.syn1 = nn.Embedding( num_embeddings=vocab_size + 1, embedding_dim=embedding_dim, padding_idx=vocab_size ) torch.nn.init.constant_(self.syn1.weight.data, val=0) self.huffman_tree = HuffmanTree(freq_dict) def forward(self, neu1, target): # neu1: [b_size, embedding_dim] # target: [b_size, 1] # turns:[b_size, max_code_len_in_batch] # paths: [b_size, max_code_len_in_batch] turns, paths = self._get_turns_and_paths(target) paths_emb = self.syn1(paths) # [b_size, max_code_len_in_batch, embedding_dim] loss = -F.logsigmoid( (turns.unsqueeze(2) * paths_emb * neu1.unsqueeze(1)).sum(2)).sum(1).mean() return loss def _get_turns_and_paths(self, target): turns = [] # turn right(1) or turn left(-1) in huffman tree paths = [] max_len = 0 for n in target: n = n.item() node = self.huffman_tree.node_dict[n] code = target.new_tensor(node.code).int() # in code, left node is 0; right node is 1 turn = torch.where(code == 1, code, -torch.ones_like(code)) turns.append(turn) paths.append(target.new_tensor(node.node_path)) if node.code_len &gt; max_len: max_len = node.code_len turns = [F.pad(t, pad=(0, max_len - len(t)), mode='constant', value=0) for t in turns] paths = [F.pad(p, pad=(0, max_len - p.shape[0]), mode='constant', value=net.hs.vocab_size) for p in paths] return torch.stack(turns).int(), torch.stack(paths).long() syn1 表 $W’$ 裡面的 vector 對應到 huffman tree non leaf node 的 vector 實作上 $W’$ row vector 才有意義 neu1 即 $\\text{h}$ 為 hidden layer 的輸出 target 為 center word $w_O$ function _get_turns_and_paths 中 實作時 -1 表 turn left ; 1 表 turn right ，其實兩者只要相反就好，因爲對於 binary classification $p(\\text{true}) = \\sigma(x)$ ⇒ $p(\\text{false}) = 1- \\sigma(x) = \\sigma(-x)$ 只是 $\\sigma$ 裡的正負號對換而已 CBOW + Hierarchical Softmax12345678910111213class CBOWHierarchicalSoftmax(nn.Module): def __init__(self, vocab_size, embedding_dim, fre_dict): super().__init__() self.syn0 = nn.Embedding(vocab_size, embedding_dim) self.hs = HierarchicalSoftmaxLayer(vocab_size, embedding_dim, fre_dict) torch.nn.init.xavier_uniform_(self.syn0.weight.data) def forward(self, context, target): # context: [b_size, 2 * window_size] # target: [b_size] neu1 = self.syn0(context.long()).mean(dim=1) # [b_size, embedding_dim] loss = self.hs(neu1, target.long()) return loss neu1 為 average of context words’ vector Training訓練過程省略，有興趣請見 notebook seed9D/hands-on-machine-learning Evaluation訓練語料是聖經，看看 jesus 跟 christ 的相近詞 In : 12cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)cosinSim.get_synonym('christ') out: 12345678910[('christ', 1.0), ('hope', 0.78780156), ('gospel', 0.7656436), ('jesus', 0.74575657), ('faith', 0.7190881), ('godliness', 0.7005944), ('offences', 0.70045626), ('grace', 0.6946964), ('dear', 0.666232), ('willing', 0.66131693)] In 1cosinSim.get_synonym('jesus') Out 12345678910[('jesus', 0.9999999), ('gospel', 0.8051339), ('grace', 0.75879383), ('church', 0.7542972), ('christ', 0.74575657), ('manifest', 0.7415799), ('believed', 0.7215627), ('faith', 0.7198993), ('godliness', 0.7091305), ('john', 0.7015951)] In 1cosinSim.get_synonym('god') Out 12345678910[('jesus', 0.9999999), ('gospel', 0.8051339), ('grace', 0.75879383), ('church', 0.7542972), ('christ', 0.74575657), ('manifest', 0.7415799), ('believed', 0.7215627), ('faith', 0.7198993), ('godliness', 0.7091305), ('john', 0.7015951)] Reference https://github.com/tmikolov/word2vec c code 基于Numpy实现Word2Vec Hierarchical Softmax CBOW and SkipGram模型 http://ziyangluo.tech/2020/02/29/W2VHierarchical/ https://github.com/ilyakhov/pytorch-word2vec https://github.com/weberrr/pytorch_word2vec other Binary Tree: Intro(簡介) http://alrightchiu.github.io/SecondRound/binary-tree-introjian-jie.html#fullcomplete","link":"/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/"},{"title":"Word2Vec (4):Pytorch 實作 Word2Vec with Softmax","text":"用 pytorch 實現最簡單版本的 CBOW 與 skipgram，objective function 採用 minimize negative log likelihood with softmax CBOWCBOW 的思想是用兩側 context 詞預測中間 center 詞，context 詞有數個，視 window size 大小而定 P(center|context;\\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ Condition probability $P(center | context; \\theta)$ 中 variable $\\textit{center word}$ 有限，所以是個 descrete probability，可以轉化成多分類問題來解 令 $w_O$ 表 center word, $w_I$ 表 input 的 context word，則 P(center|context;\\theta) = P(w_O|w_I; \\theta) = \\cfrac{\\exp(h^\\top \\text{v}^{'}_{w_{O}})}{\\sum_{w_ \\in V}\\exp(h^\\top \\text{v}'_{w_i})} $h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\\cfrac{1}{C}(\\text{v}_{w_1} + \\text{v}_{w_2}+ …+ \\text{v}_{w_C})^T$ 訓練過程 $\\text{maximize log of condition probability } P(w_O|w_I; \\theta$ \\begin{aligned} & \\text{maxmize}_\\theta \\ \\log P(w_O|w_I; \\theta) \\\\& = \\text{minimize}_\\theta \\ -\\log \\ P(w_O|w_I; \\theta) \\\\& = \\text{minimize}_\\theta \\ - \\log \\cfrac{\\exp(h^\\top \\text{v}^{'}_{w_{O}})}{\\sum_{w_i \\in V} \\exp(h^\\top \\text{v}^{'}_{w_i})} \\end{aligned}Pytorch CBOW + softmaxCBOW + softmax 模型定義123456789101112131415class CBOWSoftmax(nn.Module): def __init__(self, vocab_size, embedding_dim): super().__init__() self.syn0 = nn.Embedding(vocab_size, embedding_dim) self.syn1 = nn.Linear(embedding_dim, vocab_size) def forward(self, context, center): # context: [b_size, windows_size] # center: [b_size, 1] embds = self.syn0(context).mean(dim=1) # [b_size, embedding_dim] out = self.syn1(embds) log_probs = F.log_softmax(out, dim=1) loss = F.nll_loss(log_probs, center.view(-1), reduction='mean') return loss syn0 對應到 input 側的 embedding matrix $W$ syn1 對應到 output 側的 embedding matrix $W’$ loss 的計算 $- log \\cfrac{\\exp(h^\\top \\text{v}^{‘}_{w_{O}})}{\\sum_{w_i \\in V} \\exp(h^\\top \\text{v}^{‘}_{w_i})}$ input: context 跟 center 內容都是將 word index 化 因爲 context 是由 windows size N 個 words 組成，所以總共有 N 個 word embedding ，常規操作是 sum or mean Training Stage訓練過程省略，有興趣的可以去 github 看 notebook seed9D/hands-on-machine-learning 取出 Embedding創建一個衡量 cosine similarity的 class 12345678910111213141516171819class CosineSimilarity: def __init__(self, word_embedding, idx_to_word_dict, word_to_idx_dict): self.word_embedding = word_embedding # normed already self.idx_to_word_dict = idx_to_word_dict self.word_to_idx_dict = word_to_idx_dict def get_synonym(self, word, topK=10): idx = self.word_to_idx_dict[word] embed = self.word_embedding[idx] cos_similairty = w2v_embedding @ embed topK_index = np.argsort(-cos_similairty)[:topK] pairs = [] for i in topK_index: w = self.idx_to_word_dict[i]# pairs[w] = cos_similairty[i] pairs.append((w, cos_similairty[i])) return pairs 僅使用 syn0 做為 embedding，記得 L2 norm 123456syn0 = model.syn0.weight.dataw2v_embedding = syn0 w2v_embedding = w2v_embedding.numpy()l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)w2v_embedding = w2v_embedding / l2norm 訓練的 corpus 是聖經，所以簡單看下 jesus 與 christ 兩個 word 的相似詞，效果不予置評 Skipgramskipgram 的思想是用中心詞 center word 去預測兩側的 context words P(context|center; \\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ 令 $w_I$ 表 input 的 center word， $w_{O,j}$ 表 target 的 第 $j$ 個 context word ，則 condition probability P(context|center;\\theta) = P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) = \\prod^C_{c=1 }\\cfrac{\\exp(h^\\top \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(h^\\top \\text{v}'_{w_i})} $h$ 表 hidden layer 的輸出，在 skipgram 實際上就是 $\\text{v}_{w_I}$ Skipgram 的 objective function \\begin{aligned} & -\\log P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) \\\\ & = -\\log \\prod^C_{c=1}\\cfrac{\\exp(h^\\top \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(h^{\\top} \\text{v}'_{w_i})} \\\\ & = -\\log \\prod^C_{c=1}\\cfrac{\\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_i})} \\\\& = -\\sum^C_{c=1}\\log \\cfrac{\\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_i})} \\end{aligned}Pytorch skipgram + softmax模型12345678910111213141516class SkipgramSoftmax(nn.Module): def __init__(self, vocab_size, embedding_dim): super().__init__() self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.syn0 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K| self.syn1 = nn.Linear(embedding_dim, vocab_size) # |K| x |V| def forward(self, center, context): # center: [b_size, 1] # context: [b_size, 1] embds = self.syn0(center.view(-1)) out = self.syn1(embds) log_probs = F.log_softmax(out, dim=1) loss = F.nll_loss(log_probs, context.view(-1), reduction='mean') return loss syn0 對應到 input 側的 embedding matrix $W$ syn1 對應到 output 側的 embedding matrix $W’$ 實際上，skipgram 每筆 training data 只需要 (a center word, a context word) 的 pair 即可 所以 loss function 實現上非常簡單 -\\log \\cfrac{\\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,c}})}{\\sum_{w_i \\in V} \\exp(\\text{v}^\\top_{w_I} \\text{v}'_{w_i})}Training Stage訓練過程省略，有興趣的可以去 github 看 notebook seed9D/hands-on-machine-learning Evaluation取出 embedding，這次 embedding 嘗試 $(W + W’)/2$ 1234567syn0 = model.syn0.weight.datasyn1 = model.syn1.weight.dataw2v_embedding = (syn0 + syn1) / 2w2v_embedding = w2v_embedding.numpy()l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)w2v_embedding = w2v_embedding / l2norm 一樣看 jesus 跟 christ 的相似詞，感覺似乎比 CBOW 好一點 Reference https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb 基于PyTorch实现word2vec模型 https://lonepatient.top/2019/01/18/Pytorch-word2vec.htm Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from http://arxiv.org/abs/1411.2738 https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py","link":"/Pytorch-Implement-Naive-Word2Vec-with-Softmax/"},{"title":"Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling","text":"Skipgram with Negative Samplingskipgram 的思想是用中心詞 center word 去預測兩側的 context words P(context|center; \\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ Objective Function 令 $w_I$ 表 input 的 center word ; $w_{O,j}$ 表 target 的 第 $j$ 個 context word。 則 Negative Sampling 下的 objective function \\mathcal{L}_\\theta = - [ \\log \\sigma(\\text{v}^\\top_{w_I} \\text{v}'_{w_{O,j}}) + \\sum^M_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}\\sigma(-\\text{v}^\\top_{w_I} \\text{v}'_{\\tilde{w}_i})] $\\tilde{w}_i$ 為從 distribution $Q$ sample 出的 word M 為 從 $Q$ sample 出的 $\\tilde{w}$ 數量 第一項為 input center word $w_I$ 與 target context word $w_{O,j}$ 產生的 loss 第二項為 negative sample 產生的 loss ，共 sample 出 $M$ 個 word 有興趣看從 softmax 推導到 NEG的，參閱 Word2Vec (3):Negative Sampling 背後的數學 Negative Sample (NEG)目標是從一個分佈 $Q$ sample 出 word $\\tilde{w}$ 實作上從 vocabulary $V$ sample 出 ${w}_i$ 的 probability $P(w_i)$ 為 P(w_i) = \\cfrac{f(w_i)^{\\alpha}}{\\sum^M_{j=0}(f(w_j)^\\alpha)} $f(w_i)$ 為 $w_i$ 在 corpus 的 frequency count $\\alpha$ 為 factor, 通常設為 $0.75$，其作用是 increase the probability for less frequency words and decrease the probability for more frequent words 每個 word $w_i$ 都有個被 sample 出的 probability $P(w_i)$， 目的是從 $P(w)$ sample 出 $M$ 個 word 做為 negative 項 網路上常見的實現方法是調用 1np.random.multinomial(sample_size, pvals) 此法應該是透過 inverse CDF 來 sample word，每筆 training data 都調用一次的話運算效率不高 Word2Vec 作者 Tomas Mikolov 在他的 c code 中，採用了一種近似方式，其思想是在極大的抽樣次數下 $M = 1e8$，word 的 probability 越高代表其 frequency 越大，也就是在 M 中所占份額 shares 越多。 例如 yellow 的 probability 最大，理應在 M=30 中佔據較多的份額。 $P(\\text{blue}) = \\frac{2}{30}$ $P(\\text{green}) = \\frac{6}{30}$ $P(\\text{yellow}) = \\frac{10}{30}$ $P(\\text{red}) = \\frac{5}{30}$ $P(\\text{gray}) = \\frac{7}{30}$ 所以事先準備好一張 size 夠大的 table ($M = 1e8$)，根據 word frequency 給予相應的 shares ，真正要 sample word 的時候，只要從 $M$ 中 uniform random 出一個 index $m$ ， index $m$ 對應到的 word 就是被 sample 出的 word $\\tilde{w}$，是個以空間換取時間的做法。 Seeing is Believing做了一下測試 ，10000 次迭代，每次取 6 個 negatvie sample 的情景下，Tomas Mikolov 的近似思路比較有效率，而且是碾壓性的 但在 一次 sample 較多 word 的時候，multinomial 較有效率，可能 numpy 內部有做平行化的關係 Pytorch Skipgram with Negative SamplingNegative Sample1234567891011121314151617181920212223242526272829class NegativeSampler: def __init__(self, corpus, sample_ratio=0.75): self.sample_ratio = sample_ratio self.sample_table = self.__build_sample_table(corpus) self.table_size = len(self.sample_table) def __build_sample_table(self, corpus): counter = dict(Counter(list(itertools.chain.from_iterable(corpus)))) words = np.array(list(counter.keys())) probs = np.power(np.array(list(counter.values())), self.sample_ratio) normalizing_factor = probs.sum() probs = np.divide(probs, normalizing_factor) sample_table = [] table_size = 1e8 word_share_list = np.round(probs * table_size) ''' the higher prob, the more shares in sample_table ''' for w_idx, w_fre in enumerate(word_share_list): sample_table += [words[w_idx]] * int(w_fre)# sample_table = np.array(sample_table) // too slow return sample_table def generate(self, sample_size=6): negatvie_samples = [self.sample_table[idx] for idx in np.random.randint(0, self.table_size, sample_size)] return np.array(negatvie_samples) In: 123sampler = NegativeSampler(corpus)sampler.generate() Out: 12array(['visiting', 'defiled', 'thieves', 'beyond', 'lord', 'fill'], dtype='&lt;U18') Skipgram + NEG123456789101112131415161718192021222324class SkipGramNEG(nn.Module): def __init__(self, vocab_size, embedding_dim): super().__init__() self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.syn0 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K| self.neg_syn1 = nn.Embedding(vocab_size, embedding_dim) # |V| x |K| torch.nn.init.constant_(self.neg_syn1.weight.data, val=0) def forward(self, center: torch.Tensor, context: torch.Tensor, negative_samples: torch.Tensor): # center : [b_size, 1] # context: [b_size, 1] # negative_sample: [b_size, negative_sample_num] embd_center = self.syn0(center) # [b_size, 1, embedding_dim] embd_context = self.neg_syn1(context) # [b_size, 1, embedding_dim] embd_negative_sample = self.neg_syn1(negative_samples) # [b_size, negative_sample_num, embedding_dim] prod_p = (embd_center * embd_context).sum(dim=1).squeeze() # [b_size] loss_p = F.logsigmoid(prod_p).mean() # 1 prod_n = (embd_center * embd_negative_sample).sum(dim=2) # [b_size, negative_sample_num] loss_n = F.logsigmoid(-prod_n).sum(dim=1).mean() # 1 return -(loss_p + loss_n) syn0 對應到 input side 的 matrix $W$ neg_syn1 對應到 output side 的 matrix $W’$ Tomas Mikolov 在 WordVec c code 初始化為 0 loss function loss_p 對應到 $\\log \\sigma(\\text{v}^\\top_{w_I} \\text{v}’_{w_{O,j}})$ loos_n 對應到 $\\sum^M_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}\\exp(\\text{v}^\\top_{w_I} \\text{v}’_{\\tilde{w}_i})$ Training Skipgram + Negative Sampling訓練過程省略，參閱 notebook seed9D/hands-on-machine-learning Evaluation取回 embedding簡單的把 syn0 跟 neg_syn1 平均 1234567syn0 = model.syn0.weight.dataneg_syn1 = model.neg_syn1.weight.dataw2v_embedding = (syn0 + neg_syn1) / 2w2v_embedding = w2v_embedding.numpy()l2norm = np.linalg.norm(w2v_embedding, 2, axis=1, keepdims=True)w2v_embedding = w2v_embedding / l2norm Cosine similarity123456789101112131415161718class CosineSimilarity: def __init__(self, word_embedding, idx_to_word_dict, word_to_idx_dict): self.word_embedding = word_embedding # normed already self.idx_to_word_dict = idx_to_word_dict self.word_to_idx_dict = word_to_idx_dict def get_synonym(self, word, topK=10): idx = self.word_to_idx_dict[word] embed = self.word_embedding[idx] cos_similairty = w2v_embedding @ embed topK_index = np.argsort(-cos_similairty)[:topK] pairs = [] for i in topK_index: w = self.idx_to_word_dict[i] pairs.append((w, cos_similairty[i])) return pairs 訓練語料是聖經，看看 jesus 跟 christ 的相近詞 In: 12cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)cosinSim.get_synonym('christ') Out: 12345678910[('christ', 1.0), ('jesus', 0.7170907), ('gospel', 0.4621805), ('peter', 0.39412546), ('disciples', 0.3873747), ('noise', 0.28152165), ('asleep', 0.26372147), ('taught', 0.2422184), ('zarhites', 0.24168596), ('nobles', 0.23950878)] In: 1cosinSim.get_synonym('jesus') out: 12345678910[('jesus', 1.0), ('christ', 0.7170907), ('gospel', 0.5360588), ('peter', 0.3603956), ('disciples', 0.3460646), ('church', 0.2755898), ('passed', 0.24744174), ('noise', 0.23768528), ('preach', 0.23454829), ('send', 0.2337867)] Reference https://github.com/tmikolov/word2vec c code word2vec的PyTorch实现 https://samaelchen.github.io/word2vec_pytorch/ CBOW + NEG https://rguigoures.github.io/word2vec_pytorch/ CBOW + NEG https://github.com/ilyakhov/pytorch-word2vec Word2Vec Tutorial Part 2 - Negative Sampling http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ 基于PyTorch实现word2vec模型 https://lonepatient.top/2019/01/18/Pytorch-word2vec.html other https://github.com/Adoni/word2vec_pytorch/blob/master/model.py medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4","link":"/Pytorch-Implement-Skipgram-with-Negative-Sampling/"},{"title":"透視 XGBoost(3) 蘋果樹下的 objective function","text":"XGBoost General Objective FunctionXGboost 是 gradient boosting machine 的一種實作方式，xgboost 也是建一顆新樹 $f_m(x)$ 去擬合上一步模型輸出 $F_{m-1}(x)$ 的 $\\text{residual}$ \\begin{aligned} F_m(x) & = F_{m-1}(x) + f_m(x) \\\\ F_{m-1}(x) &= F_0(x) + \\sum_{i=0}^{m-1}f_{m-1}(x) \\end{aligned}不同的是， XGBoost 用一種比較聰明且精準的方式去擬合 residual 建立專屬 XGBoost 的蘋果樹 $f_m(x)$ 此篇首先推導了 XGBoost 的通用 Objective Function，然後解釋為何 second order Taylor expansion 可以讓 XGBoost 收斂更快更準確 XGBoost’s Objective FunctionXGBoost 的 objective function 由 loss function term $l(y_i, \\hat{y_i})$ 和 regularized term $\\Omega$ 組成 \\begin{aligned} \\mathcal{L}& = [\\sum_i^n l(y_i, \\hat{y_i}) ] + \\sum_{m}\\Omega({f_m}) \\end{aligned} $\\mathcal{L}$ 表 objective function $l(y_i, \\hat{y_i})$ 表 loss function in regression, loss function $l(y_i, \\hat{y_i})$ commonly use square error in classification，最大化 log likelihood 就是最小化 cross entropy $f_m$ 表 第 m 步 XGB tree regularized term $\\Omega$ 跟 XGB tree $f_m(x)$ leaf node number $T_m$ 與 each leaf node output $\\gamma$ 有關 $\\Omega(f_m) = \\tau T_m + \\cfrac{1}{2}\\lambda \\lVert \\gamma \\rVert ^2$ $\\lambda$ is a regularization parameter $T_m$ is the number of leaf in tree $f_m$ $\\tau$ 表對 $T_m$ 的 factor 在 第 $m-1$ 步建完 tree $f_{m-1}$時，total cost 為 \\begin{aligned} \\mathcal{L}^{(m-1)}& = [\\sum_i^n l(y_i, \\hat{y_i}^{(m-1)}) ] + \\Omega({f_{(m-1)}}) \\\\ &=[\\sum_i^n l(y_i, F_{(m-1)}(x_i) ] + \\Omega({f_{(m-1)}}) \\end{aligned} \\tag{4} $\\hat{y_i}^{(m-1)}$ 表 $F_{m-1}(x_i)$ 預測值 $f_{m-1}$ 表 第 $m-1$ 步建立的 XGB tree 進入第 $m$ 步，我們建一顆新樹 $f_m(x)$ 進一步減少 total loss，使得 $\\mathcal{L}^{(m)}&lt; \\mathcal{L}^{(m-1)}$ ， 則 $m$ 步 cost function 為 \\begin{aligned} \\mathcal{L}^{(m-1)}& = [\\sum_i^n l(y_i, \\hat{y_i}^{(m-1)}) ] + \\Omega({f_{(m-1)}}) \\\\ &=[\\sum_i^n l(y_i, F_{(m-1)}(x_i) ] + \\Omega({f_{(m-1)}}) \\end{aligned} \\tag{4}現在要找出新樹 $f_m(x_i)$ 的每個 leaf node 輸出能使式 (5) $\\mathcal{L}^{(m)}$ 最小 \\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} [l(y_i, F_{m-1}(x_i) + \\gamma)] + \\Omega({f_{(m)}}) \\tag{6} $j$ 表 leaf node index $m$ 表第 $m$ 步 $\\gamma_{j,m}$ $m$ 步 $f_m$ 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，所包含的 data sample $x$ 集合 當 loss function $l(y, \\hat{y})$ 為 square error 時，求解式 (6) 不難，但如果是 classification problem 時，就變得很棘手。 所以 GBDT 對 regression problem 與 classification problem 分別用了兩種不同方式求解極值 for regression ：硬解 derivative ，因爲 MSE 的 derivative 簡單。參閱 一步步透視 GBDT Regression Tree for classification: use the Second Order Tayler Approximation 化簡 loss function。參閱 一步步透視 GBDT Classifier Second Order Tayler Approximation 的步步逼近XGBoost 直接以 Second Order Tayler Approximation 處理 classification 跟 regression 的 loss function part $l(y,\\hat{y})$ 式(5) 的 loss function part 在 $f_m(x)$ 附近展開: \\begin{aligned} l(y,\\hat{y}^{(m-1)} + f_m(x)) \\approx & \\quad l(y, \\hat{y}^{(m-1)}) + [\\cfrac{d}{d \\ \\hat{y}^{(m-1)}} \\ l(y, \\hat{y}^{(m-1)})]f_m(x) + \\cfrac{1}{2}[\\cfrac{d^2}{d \\ (\\hat{y}^{(m-1)})^2} \\ l(y, \\hat{y}^{(m-1)})] f_m(x)^2 \\\\ & = l(y, F_{m-1}(x)) + [\\cfrac{d}{d \\ F_{m-1}} \\ l(y, F_{m-1})]f_m(x) + \\cfrac{1}{2}[\\cfrac{d^2}{d \\ F_{m-1}^2} l(y, F_{m-1})] f_m(x)^2 \\\\ &= l(y, F_{m-1}(x)) + gf_m(x) + \\cfrac{1}{2}hf_m(x)^2 \\end{aligned}\\tag{7} $\\hat{y}^{(m-1)}$ 為 XGB 在 $m-1$ 步的 prediction $F_{m-1}(x)$ $g = \\cfrac{d}{d \\ F_{m-1}} \\ l(y, F_{m-1})$ $h = \\cfrac{d^2}{d^2 \\ F_{m-1}} l(y, F_{m-1})$ 將式 (7) 代入式(5)： \\begin{aligned} \\mathcal{L}^{(m)}& \\approx \\ [ \\sum_i^n(l(y_i, \\hat{y}^{(m-1)}_i) +g_if_m(x_i) + \\cfrac{1}{2}h_if_m(x_i)^2 ] + \\Omega({f_{m}}) \\end{aligned} \\tag{8} $g_i$ 為 first derivative of loss function related to data sample $x_i$ $h_i$ 為 second derivative of loss function related to data sample $x_i$ $f_m(x_i)$ 為 $x_i$ 在 XGB tree $f_m(x)$ 的 output value 式 (8) 即 XGBoost 在 $m$ 步的 cost function $\\mathcal{L}^{(m)}$ 束手就擒 Second Order Tayler Approximation 逼近 loss function $l(y, \\hat{y})$後 ，對 $\\mathcal{L}^{(m)}$ 求 $\\gamma_{j,m}$ 極值，就變得容易多了 先將 regularization term $\\Omega({f_{m}})$ 展開代入式 (8)，並拿掉之後對微分沒影響的常數項 $l(y_i, \\hat{y}^{(m-1)}_i)$ \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)}& = [ \\sum_i^n( g_if_m(x_i) + \\cfrac{1}{2}h_if_m(x_i)^2) ] + [\\tau T_m + \\cfrac{1}{2}\\lambda \\sum^T_{j} \\gamma_{m,j}^2] \\\\ &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned} \\tag{9} $T_m$ is the number of leaf in tree $f_m$ $\\tau$ 表對 $T_m$ 的 factor $\\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$ 第 $j$ 個 leaf node 的輸出值 $R_{m,j}$ 表 leaf node j 下的 data sample $x$ 集合 第一個等式到第二個等式 : 從原本遍歷所有 data sample $x$ 到遍歷所有 leaf node $R_{m,j}$ 下的 data sample 式 (9) 對 $\\gamma_{j,m}^2$ 微分取極值 \\cfrac{d}{d \\ \\gamma_{j, m}} \\mathcal{\\tilde{L}}^{(m)} = \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j} }g_i) + (\\sum_{i \\in R_{j,m}}h_i)\\gamma_{j,m}] = 0 \\tag{10}對於已經固定結構了 tree $f_m(x)$，式 (10) leaf node $j$ 的 output value $\\gamma_{j,m}$ 為 \\gamma_{j,m}^*=- \\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} \\tag{11} $g_i$ 為 first derivative of loss function related to data sample $x_i$ : $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, F_{m-1}(x_i))$ $h_i$ 為 second derivative of loss function related to data sample $x_i$: $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$ 將 式(11) $\\gamma_{j,m}^*$ 代回 式 (9)，即 objective function 的極值 \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\\\ &=\\sum^{T_m}_{j=1}[-(\\sum_{i \\in R_{j,m}}g_i)\\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)(\\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda})^2 ] + \\tau T_m \\\\ & = \\sum^{T_m}_{j=1}[-\\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} + \\cfrac{1}{2} \\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda}] + \\tau T_m \\\\ & = -\\cfrac{1}{2} \\sum^{T_m}_{j=1}[\\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} ] + \\tau T_m \\end{aligned} \\tag{12}總結XGBoost 通用 Objective Function \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned} $g_i$ 為 first derivative of loss function related to data sample $x_i$ : $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, F_{m-1}(x_i))$ $h_i$ 為 second derivative of loss function related to data sample $x_i$: $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$ $T_m$ is the number of leaf in tree $f_m$ $\\tau$ 表對 $T_m$ 的 factor $\\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$ 第 $j$ 個 leaf node 的輸出值 Optimal Output of Leaf Node $j$\\gamma_{j,m}^*=- \\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda}Optimal Value of Objective Function\\mathcal{\\tilde{L}}^{*(m)} = \\cfrac{1}{2} \\sum^{T_m}_{j=1}[\\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} ] + \\tau T_mWhy does Approximation with Taylor Expansion Work?Traditional Gradient Decent當我們在做 gradient decent 時，我們是在嘗試 minimize a cost function $f(x)$，然後讓 $w^{(i)}$ 往 negative gradient 的方向移動 w^{(i+1)} =w^{(i)} - \\nabla f(w^{(i)})但 gradient $\\nabla f(w^{(i)})$ 本身只告訴我們方向，不會告訴我們真正的 the minimum value，我們得到的是每步後盡量靠近 the minimum value 一點，這使的我們無法保證最終到達極值點 同樣的表達式下的 gradient boost machine 也相似的的問題 F_m(x) = F_{m-1}(x) + \\nu f_{m}(x)傳統的 GBDT 透過建一顆 regression tree $f_m(x)$ 和合適的 step size $\\nu$ (learning rate) 擬合 negative gradient 往低谷靠近，可以說 GBDT tree $f_m(x)$ 本身就代表 loss function 的 $\\text{negative graident}$ 的方向 Better Gradient Decent: Newton’s MethodNewton’s method 是個改良自 gradient decent 的做法。他不只單純考慮了 gradient 方向 (即 first order derivative) ，還多考慮了 second order derivative 即所謂 gradient 的變化趨勢，這他更精準的定位極值的方向 w^{(i+1)} = w^{(i)} - \\cfrac{\\nabla f(w^{(i)})}{\\nabla ^2f(w^{(i)})} = w^{(i)} - \\cfrac{\\nabla f(w^{(i)})}{\\text{Hess}f(w^{(i)})}XGBoost 引入了 Newton’s method 思維，在建立子樹 $f_m(x)$ 不再單純只是 $\\text{negatvie gradient}$ 方向 (即 first order derivative) ，還多考慮了 second order derivative 即 gradient 的變化趨勢，這也是為什麼 XGBoost 全稱叫 Extreme Gradient Boosting f_m(x_i) = \\gamma_{j,m}=- \\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} $\\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$ 第 $j$ 個 leaf node 的輸出值 $g_i$ 為 first derivative of loss function related to data sample $x_i$ $h_i$ 為 second derivative of loss function related to data sample $x_i$ 而 loss function $l(y_i,\\hat{y_i})$ 會因不同應用: regression , classification, rank 而有不同的 objective function，並不一定存在 second order derivative，所以 XGBoost 利用 Taylor expansion 藉由 polynomial function 可以逼近任何 function 的特性，讓 loss function 在 $f_m(x_i)$ 附近存在 second order derivative \\begin{aligned} l(y,\\hat{y}^{(m-1)} + f_m(x)) \\approx l(y, F_{m-1}(x)) + gf_m(x) + \\cfrac{1}{2}hf_m(x)^2 \\end{aligned}可以說，XGBoos tree 以一種更聰明的方式往 the minimum 移動 總結 一般 GBDT 用 regression tree 擬合 residuals，本質上是往 negative gradient 方向移動 XGBoost tree $f_m(x)$ 擬合 residuals，同時考慮 gradient 的方向和 gradient 變化趨勢，這讓他朝 optimal value 移動時顯得更加聰明有效 gradient 的方向： first order derivative gradient 變化趨勢： second order derivative Reference Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. https://doi.org/10.1145/2939672.2939785 XGBoost Part 1 (of 4): Regression XGBoost Part 3 (of 4): Mathematical Details Tayler expansion XGBoost Loss function Approximation With Taylor Expansion https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion https://en.wikipedia.org/wiki/Taylor_series#Approximation_and_convergence https://en.wikipedia.org/wiki/Taylor’s_theorem","link":"/XGBoost-General-Objective-Function/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/cklbuf1gv000gc7e2crkx3j0q/"},{"title":"Word2Vec (2):Hierarchical Softmax 背後的數學","text":"以 CBOW 為例 $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ $\\text{v}’_j$ 表 $W’$ 中 j-th columns vector 在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是 leaf node ，跟 column vector $\\text{v}’_j$ 與 word $w_i$ 不是直接對應的關係 Hierarchical Softmax 優化了原始 softmax 分母 normalization 項需要對整個 vocabulary set 內的所有 word 內積，原始 softmax 如下 p(w | c) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_w})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$，is average of input context words’ vector representation in $W$ Hierarchical Softmax build a full binary tree to avoid computation over all vocabulary，示意圖： 每個 leaf node 代表一個 word $w_i$ Matrix $W^{‘}$ 就是所有 non-leaf node $n$ 代表的 vector $\\text{v}^{‘}_n$ 集合，與 word $w_i$ 無一對一對應關係 Binary tree 中每個 node 分岔的 probability 是個 binary classification problem $p(n, \\text{left}) = \\sigma({\\text{v}’_n}^{\\top} h)$ $p(n, \\text{righ}) = 1 - p(\\text{left},n) = \\sigma(-{\\text{v}’_n}^{\\top} h)$ $\\text{v}^{‘}_{n}$ 代表 node $n$ 的 vector 則每個 word $w_O = w_i$ 的 generate probability $p(w_i)$ 為其從 root node 分岔到 lead node 的 probability $p(w_i = w_O) = \\prod^{L(w_O)-1}_{j=1} \\sigma(\\mathbb{I}_{\\text{turn}}(n(w_O, j), n(w_O, j + 1) \\cdot {v^{‘}_{n(w_O, j)}}^{\\top}h)$ $w_O$ 表 output word 的意思 $L(w_O)$ is the depth of the path leading to the output word $w_O$ $\\mathbb{I}_{turn}$ is a specially indicator function 1 if $n(w_O, k+1)$ is the left child of $n(w_O, k)$ -1 if $n(w_O, k+1)$ is the right child of $n(w_O, k)$ $n(w, j)$ means the $j$ th unit on the path from root to the word $w$ $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$ average of all context word vector 簡單的例子Ex 1: Following the path from the root to leaf node of $w_2$, we can compute the probability of $w_2$ $p(w_2)$: Ex 2: $\\sum^{V}_{i=1} p(w_i = w_O) = 1$ probability $p(\\text{cat}| context)$, 是由 $ node1 \\stackrel{\\text{left}}{\\to} node \\stackrel{\\text{right}}{\\to} node 5 \\stackrel{\\text{right}}{\\to} cat $ 這條路徑組成 其中 context words 經過 hidden layer 後的輸出為 $h(\\text{context words})$ 為什麼 Hierarchical Softmax 可以減少 Time Complexity?透過 Hierarchical Softmax ， 原本計算 $p(w|c)$ 需要求所有 word $w_i$ 的 vector $\\text{v}_{w_i}$對 $h$ 的內積，現在只需要計算路徑上的節點數 $L(w_i) -1$，又 HS 是 full binary tree ，其最大深度為 $\\log_2|V|$ So we only need to evaluate at most $log_2|V|$ Hierarchical Softmax 如何 update 參數Error Funtion of Hierarchical SoftmaxError function $E$ is negative log likelihood $L(w_i) -1$ 表 從 root node 到 leaf node of $w_i$ 的 node number $[ \\cdot ]$表分岔判斷 $h = \\frac {1}{C} \\sum^{C}_{c=1}\\text{v}_{w_c}$ average of all context word vector And we use gradient decent to update $\\text{v}^{‘}_j$ and $h$ in $W’$ and $W’$ Calculate the Derivate $E$ with Regard to $\\text{v}^{‘\\top}_jh$先求 total loss 對 $\\text{v}^{‘\\top}_jh$ 的 gradient $\\sigma^{‘}(x) = \\sigma(x)[1 - \\sigma(x)]$ $[\\log\\sigma(x)]^{‘} = 1 - \\sigma(x)$ ⇒ $[log(1 - \\sigma(x)]^{‘} = -\\sigma(x)$ Calculate the Derivate $E$ with Regard to $\\text{v}^{‘}_j$根據 chain rule 可以求出 total loss 對 huffman tree node vector $\\text{v}^{‘}_j$ 的 gradient Update Equation Calculate the Derivate $E$ with Regard to $h$ 最後求 total loss 對 hidden layer outpot $h$ 的 gradient $EH$: an N-dim vector, is the sum of the output vector of all words in the vocabulary, weighted by their prediction error Update EquationBecause hidden vector $h$ is composed with all the context word $w_{I,c}$ $\\text{v}_{w_{I,c}}$ is the input vector of c-th word in input context ImplementCBOW + HS 實現 Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax Reference Language Models, Word2Vec, and Efficient Softmax Approximations https://rohanvarma.me/Word2Vec/ Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722","link":"/hierarchical-softmax-in-word2vec/"},{"title":"Word2Vec (1):NLP Language Model","text":"General Form p(w_1 , \\cdots , w_T) = \\prod\\limits_i p(w_i \\: | \\: w_{i-1} , \\cdots , w_1)展開來後 $P(w_1, w_2, w_3,…,w_T) = P(w_1)P(x_2|w_1)P(w_3|w_2, w_1)…P(w_T|w_1,…w_{T-1})$ EX Ngram Model根據 Markov assumption， word $i$ 只跟其包含 $i$ 的連續 $n$ 個 word 有關, 即前面 $n -1$ 個 word p(w_1 , \\cdots , w_T) = \\prod\\limits_i p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-n+1})其中 conditional probability 統計 corpus 內的 word frequency，可以看到 ngram 只跟前面 $n-1$ 個 word 有關 p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-n+1}) = \\dfrac{count(w_{i-n+1}, \\cdots , w_{i-1},w_o)}{count({w_{i-n+1}, \\cdots , w_{i-1}})}如果 $k=2$ 則稱為 bigram model : p(w_i|w_1, w_2, ... w_{i-1}) \\approx p(w_i|w_{i-1})最簡單的實作直接統計 corpus 內的 word frequency 得到 conditional probability: 但通常泛化性不足，所以用 neural network 與 softmax 來泛化 n gram conditional probability $p(w_i \\: | \\: w_{i-1} , \\cdots , w_{i-i+1})$ Neural Network ImplementationIn neural network, we achieve the same objective using the softmax layer $p(w_t \\: | \\: w_{t-1} , \\cdots , w_{t-n+1}) = \\dfrac{\\text{exp}({h^\\top v’_{w_t}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top v’_{w_i}})}$ $h$ is the output vector of the penultimate network layer $v^{‘}_{w}$ is the output embedding of word $w$ the inner product $h^\\top v’_{w_t}$ computes the unnormalized log probability of word $w_t$ the denominator normalizes log probability by sum of the log-probabilities of all word in $V$ Implement Ngram model with PytorchCreating Corpus and Training Pairs1234567891011121314151617181920212223import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimtest_sentence = &quot;&quot;&quot;When forty winters shall besiege thy brow,And dig deep trenches in thy beauty's field,Thy youth's proud livery so gazed on now,Will be a totter'd weed of small worth held:Then being asked, where all thy beauty lies,Where all the treasure of thy lusty days;To say, within thine own deep sunken eyes,Were an all-eating shame, and thriftless praise.How much more praise deserv'd thy beauty's use,If thou couldst answer 'This fair child of mineShall sum my count, and make my old excuse,'Proving his beauty by succession thine!This were to be new made when thou art old,And see thy blood warm when thou feel'st it cold.&quot;&quot;&quot;.split()trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2]) for i in range(len(test_sentence) - 2)]vocab = set(test_sentence)word_to_idx = {word: i for i, word in enumerate(vocab)} Define N Gram Model123456789101112131415class NGramLanguageModel(nn.Module): def __init__(self, vocab_size, embedding_dim, context_size): super(NGramLanguageModel, self).__init__() self.embeddings = nn.Embedding(vocab_size, embedding_dim) self.linear1 = nn.Linear(context_size * embedding_dim, 128) self.linear2 = nn.Linear(128, vocab_size) def forward(self, inputs): embeds = self.embeddings(inputs).view(1, -1) out = F.relu(self.linear1(embeds)) out = self.linear2(out) log_probs = F.log_softmax(out, dim=1) return log_probs Training1234567891011121314151617181920CONTEXT_SIZE = 2EMBEDDING_DIM = 10loss_function = nn.NLLLoss()net = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)optimizer = optim.SGD(net.parameters(), lr=0.001)losses = []for epoch in range(10): total_loss = 0 for context, target in trigrams: context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long) net.zero_grad() log_probs = net(context_idxs) loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long)) loss.backward() optimizer.step() total_loss += loss.data print(&quot;epcoh {} loss {}&quot;.format(epoch, total_loss)) losses.append(total_loss) Fetch Embedding1emb = net.embeddings(torch.tensor([i for i in range(len(vocab))])).detach().numpy() Reference on word embeddings https://ruder.io/word-embeddings-1/ https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html","link":"/NLP-language-model/"},{"title":"Word2Vec (3):Negative Sampling 背後的數學","text":"以下用 Skip-gram 為例 $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ Noise Contrastive Estimation (NCE) NCE attempts to approximately maximize the log probability of the softmax output The Noise Contractive Estimation metric intends to differentiate the target word from noise sample using a logistic regression classifier 從 cross entropy 說起 True label $y_i$ is 1 only when $w_i$ is the output word: \\mathcal{L}_\\theta = - \\sum_{i=1}^V y_i \\log p(w_i | w_I) = - \\log p(w_O \\vert w_I)又 $p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ $\\text{v}^{‘}_{w_i}$ is vector of word $w_i$ in $W^{‘}$ $w_O$ is the output word in $V$ $w_I$ is the input word in $V$ 代入後 \\mathcal{L}_{\\theta} = - \\log \\frac{\\exp({\\text{v}'_{w_O}}^{\\top}{\\text{v}_{w_I}})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top}{\\text{v}_{w_I} })} = - {\\text{v}'_{w_O}}^{\\top}{\\text{v}_{w_I} } + \\log \\sum_{i=1}^V \\exp({\\text{v}'_{w_i} }^{\\top}{\\text{v}_{w_I}})Compute gradient of loss function w.s.t mode’s parameter $\\theta$，令 $z_{IO} = {\\text{v}’_{w_O}}^{\\top}{\\text{v}_{w_I}}$ ; $z_{Ii} = {\\text{v}’_{w_i}}^{\\top}{\\text{v}_{w_I}}$ \\begin{aligned} \\nabla_\\theta \\mathcal{L}_{\\theta} &= \\nabla_\\theta\\big( - z_{IO} + \\log \\sum_{i=1}^V e^{z_{Ii}} \\big) \\\\ &= - \\nabla_\\theta z_{IO} + \\nabla_\\theta \\big( \\log \\sum_{i=1}^V e^{z_{Ii}} \\big) \\\\ &= - \\nabla_\\theta z_{IO} + \\frac{1}{\\sum_{i=1}^V e^{z_{Ii}}} \\sum_{i=1}^V e^{z_{Ii}} \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\sum_{i=1}^V \\frac{e^{z_{Ii}}}{\\sum_{i=1}^V e^{z_{Ii}}} \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\sum_{i=1}^V p(w_i \\vert w_I) \\nabla_\\theta z_{Ii} \\\\ &= - \\nabla_\\theta z_{IO} + \\mathbb{E}_{w_i \\sim Q(\\tilde{w})} \\nabla_\\theta z_{Ii} \\end{aligned}可以看出，gradient $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$是由兩部分組成 : a positive reinforcement for the target word $w_O$, $\\nabla_{\\theta}z_{O}$ a negative reinforcement for all other words $w_i$, which weighted by their probability, $\\sum_{i=1}^V p(w_i \\vert w_I) \\nabla_\\theta z_{Ii}$ Second term actually is just the expectation of the gradient $\\nabla_{\\theta}z_{Ii}$ for all words $w_i$ in $V$。 And probability distribution $Q(\\tilde{w})$ could see as the distribution of noise samples NCE sample 原理According to gradient of loss function $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$ \\nabla_{\\theta}\\mathcal{L}_{\\theta} =- \\nabla_\\theta z_{IO} + \\mathbb{E}_{w_i \\sim Q(\\tilde{w})} \\nabla_\\theta z_{Ii}Given an input word $w_I$, the correct output word is known as $w$，我們同時可以從 noise sample distribution $Q$ sample 出 $M$ 個 samples $\\tilde{w}_1, \\tilde{w}_2, \\dots, \\tilde{w}_M \\sim Q$ 來近似 cross entropy gradient 的後半部分 現在我們手上有個 correct output word $w$ 和 $M$ 個從 $Q$ sample 出來的 word $\\tilde{w}$ ， 假設我們有一個 binary classifier 負責告訴我們，手上的 word $w$，哪個是 correct $p(d=1 | w, w_I)$，哪個是 negative $p(d=0 | \\tilde{w}, w_I)$ 於是 loss function 改寫成： \\mathcal{L}_\\theta = - [ \\log p(d=1 \\vert w, w_I) + \\sum_{i=1, \\tilde{w}_i \\sim Q}^M \\log p(d=0|\\tilde{w}_i, w_I) ]According to the law of large numbers $E_{p(x)} [ f(x)] \\approx \\frac{1}{n} \\sum^{n}_{i=1}f(x_i)$，we could simplify: \\mathcal{L}_\\theta = - [ \\log p(d=1 \\vert w, w_I) + M \\mathbb{E}_{\\tilde{w}_i \\sim Q} \\log p(d=0|\\tilde{w}_i, w_I)]$p(d |w, w_I)$ 可以從 joint probability $p(d, w|w_I)$ 求 $p(d, w | w_I) =\\begin{cases} \\frac{1}{M+1} p(w \\vert w_I) &amp; \\text{if } d=1 \\\\ \\frac{M}{M+1} q(\\tilde{w}) &amp; \\text{if } d=0 \\end{cases}$ $d$ is binary value $M$ is number of samples，we have a correct output word $w$ and sample M word from distribution $Q$ 因爲 $p(d| w, w_I) = \\frac{p(d, w, w_I)}{p(w, w_I)} = \\frac{p(d, w | w_I) p(w_I)}{p(w|w_I)p(w_I)} = \\frac{p(d, w| w_I)}{\\sum_dp(d,w| w_I)}$ 可以得出 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(d=1, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned} \\begin{aligned} p(d=0 \\vert w, w_I) &= \\frac{p(d=0, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{Mq(\\tilde{w})}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned} $q(\\tilde{w})$ 表從 distribution $Q$ sample 出 word $\\tilde{w}$ 的 probability 最終 loss function of NCE \\begin{aligned} \\mathcal{L}_\\theta & = - [ \\log p(d=1 \\vert w, w_I) + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log p(d=0|\\tilde{w}_i, w_I)] \\\\ & = - [ \\log \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{p(w \\vert w_I) + Mq(\\tilde{w}_i)}] \\end{aligned}$p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 代入 $\\mathcal{L}_{\\theta}$ \\begin{aligned}\\mathcal{L}_{\\theta} &= -[log\\frac{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})}}{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})+ Mq(\\tilde{w})}} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i\\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{\\frac{\\exp({\\text{v}'_{w}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}'_{w_i}}^{\\top} \\text{v}_{w_I})}+ Mq(\\tilde{w}_i)}]\\end{aligned}可以看到 normalizer $Z(w) = {\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 依然要 sum up the entire vocabulary，實務上通常會假設 $Z(w)\\approx1$，所以 $\\mathcal{L}_\\theta$ 簡化成: \\mathcal{L}_\\theta = - [ \\log \\frac{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}})}{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}}) + Mq(\\tilde{w})} + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\frac{Mq(\\tilde{w}_i)}{\\exp({\\text{v}'_w}^{\\top}{\\text{v}_{w_I}}) + Mq(\\tilde{w}_i)}]關於 Noise distribution $Q$關於 noise distribution $Q$，在設計的時候通常會考慮 it should intuitively be very similar to the real data distribution. it should be easy to sample from. Negative Sampling (NEG)Negative sampling can be seen as an approximation to NCE Noise Contrastive Estimation attempts to approximately maximize the log probability of the softmax output The objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modeling 從 NCE 說起NCE $p(d|w, w_I)$ equation，$p(d=1 |w, w_I)$ 代表 word $w$ 是 output word 的機率; $p(d=0|w, w_I)$ 表 sample 出 noise word 的機率 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(d=1, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned}\\begin{aligned} p(d=0 \\vert w, w_I) &= \\frac{p(d=0, w \\vert w_I)}{p(d=1, w \\vert w_I) + p(d=0, w \\vert w_I)} &= \\frac{Mq(\\tilde{w})}{p(w \\vert w_I) + Mq(\\tilde{w})} \\end{aligned}NCE 假設 $p(w_O \\vert w_I) = \\frac{\\exp({\\text{v}’_{w_O}}^{\\top} \\text{v}_{w_I})}{\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})}$ 中的分母 $Z(w) = {\\sum_{i=1}^V \\exp({\\text{v}’_{w_i}}^{\\top} \\text{v}_{w_I})} = 1$，所以簡化成 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{p(w \\vert w_I)}{p(w \\vert w_I) + Mq(\\tilde{w})} &= \\frac{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})}{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) + Mq(\\tilde{w})} \\end{aligned}NEG 繼續化簡NEG 繼續假設 $Nq(\\tilde{w}) = 1$ 式子變成 \\begin{aligned} p(d=1 \\vert w, w_I) &= \\frac{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})}{\\exp({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) + 1} &= \\frac{1}{1 +\\exp(-{\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})} = \\sigma({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) \\end{aligned}p(d=0|w, w_I) = 1 - \\sigma({\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i}) = \\sigma(-{\\text{v}^{'}_w}^{\\top}\\text{v}_{w_i})最終得到 loss function \\mathcal{L}_\\theta = - [ \\log \\sigma({\\text{v}'_{w}}^\\top \\text{v}_{w_I}) + \\sum_{\\substack{i=1 \\\\ \\tilde{w}_i \\sim Q}}^M \\log \\sigma(-{\\text{v}'_{\\tilde{w}_i}}^\\top \\text{v}_{w_I})]前項是 positive sample $p(d=1 \\vert w, w_I)$，後項是 M 個 negative samples $p(d=0|w, w_I) $ 在 skipgram with negative sampling 上 $\\text{v}_{w_I}$ 表 input 的 center word $w_I$ 的 vector，來自 $W$ $\\text{v}’_{w}$ 表 output side 的一個 context word $w$ 的 vector， 來自 $W’$ 實作上 skipgram 一個單位的 data sample 只會包含 一個 center word 與 一個 context word 的 pair 對 $(w_I, w_{C,j})$ 參閱 Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling 結論 NEG 實際上目的跟 Hierarchical Softmax 不一樣，並不直接學 likelihood of correct word，而是直接學 words 的 embedding，也就是更好的 word distribution representation Reference Learning Word Embedding https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions 此篇從 skip gram 講解 negative sampling On word embeddings - Part 2: Approximating the Softmax https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches 此篇從 CBOW 講解 negative sampling Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from http://arxiv.org/abs/1402.3722 Word2Vec Tutorial Part 2 - Negative Sampling [http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/","link":"/negative-sampling-in-word2vec/"},{"title":"內容推薦 (2) Title Embedding with Keyword","text":"前言在前篇 內容推薦 (1) 關鍵詞識別 中，我們利用 entropy 從商品池的 title 中辨識出 product word &amp; label word 此篇，我們將利用已經辨識出的 product word &amp; label word 回頭對商品池中的商品 title 做 embedding 當然你也可以直接將所有 title 送進 Word2Vec 硬 train 一發，然後對 title 內的所有的 word vectors 取平均得到 title vector。 推薦系統相關文章 Weight Keyword Embedding假設我們有一個 title ，我們希望能根據 word 在 title 中的重要程度將他 embedding 化，要怎麼做？ 1'summer fisherman hat female outdoor sun hat sun hat japanese student basin hat watch travel fishing sun hat male' 從 CBOW 說起CBOW 的思想是用兩側 context words 去預測中間的 center word P(center|context;\\theta)換句話說，給定 context words 集合 $w_{I,C}$， word $w_j$ 是 center word $w_O$ 的 probability 越大 是否代表 $w_j$ 在 context $C$ 中越關鍵？ P(w_O = w_j |w_{I,C};\\theta)如果上面的推測成立的話，CBOW 在 Hierarchical Softmax 下的 objective function: negative log likelihood \\begin{aligned} & -\\log p(w_O| w_I) = -\\log \\dfrac{\\text{exp}({h^\\top \\text{v}'_O})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} \\\\ & = - \\sum^{L(w_O)-1}_{l=1} \\log\\sigma([ \\cdot] h^\\top \\text{v}^{'}_l) \\end{aligned} \\tag{1} CBOW with Hierarchical Softmax 有兩個 matrix $W$ and $W’$ $W$ 的 row vector 對應到 word $w_i$ 的 vector $W’$ 對應的是 huffman tree non-leaf node 的 vector 參見 Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax $\\text{v}’_j$ 表 output side matrix $W’$ 中 j-th columns vector，跟任何 word 沒一對一對應關係 $L(w_i) -1$ 表 huffman tree 中從 root node 到 leaf node of $w_i$ 的 node number $[ \\cdot ]$表 huffman tree 的分岔判斷 $[ \\cdot ] = 1$ 表 turn left $[ \\cdot ] = -1$ 表 turn right $h = \\frac {1}{C} \\sum^{C}_{j=1}\\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$ Score function $\\log p(w_O| w_I)$ (沒負號)， 本質上是對 output word $w_O$ 的打分。 先改寫一下 score function，等等會用到 \\begin{aligned} \\log p(w_O| w_I) &= \\sum^{L(w_O)-1}_{l=1}\\log\\sigma([ \\cdot] h^\\top \\text{v}^{'}_l) \\\\ &= \\sum^{L(w_O)-1}_{l=1} \\log(\\cfrac{1}{1+ \\exp^{- [ \\cdot] h^{\\top} v_l^{'}}}) \\\\ &= \\sum^{L(w_O)-1}_{l=1} [\\log(1) -\\log(1+ \\exp^{- [ \\cdot] h^{\\top} v_l^{'}}) ]\\\\ & = \\sum^{L(w_O)-1}_{l=1}-\\log(1 + \\exp^{- [ \\cdot] h^{\\top} v_l^{'}}) \\end{aligned} \\tag{2}有了 式(2) score function ，給定一 title words 集合 $w_{T}$ 只要對 title 裡的每個 word $w_j \\in w_{T}$ ，令 $\\log p(w_O=w_j|w_I = w_{T, \\lnot j})$，進行打分即可得到每個 word $w_j$ 在 title 裡的重要程度 \\text{weight}_j = \\log p(w_O=w_j|w_I = w_{T, \\lnot j}) \\tag{3}而我們要的 title embedding 即 weighted sum of words in title \\text{v}_{\\text{title}} = \\sum_{w_j \\in w_T}\\text{weight}_j \\times \\text{v}_{w_{j}} \\tag{4} $w_T$: 某 title 的 word 集合 $\\text{v}_{w_{j}}$: word $w_j$ 對應 matrix $W$ 中 row vector Gensim 實作Train CBOW + HS12345678910111213141516171819from gensim.models import Word2Vecw2v_model = Word2Vec( min_count=3, window=5, size=100, alpha=0.005, min_alpha=0.0007, hs=1, sg=0, workers=4, batch_words=100, cbow_mean = 1 )w2v_model.build_vocab(corpus) # build huffman treew2v_model.train( corpus, total_examples=w2v_model.corpus_count, epochs=50, report_delay=1) corpus 裡的每個 title，應該已經先行合併 bigram and trigram 的 product &amp; label 詞 ，最好可以去除無用詞，如下 sentence，某些 words 已合併 : 1'hyuna_style ins cute wild sunscreen female hand sleeves arm_guard ice_silk sleeves driving anti_ultraviolet ice gloves tide' Scoring Words in Title訓練完 model 後，對每個 title 內的 words 重要性進行評分: 12345def cal_log_probs(model, target_w, context_embd: np.ndarray)-&gt; np.ndarray: turns = (-1.0) ** target_w.code path_embd = model.trainables.syn1[target_w.point] log_probs = -np.logaddexp(0, -turns * np.dot(context_embd, path_embd.T)) return np.sum(log_probs) 為 式 (2) 的實現，實際上就是 gensim Word2Vec 內的 score_cbow_pair word 在 huffman tree 的 path code 是 0/1 code，使用時須轉換成 -1 or 1 model.trainables.syn1 即 $W’$ ，存放 huffman tree non-leaf node 的 vector 12345678910111213141516171819202122def _cal_keyword_score(model, sentence:List[str]) -&gt; Dict[str, float]: word_vocabs = [model.wv.vocab[w] for w in sentence if w in model.wv.vocab] word_importance = {} for pos_center, center_w in enumerate(word_vocabs): context_w_indices = [w.index for pos_w, w in enumerate(word_vocabs) if pos_center != pos_w] context_embed = np.mean(model.wv.vectors[context_w_indices], axis=0) log_probs = cal_log_probs(model, center_w, context_embed) center_w_term = w2v_model.wv.index2word[center_w.index] word_importance[center_w_term] = word_importance.get(center_w_term, 0) + log_probs return word_importancedef cal_keyword_score(model, sentence: List[str]) -&gt; np.ndarray: word_importance = _cal_keyword_score(model, sentence) ds = pd.Series(word_importance).sort_values(ascending=False) scalar = MinMaxScaler(feature_range=(0.1, 1)) array = ds.to_numpy() array = scalar.fit_transform(array.reshape(array.shape[0], -1)) ds = pd.Series(array.reshape(-1, ), index=ds.index) return ds model.wv.vectors 存放 $W$，即訓練完後每個 word 的 vector MinMaxScaler: 縮放到 0.1 到 1 是為了方便觀察 使用方式如下 In: 123sent = corpus_with_bigram_trigram[7676]ds = cal_keyword_score(w2v_model, sent)print(sent), print(ds) Out: 123456789101112131415['haining', 'leather', 'male', 'stand_collar', 'middle_aged', 'coat', 'fur', 'one', 'winter', 'cashmere', 'thick', 'money', 'father_loaded']coat 1.000000leather 0.874738fur 0.861750middle_aged 0.812752winter 0.773609male 0.734654stand_collar 0.699505thick 0.676800cashmere 0.642869one 0.546631haining 0.457806father_loaded 0.393533money 0.100000dtype: float64 Weighted Sum of Word Vectors從 w2v_model 中取出某 title 內所有 words 的 vector 做 weighed sum 123456def weighted_sum_w2v(w2v_model, ds: pd.Series) -&gt; np.ndarray: ds_ = ds.copy() / sum(ds) w2v = w2v_model.wv[ds_.index] weights = np.expand_dims(ds_.to_numpy(), 1) return np.sum((w2v * weights), axis=0) 得出的 title vector is un-normalized ，要使用前得先 L2-norm 參閱 bible 做的示例 notebook seed9D/hands-on-machine-learning Title Embedding 應用title embedding 最直覺的應用是 content I2I，用戶點擊了 商品 A，我們就可以透過 商品 A 的 title vector 召回 TopK 個最相似 title 的商品推薦給他。 而 title embedding with keyword weighting 中，我們將 product word 與 label word 在 title 中的重要程度進行 weighted sum，能更準確的表達 title 的意思，不再只是簡單的對 word vector 取平均，連一些無用詞的 vector 也混進去。 不過 title embedding 在推薦的效果不如利用用戶交互數據訓練出來的 embedding，但因爲每個商品一定會有 title， 很適合作為商品冷啟動召回策略之一使用。在我負責的推薦應用裡，也是利用 title embedding 關聯新商品到有交互數據的舊商品上後讓新商品取得曝光機會。 title embedding 結合 label 詞 &amp; product 詞的另一個業務應用就是卡片式的主題推薦，類似淘寶上的一個頁面就講一個購物主題，選定一個主題 (ex: 旅遊)與某樣你曾經互動過商品進行推薦 這個算法側的實作不難，留到下次說吧 Reference 【不可思议的Word2Vec】 3.提取关键词 https://spaces.ac.cn/archives/4316 以 skipgram 角度計算 my post Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax Word2Vec (2):Hierarchical Softmax 背後的數學 gensim https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html Gensim Word2Vec Tutorial [https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial","link":"/title-embedding-with-keywords/"},{"title":"內容推薦 (3):主題卡片推薦","text":"什麼是主題卡片推薦？主題推薦，就是給定一個概念，然後推薦系統圍繞著這個主題將將相關聯的商品推薦給用戶。 “主題” 是個超越類目, 產品詞或標籤的存在，他可以是： 旅行出行 文青 寬鬆顯瘦 愛車人 追星族 愛美 等於是把商品池中的類目關係解構又重構出一個新的子集。 以用戶端接觸的 UI 來看，主題卡片推薦由兩個構成： Feed 流卡片推薦與 Landing Page 主題推薦。 推薦系統相關文章 Feeds 流卡片推薦Feeds 流卡片推薦是在推薦流裡面展示一張卡片，這張卡片 summarize 了最今用戶點擊的商品，形成一個主題展示給用戶。 卡片上會有一張圖片以及主題詞吸引用戶點擊，如上圖的 Tea Set 與其搭配圖。 卡片產生方式主要有二，兩者可以互補使用 人工運營 算法自動化 人工運營即人工事先的定義好商品池跟搭配語，只要用戶的歷史交互商品達到觸發門檻就推薦給他，人工運營的好處是可以精確的定義某類風格而且很難出現 bad case，EX：日系穿搭，文青風 …. 算法自動化就得利用商品的 label 詞與 product 詞，產生 商品-詞 pair。 最立即有效的做法，直接將 product and label phrase 按照其在 title 中的重要性程關聯給商品。當用戶達到主題觸發門檻，就從主題下挑選出一個商品以及其中一個 keyword 做為展示詞形成卡片。 product &amp; label phrase 挖掘參考 內容推薦 (1) 關鍵詞識別 product and label phrase 在 title 中的重要性程度可以參考 內容推薦 (2) Title Embedding with Keyword Landing Page 主題推薦當用戶點擊卡片後會跳轉到此頁，此頁的推薦必須圍繞著 trigger 卡片的主題詞以及商品展開 此頁可以用的召回有 主題詞下的其他商品召回 trigger 商品的 label &amp; product 召回 trigger 商品的 title embedding 召回 trigger 商品的 image I2I …etc 基本上只要符合主題的都能用，但得注意過多同類型商品會造成用戶疲勞，所以多樣性還是得考慮。 實作 Tips關於效用這就得提到為什麼需要在 feed 流展示卡片 首頁 feeds 流的定位比較偏向讓用戶去逛，所以很大的考量推薦商品列表中商品間的多樣性程度，因此需要另一個空間來將同質性高的商品聚合展示，讓用戶看中一樣商品後不搜索直接點進去開始挑選比較。 其二是透過卡片上的文字展示，可以起到導引用戶心智的效果 EX：用戶可能只想買個被子，被 ＂居家＂主題吸引後，不知不覺其他東西都買了。 另外，加入卡片推薦之後，原本場景轉化肯定是會掉的，因爲卡片相當於是一個可以點擊但不能購買的 entity，得將 Landing page 內的指標歸因回 feeds 流才行。 事實上，卡片的加入原本就會傷害 feeds 流某些指標的轉化了，因為你提前讓用戶跳轉到其他頁面，所以對於卡片推薦的效用得從多方面評估。 儲存方式要找出 商品 → 標籤詞/產品詞 的關係，肯定會需要一個正排索引，一個類似這樣的結構： 12345678910111213{ &quot;trigger_key&quot;: &quot;2734070&quot;, &quot;pairs&quot;: [ { &quot;key&quot;: &quot;產品詞A&quot;, &quot;score&quot;: 21.713 }, { &quot;key&quot;: &quot;產品詞B&quot;, &quot;score&quot;: 15.63 } ]} 同時如果需要 產品詞/標籤詞 → 商品的對應關係，還會需要一個倒排索引，所以在持久化儲存方面得多調研，DynamoDB 的 GSI 是選項。 卡片圖片的選擇根據 AB 實驗顯示，如果想增加卡片的 CTR 和轉化，放他點擊過的商品的圖片準沒錯，不過記得跳轉進 Landing page 時得展示卡片圖片上的商品，不然會有種被欺騙的感覺。","link":"/topic-recommendation/"},{"title":"Word2Vec (0):從原理到實現","text":"這篇是在 notion 整理的筆記大綱，只提供綱要性的說明 預備知識 language model： NLP 語言模型 參閱 Word2Vec (1):NLP Language Model huffman tree 簡介兩種網路結構 Continuous bag of words (CBOW) &amp; Softmax CBOW feeds $n$ words around the target word $w_t$ at each step P(center|context;\\theta) $V$： the vocabulary size $N$ : the embedding dimension $W$： the input side matrix which is $V \\times N$ each row is the $N$ dimension vector $\\text{v}_{w_i}$ is the representation of the input word $w_i$ $W’$: the output side matrix which is $N \\times V$ each column is the $N$ dimension vector $\\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$ CBOW 的 Objective Function$J_\\theta = \\frac{1}{V}\\sum\\limits_{t=1}^V\\ \\text{log} \\space p(w_t \\: | \\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n})$ 其中 p(w_t \\: | \\: w_{t-n} , \\cdots , w_{t-1}, w_{t+1}, \\cdots , w_{t+n}) = \\cfrac{\\exp(h^\\top \\text{v}^{'}_{w_{t}})}{\\sum_{w_i \\in V}\\exp(h^\\top \\text{v}'_{w_i})} $n$ 表 window size $w_t$ 表 CBOW target center word $w_i$ 表 word $i$ in vocabulary $V$ $\\text{v}_{w_i}$ 表 matrix $W$ 中，代表 $w_i$ 的那個 row vector $\\text{v}’_{w_i}$ 表 matrix $W’$ 中，代表 $w_i$ 的那個 column vector $h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\\cfrac{1}{C}(\\text{v}_{w_1} + \\text{v}_{w_2}+ …+ \\text{v}_{w_C})^T$ Because there are multiple contextual words, we construct the input vector by averaging each words’s distribution representation Skipgram &amp; Softmax skipgram uses the centre word to predict the surrounding words instead of using the surrounding words to predict the centre word P(context|center; \\theta) Skipgram 的 Objective Function J_\\theta = \\frac{1}{V}\\sum\\limits_{t=1}^V\\ \\sum\\limits_{-n \\leq j \\leq n , \\neq 0} \\text{log} \\space p(w_{t+j} \\: | \\: w_t)其中 p(w_{t+j} \\: | \\: w_t ) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_{w_{t+j}}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})} $n$ 為 window size $w_{t+j}$ 表 skipgram target 第 j 個 context word $w_t$ 為 skipgram input 的 center word skip-gram 有兩個獨立的 embedding matrix $W$ and $W^{‘}$ $W$ : $V \\times N$ , $V$ is vocabulary size; N is vector dimension output matrix $W^{‘}$: $N \\times V$, encoding the meaning of context $\\text{v}^{‘}_{w_i}$ is column vector of word $w_i$ in $Ｗ^{‘}$ $h$ is the hidden layer’s output 事實上，skip-gram 沒有 hidden layer, 因爲 input 只有一個 word, $h$ 就是 word embedding $\\text{v}_{w_t}$of the word $w_t$ in $W$。 所以 $p(w_{t+j} \\: | \\: w_t ) = \\dfrac{\\text{exp}({\\text{v}^\\top_{w_t} \\text{v}’_{w_{t+j}}})}{\\sum_{w_i \\in V} \\text{exp}({\\text{v}^\\top_{w_t} \\text{v}’_{w_i}})}$ 兩種 loss function 優化原始 softmax 作為 objective function， 分母必須對所有 word $w_i$ in vocabulary 與 vector $ h$ 內積，造成運算瓶頸 p(w_O | w_I) = \\dfrac{\\text{exp}({h^\\top \\text{v}'_{w_{O}}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top \\text{v}'_{w_i}})}所以 word2Vec 論文中採用兩種 objective function 的優化方案，Hierarchical Softmax 與 Negatvie Sampling Hierarchical Softmax原理推導請參閱 Word2Vec (2):Hierarchical Softmax 背後的數學 Hierarchical softmax build a full binary tree to avoid computation over all vocabulary Negative Sampling原理推導請參閱 Word2Vec (3):Negative Sampling 背後的數學 negative sampling did further simplification because it focuses on learning high-quality word embedding rather than modeling the likelihood of correct word in natural language. In implement，negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them 實現 WordVec skip gram + softmax Word2Vec (4):Pytorch 實作 Word2Vec with Softmax CBOW + softmax Word2Vec (4):Pytorch 實作 Word2Vec with Softmax CBOW + hierarchical softmax Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax CBOW + negatove sampling skip gram + hierarchical softmax skip gram + negative sampling Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling ConclusionSkip gram 與 CBOW 實際上都 train 了兩個 embedding matrix $W$ and $W’$ $W:$ 在 C implement 稱作 $syn0$。 $W’$: 若採用 hierarchical softmax 稱為 $syn1$ 若採用 negative sampling 叫 $syn1neg$ 根據性質，$syn0$ 與 $syn1neg$ 是一體兩面的，本身都可以代表 word embedding，皆可使用。 而代表 $W’$ 的 $syn1$ 因爲連結的是 huffman tree 的 non leaf node，所以其本身對 word $w_i$ 沒直接意義。 Reference On word embeddings - Part 1 https://ruder.io/word-embeddings-1/ On word embeddings - Part 2: Approximating the Softmax https://ruder.io/word-embeddings-softmax/ Learning Word Embedding https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy other Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from http://arxiv.org/abs/1411.2738 Language Models, Word2Vec, and Efficient Softmax Approximations https://rohanvarma.me/Word2Vec/ Word2Vec Tutorial - The Skip-Gram Model word2vec原理(一) CBOW与Skip-Gram模型基础 The Illustrated Word2vec Word2vec数学原理全家桶 http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/ Word2Vec-知其然知其所以然 https://www.zybuluo.com/Dounm/note/591752#word2vec-知其然知其所以然 C source code Word2Vec源码解析 https://www.cnblogs.com/neopenx/p/4571996.html 應用 小白看Word2Vec的正确打开姿势|全部理解和应用 https://zhuanlan.zhihu.com/p/120148300 推荐系统从零单排系列(六)—Word2Vec优化策略层次Softmax与负采样 [https://zhuanlan.zhihu.com/p/66417229","link":"/word2vec-from-theory-2-implement/"},{"title":"透視 XGBoost(0) 總結篇","text":"此篇總結 透視 XGBoost 系列，建議系列閱讀順序為 透視 XGBoost(1) 圖解 Regression 透視 XGBoost(2) 圖解 Classification 透視 XGBoost(3) 蘋果樹下的 objective function 透視 XGBoost(4) 神奇 optimization 在哪裡？ XGBoost 通用 objective functionXGBoost 的 loss function 以 second order Taylor Expansion approximate，使的 loss function 存在 first and second order derivative \\begin{aligned} l(y,\\hat{y}^{(m-1)} + f_m(x)) \\approx l(y, F_{m-1}(x)) + gf_m(x) + \\cfrac{1}{2}hf_m(x)^2 \\end{aligned}通用 objective function 使 XGBoost 在 classification, regression, rank 的任務上表達式皆一致 \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned}請參閱 透視 XGBoost(3) 蘋果樹下的 objective function XGBoost v.s. GBDT 一般 GBDT 用 regression tree 擬合 residuals，本質上是往 negative gradient 方向移動 XGBoost tree $f_m(x)$ 擬合 residuals，同時考慮 gradient 的方向和 gradient 變化趨勢，這讓他朝 optimal value 移動時顯得更加聰明有效 gradient 的方向： first order derivative gradient 變化趨勢： second order derivative 請參閱 透視 XGBoost(3) 蘋果樹下的 objective function What Makes XGBoost so Effective?Avoiding Overfitting Objective function 內加入 regularization term，限制 leaf node 輸出大小 和 leaf node number \\begin{aligned} \\mathcal{L}& = [\\sum_i^n l(y_i, \\hat{y_i}) ] + \\sum_{m}\\Omega({f_m}) \\\\ & \\Omega(f_m) = \\tau T_m + \\cfrac{1}{2}\\lambda \\lVert \\gamma \\rVert ^2 \\end{aligned} 可以設置 min_child_weight 限制 each leaf node 的 cover 可以設置 max_depth 限制 XGB tree 的深度 Column sampling and Row sampling 參閱 透視 XGBoost(1) 圖解 Regression 透視 XGBoost(2) 圖解 Classification Split Findding 採用 approximate greedy algorithm 選取 candidate split 點，candidate split 基於 weighted quantile 切分 weighted Quantile Sketch 盡量使 quantiles 間的 summation of weight 相等 weight 計算直接來自 second derivative of loss function $h_i$ 參閱 透視 XGBoost(4) 神奇 optimization 在哪裡？ System Design 數據量大時，將 data samples 切分成 multi blocks 方便 parallel computing 訓練前 sorting block 內的 columns values，避免每次分裂皆要對特徵 (columns) 排序 開 thread pre-fetch $g_i \\ h_i$， 優化 cache hitting rate out-of-core blocks 將放不進 memory 的 block 成塊的放到 hard disks，並在要計算前 pre-fetch to memory using thread 參閱 透視 XGBoost(4) 神奇 optimization 在哪裡？ Other data samples 內的 missing value (NAs) 可以在訓練時學出來分裂位置 參閱 透視 XGBoost(4) 神奇 optimization 在哪裡？","link":"/what-make-XGBoost-so-effective/"},{"title":"透視 XGBoost(1) 圖解 Regression","text":"Introduction XGboost 是 gradient boosting machine 的一種實作方式，xgboost 也是建一顆新樹 $f_m(x)$ 去擬合上一步模型輸出 $F_{m-1}(x)$ 的 $\\text{residual}$ \\begin{aligned} F_m(x) & = F_{m-1}(x) + f_m(x) \\\\ F_{m-1}(x) &= F_0(x) + \\sum_{i=0}^{m-1}f_{m-1}(x) \\end{aligned}不同的是，Xgboost 做了大量運算和擬合的優化，這讓他比起傳統 GBDT 更為高效率與有效 跟擬合目標有關的有 Second Order Tayler Approximation Approximate Greedy Algorithm Weighted Quantile Sketch Sparsity-Aware Split Finding 跟工程優化有關的有 Cache - Aware Access Block for Out-of-Core Computation Parallel Learning 以下章節主要分兩大塊 XGBoost for Regression： 藉由 regression 介紹 XGBoost 如何 train 一棵 XGB tree，以圖文方式說明 XGB tree 如何擬合目標到剪枝。此章節不涉及公式證明，只有少量運算，適合快速理解 XGB 訓練流程 XGBoost Regression Math Background：此章節深入討論在前一章節中用到的公式原理，並給予證明，適合深入理解 XGBoost 為何 work 篇幅關係 XGBoost 的優化手段放在 透視 XGBoost(4) 神奇 optimization 在哪裡？ XGBoost for Regression從 gradient boosting 說起 XGBoost 跟 gradient boosting algorithm 框架一樣，皆是依序建立多棵樹 $f_1(x), f_2(x), ….,f_M(x)$ 組成模型 F_m(x) = F_0(x) + \\nu\\sum^m_{i=1} f_i(x) = F_{m-1} + \\nu f_{m-1}(x) 其中第 $m$ 步的 tree $f_m(x)$ 是擬合模型 $F_{m-1}(x)$ 預測值 $\\text{predicted }$ 與 真實值 $\\text{observed}$ 的 $\\text{residual}$ $\\nu$ 為 learning rate 算法差別主要體現在 objective function 的設計 Step 2 (B) (C) 建樹，GBDT 是建一顆傳統 regression tree $f_m(x)$ 去擬合 $\\text{residual}$; XGBoost 有自己衡量分裂 gain 的方式去擬合 residual 建立 XGB tree $f_m(x)$ 可以說 XGBoost 用一種比較精準的方式去擬合 residual 建立子樹 $f_m(x)$ Data之後用到的例子，數據如下： 假設有多筆 data sample，目標是用 drug dosage 預測 drug effectiveness 如何建一顆 XGB tree $f_m(x)$ ?Regression problem 上，XGB 在所有 data sample x 的每個特徵下的值裡尋找最佳分裂點，最終建成一顆 binary tree $f_m(x)$ 建樹的過程涉及 Fitting Target 擬合目標 分裂好壞的恆量，如 CART 用 gini gain 衡量分類樹 Pruning 剪枝 Output Value 決定每個 leaf node 的唯一輸出 $f_m(x)$ 擬合的目標是什麼？$f_m(x)$ 擬合的目標是 $\\text{residual}$ ，利用 data sample x 的所有特徵建一顆特殊的 $\\text{regression tree}$ 去擬合 $\\text{residual}$ \\text{residual = observed - predicted} 以 $m=1$ 時舉例 XGBoost 的 predict 初始值 $F_0(x)$，預設皆為 0.5 ep_0_predict: XGBoost 的初始預測值 $F_0(x)$，預設都是 0.5 ep_1_residual: $observed$ 與 $F_0(x)$ 間的 $residual$ 也就是 XGB tree $f_1(x)$ 要擬合的目標 建 tree 時如何衡量分裂點好壞？建分支時依序在特徵 $\\text{Drug Dosage}$ 的 data sample value $\\text{[10, 20, 25, 35]}$ 中尋找最優分裂點切分 residual $\\text{[-10.5, 6.5, 7.5 -7.5]}$ 決定分裂點的優劣取決於 $Gain$ Gain = Left_{\\text{similarity}} + Right_{\\text{similarity}} - Root_{\\text{similarity}} \\tag{1}分裂 node，會產生 left child node 與 right child node，分別計算三者的 $\\text{similarity score}$ \\text{Similarity Score} = \\cfrac{(\\sum \\text{residual})^2}{\\text{Number of Residuals } + \\lambda} \\tag{2} $\\lambda$ is a regularization parameter，這邊先假設 0 注意！！ 排序的是 $\\text{Drug Dosage}$，分裂點依序在 $\\text{Drug Dosage}$ 中找，但被分開到左右子樹的是 $\\text{residual}$ 在 data sample 的特徵 $\\textit{drug dosage}$ 中找出 $gain$ 最大的做為分裂點以 Dosage &lt; 15 當作分裂點的 $gain = 110.25 + 14.08 - 4 = 120.33$ 以 Dosage &lt; 22.5 當作分裂點的 $gain = 8 + 0 -4 = 4$ 顯然以 Dosage &lt; 15 當作分裂點，好於以 Dosage &lt; 22.5 當作分裂點。 當然，還得把剩下的可能分裂點看完，才能決定最終分裂點。 找個第一個分裂點，還得找下個子樹的分裂點，如此周而復始，最終得到樹的結構 如何 Pruning ?XGBoost 對 new tree $f_m(x)$ 的 pruning 建立在 gain 上，透過設定一個 gain threshold $\\tau$ 決定是否剪枝 若 $\\tau = 130$，則 分裂點 $＂Dosage &lt; 30＂$ 的 $gain = 140.17 &gt; 130 = \\tau$，不會被剪枝，因爲子節點沒被剪枝，父節點 $\\textit{Dosage &lt; 15}$ 也保留 若 $\\tau =150$，則 分裂點 $＂Dosage &lt; 30＂$ 的 $gain = 140.17 &lt; 150 = \\tau$ ，會被剪枝，最終只保留 父節點 $\\textit{Dosage &lt; 15}$，因爲需要一個根節點 P.S. 在 XGBoost 論文裡，gain threshold 的符號是 $\\gamma$，但因為 notaion 衝突，這邊換成 $\\tau$，對應到工程 hyper parameter min_split_loss 如何決定 leaf node 的 output value?決定每個 leaf node 的 output value 公式跟式(2) similarity score 很像，差別在分子部份的 $\\text{sum of residuals}$ 是否取平方 \\text{output value} = \\cfrac{\\sum(\\text{residuals})}{\\text{Number of Residuals} + \\lambda}\\tag{3} $\\lambda$ is a regularization parameter，這邊先假設 0 計算每個 leaf node output value 後 Regularization term $\\lambda$ 的作用 prevent overfitting in training data regularization paramater $\\lambda$ 的作用是防止 overfitting，$\\lambda$ 可以減少 similarity 對 $resiudal$ 值的敏感程度 \\text{Similarity Score} = \\cfrac{(\\sum \\text{residual})^2}{\\text{Number of Residuals } + \\lambda}若 $\\lambda &gt; 0$ 值越大則分母越大， similarity score 會越小，代表分子 $\\text{sum of residual }$ 的作用越小 similarity score 越小 similarity score 越小，在分裂階段，分裂點的 $Gain = Left_{Similarity} + Right_{similarity} - Root_{similarity}$ 就越小 $\\lambda$ 對不同 node 的影響是非線性的 similarity score 越小 ， gain 也越小，在剪枝階段，其被剪枝的可能性越大。 如何整合多棵樹 $f_1(x), f_2(x),…,f_m(x)$ 的輸出$F_M(x)$ 的計算與 GBDT 一樣 F_M(x) = F_0(x) + \\nu\\sum^M_{i=1} f_i(x) = F_{M-1} + \\nu f_m(x) 假設只建立一顆 XGB tree $f_1(x)$， $\\text{learning rate = 0.8}$ 則每個 data sample 新的 prediction 為 F_1(x) = F_0(x) + 0.8* f_1(x) ep_0_predict 表 $F_0(x)$ 對每個 data sample 的預測值， XGBoost 初始預設值是 0.5 ep_1_leaf_output 表 data sample 在 XGB tree $f_1(x)$ 下所屬 leaf node 的輸出值，其值擬合 ep_1_residual ep_1_predict 表 $F_1(x)$ 表 data sample 在 $m=1$ 的預測值 XGBoost Regression Math BackgroundXGBoost 的通用 Objective Function \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned} \\tag{4} $g_i$ 為 first derivative of loss function related to data sample $x_i$ : $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, \\hat{y_i})$ $h_i$ 為 second derivative of loss function related to data sample $x_i$: $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, \\hat{y_i})$ $T_m$ is the number of leaf in tree $f_m$ $\\tau$ 表對 $T_m$ 的 factor $\\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$ 第 $j$ 個 leaf node 的輸出值 $R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合 optimal output $\\gamma_{j,m}^*$ of leaf node $j$ \\gamma_{j,m}^*=- \\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} \\tag{5}詳細推導參閱 透視 XGBoost(3) 蘋果樹下的 objective function Regression Leaf output 公式怎麼來的？ 式(3) each leaf node 的 output 公式怎麼得出的？ \\text{output value} = \\cfrac{\\sum(\\text{residuals})}{\\text{Number of Residuals} + \\lambda}\\tag{3}從式 (5)，可以直接求 regression 的 optimal leaf node output regression 的 loss 通常是 square error ，先將 $l(y_i, \\hat{y_i}) = \\cfrac{1}{2}(y_i - \\hat{y_i})^2$ 代入 $h_i, g_i$ \\begin{aligned} g_i &= -(y_i - \\hat{y}_i) = \\text{negative residual} \\\\ h_i &= \\cfrac{d}{d \\ \\hat{y}_i} -(y_i - \\hat{y_i}) = 1 \\end{aligned}代入 式(5) 得到 $\\gamma_{j,m}^*$ \\gamma_{j,m} = \\cfrac{\\sum_{i \\in R_{j,m}}(y_i - F_{m-1}(x_i))}{(\\sum_{i \\in R_{j,m}}1) + \\lambda} = \\cfrac{\\text{sum of residual}}{\\text{number of residual } + \\lambda } $i \\in R_{j,m}$ 表 leaf node $j$ 下的 data sample $x_i$ $\\lambda$ is a regularization parameter 故得證 XGB tree $f_m(x)$ for regression each leaf node $j$ 的輸出為 \\gamma_{j,m} = \\cfrac{\\text{sum of residual}}{\\text{number of residual } + \\lambda }Regression 的 Similarity Score 怎麼來的？ 式 (2) similarity score 怎麼得出的 \\text{Similarity Score} = \\cfrac{(\\sum \\text{residual})^2}{\\text{Number of Residuals } + \\lambda} \\tag{2}XGBoost 分裂的目的是要使 式(4) objective function 越小越好，怎麼評判分裂前後收益？式(1) 評價了分裂後 left/right leaf node 可以得到的＂score＂與分裂前 root node 的 ＂score＂ 差值計算分裂前後增益 gain Gain = Left_{\\text{similarity}} + Right_{\\text{similarity}} - Root_{\\text{similarity}} \\tag{1}所以 similarity score 肯定得跟 objective function 有關，才能正確的恆量 gain，但 similarity score 是 ＂score “ 越大越好， objective function 是要 minimize 的，得越小越好，兩者如何扯上關係？ 觀察一下 objective function \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned} \\tag{4}因為我們要衡量單個 node 得 gain，式 (4)中，跟某個 node 直接相關的 part 為 summation of all leaf node 裡面的 equation [(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] \\tag{6} $R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合 將 式(5) 代入 式(6) 可以得出 式(6)的極小值 \\begin{aligned} (\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2 &= -（\\sum_{i \\in R_{j,m}}g_i)\\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)(\\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda})^2 \\\\ & = -\\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} + \\cfrac{1}{2} \\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} \\\\ & = -\\cfrac{1}{2}\\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} \\end{aligned} \\tag{7}因爲要的是 ＂score＂ ，所以讓 loss 對 x 軸做對稱，拿掉係數項 $\\frac{1}{2}$ \\text{Similarity Score} = \\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} \\tag{8} $g_i$ 為 first derivative of loss function related to data sample $x_i$ : $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, \\hat{y_i})$ $h_i$ 為 second derivative of loss function related to data sample $x_i$: $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, \\hat{y_i})$ 因爲要的是 ＂score＂ ，所以讓 loss 對 x 軸做對稱，拿掉係數項 $\\frac{1}{2}$ 式 (8) 中的 $g_i, h_i$ 分別表 loss function $l(y,\\hat{y})$ 的 first derivative and second derivative 當 loss function 為 square error 時 $g_i = -(y_i - \\hat{y}_i) = \\text{negative residual}$ $h_i = \\cfrac{d}{d \\ \\hat{y}_i} -(y_i - \\hat{y_i}) = 1$ 代入式(8) 得到 square error 下的 similarity score，得證： \\text{Similarity Score} = \\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} = \\cfrac{(\\sum \\text{residual})^2}{\\text{Number of Residuals } + \\lambda}Reference Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. https://doi.org/10.1145/2939672.2939785 What makes “XGBoost” so Extreme? https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb XGBoost-from-scratch-python Boosting algorithm: XGBoost https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d https://www.hrwhisper.me/machine-learning-xgboost/ XGBoost Part 1 (of 4): Regression XGBoost Part 3 (of 4): Mathematical Details my post 一步步透視 GBDT Regression Tree 一步步透視 GBDT Classifier","link":"/xgboost-for-regression/"},{"title":"一步步透視 GBDT Classifier","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x)$ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ GBDT classifier 常用的 loss function 為 cross entropy classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$ 跟 $probability$ 有關，得將 $F(x)$ 通過 $\\textit{sigmoid function }$ 獲得 probability $p = \\sigma(F(x))$ GBDT 簡介在 一步步透視 GBDT Regression Tree 直接進入正題吧 GBDT Algorithm - step by stepGBDT classification tree algorithm 跟 regression tree 並無不同 Input Dat and Loss Function Input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data target $y_i$: who loves Troll2 features of $x_i$: “likes popcorn”, “Age”, “favorite” Our goal is using $x_i$ to predict someone like Trolls 2 or not loss function 為 cross entropy \\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​值得注意的是，GBDT - classifier $F(x)$ 輸出的是 $log(odds)$ 而不是 $probability$ 要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\\textit{sigmoide function}$ \\textit{The probability of Loving Troll 2 } = \\sigma(F(x)) = p $log(odds)$ 轉換成 $probability$ 公式 p = \\cfrac{e^{\\log(odds)}}{1 + e^{\\log(odds)}} Step 1 Initial model with a constant value $F_0(X)$ 初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可 計算完，得到 $F_0(x) = 0.69$，每個 data point 的初始 prediction 都一樣就是 $F_0(x)$。 $F_0(x)$ 是 $\\log(odds)$ 若要計算 probability of loving Troll 2 呢？ $\\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。 ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$ ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving Troll 2 Step2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At Epoch m = 1(A) Calculate Residuals of $F_{0}(x)$classification 問題中 residual 為 predicted probability 與 observed label $y$ 之間的差距 $residual = observed - \\textit{predicted probability}$ true label 為 1 false label 為 0 注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$ 輸出的是 $log(odds)$ 計算各 data sample 的 residual 後： ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$ ep_0_prob 表 $F_0(x)$ predicted probability，$\\sigma(F_0(x))$ ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$ 建樹為一般 fit regression tree 的過程，criterion 為 mean square error，假設找到的樹結構為 可以看到綠色為 leaf node，所有的 data sample $x$ 都被歸到特定 leaf node 下 ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下 \\cfrac{\\sum residual_i}{\\sum [\\textit{previous probability} \\times \\textit{(1 - previous probability)}]} 分子是 each leaf node 下的 data sample $x$ 的 residual 和 分母的 previous probability 為 $m -1$ 步 GBDT 輸出的 probability $p = \\sigma(F(x))$ 。在這個 epoch 是指 $F_0(x)$ 經過計算後，每個 leaf node 輸出 ep_0_prob 表 $\\sigma(F_0(x))$ 計算出的 probability of loving Troll2 ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob ep_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值 (D) update $F_1(x)$ with new tree $f_1(x)$現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 F_1(x) = F_0(x) + \\textit{learning rate} \\times f_1(x)輸出的 probability 為 $\\sigma(F_1(x))$ 令 $\\textit{learnign rate = 0.8}$，得到 epoch 2 每個 data sample 的 $\\log(odds)$ prediction 與 probability prediction ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$ ep_1_prob 為 $F_1(x)$ 輸出的 probability $\\sigma(F_1(x))$ At Epoch m = 2(A) Calculate Residuals of $F_1(x)$計算上一步 $\\textit{residual of } F_1(X)$ residual = observed - \\textit{predicted probability} ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\\sigma(F_1(x))$ ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$ (B) Construct New Regression Tree $f_2(x)$用 data sample x 的 columns “like_popcor”, “age”, “favorite_color” 擬合 ep_2_residual build a new tree $f_2(x)$ 假設得到 $f_2(x)$ 的樹結構： 每個 data sample 對應的 leaf index ep_2_leaf_index 表 data sample 對應到 $f_2(x)$ 上的 leaf node index (D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$計算 $f_2(x)$ 下每個 leaf node 的輸出: 對應到 data sample 上: ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node 下會有一樣的值 Update $F_2(x)$ with New Tree $f_2(x)$到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為 $F_2(x) = F_0(x) + \\nu(f_1(x) + f_2(x))$ $\\nu$ 為 learning rate，假設為 0.8 GBDT 輸出的 probability 為 $\\sigma(F_2(x))$，計算 epoch 2 的 prediction of probability of loving troll2: love_toll2: our target ep_0_pre 表 $F_0(x)$ ep_1_leaf_output 表 data sample x​ 在第一顆樹 $f_1(x)$ 的輸出值 ep_2_leaf_output 表 data sample x 在第二顆樹 $f_2(x)$ 的輸出值 ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$ ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\\sigma(F_2(x))$ Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ 把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 的每棵樹 $f_m(x)$ 都是去 fit $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$ residual = observed - \\textit{predicted probability}所以 $F_m(x)$ 又可以寫成 F_m(x) = F_{m-1}(x) + \\nu f_m(x)這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住： $F_m(x)$ 輸出 $log(odds)$ $residual$ 的計算與 probability 有關 GBDT Classifier 背後的數學Q: 為什麼用 cross entropy 做為 loss function ?在分類問題上，我們預測的是 $\\textit{The probability of loving Troll 2}$ $P(Y|x)$，$\\textit{}$ 以 $maximize$ $\\textit{log likelihood}$ 來解 $P(Y|x)$。 令 GBDT - classification tree 的 probability prediction 為 $P(Y| x) = \\sigma(F(x))$，則 objective function 為 \\text{log (likelihood of the obersved data given the prediction) } \\\\= \\sum_{i=1}^N [y_i \\log(p) + (1-y_i)\\log(1-p)] $p = P(Y=1|x)$，表 the probability of loving movie Troll 2 $y_i$ : observation of data sample $x_i$ loving Troll 2 or not $y \\in \\{1, 0\\}$ 而 $\\text{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成 \\text{objective function} = - \\sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)所以 $\\text{loss function} = -[y \\log(p) + (1-y)\\log(1-p)]$ 把 loss function 用 $odds$ 表示： \\begin{aligned} -[y \\log(p) + (1-y)\\log(1-p)] & = -y\\log(p)-(1-y)\\log(1-p) \\\\ &= -y\\log(p)-\\log(1-p) + y\\log(1-p) \\\\ &= -y[\\log(p) - \\log(1-p)] - \\log(1-p) \\\\ & = -y\\log(odds) - \\log(1-p) \\\\ &= -y\\log(odds) + \\log(1 + \\exp(\\log(odds))) \\end{aligned} 第三個等號 到 第四個等號用到 $odds=\\cfrac{p}{1-p}$ 第四個等號 到 第五個等號用到 $p = \\cfrac{\\exp(\\log(odds))}{1 + \\exp(\\log(odds))}$ 這個結論 $\\log(1-p) = \\log(1- \\cfrac{\\exp(\\log(odds))}{1 + \\exp(\\log(odds))}) = \\log(\\cfrac{1}{1 + \\exp(\\log(odds))}) = -\\log(1 + \\exp(\\log(odds)))$ 把 loss function 表示成 odds 的好處是， $ -y\\log(odds) + \\log(1 + \\exp(\\log(odds)))$ 對 $log(odds)$ 微分形式很簡潔 \\cfrac{d}{d \\ log(odds)} -ylog(odds) + log(1 + \\exp^{log(odds)}) = -y -\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}} = -y + ploss function 對 $log(odds)$ 的微分，既可以以 $\\log(odds)$ 表示，也可以以 probability $p$ 表示 以 $log(odds)$ 表示： $\\cfrac{d}{d \\log(odds)}L(y_i, p) = -y -\\cfrac{\\exp(\\log(odds))}{1 + \\exp(\\log(odds))}$ 以 $p$ 表示：$\\cfrac{d}{d \\log(odds)}L(y_i, p) = -y + p$ 用 $p$ 表示時，loss function 對 $log(odds)$ 的微分 -y + p = \\text{ -(observed - predicted) = negative residual}Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\\cfrac{count(true)}{count(false)})$ ? 來自 Step 1 的問題 根據選定的 loss function \\text{loss function} = -[y \\log(p) + (1-y)\\log(1-p)] $P(Y=1|x) = p$ 為出現正類的 probability $y \\in \\{1, 0\\}$ 將 loss function 以 $\\log(odds)$ 表示 -[y \\log(p) + (1-y)\\log(1-p)] = -[y\\log(odds) + \\log(1 + \\exp(log(odds)))]$F_0(x)$ 為能使 $\\textit{cost function}$ 最小的 $\\log(odds): \\gamma$ F_0(x) = argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma) = argmin_\\gamma \\sum^n_{i=1} -[y_i\\log(odds) + \\log(1 + \\exp(\\log(odds)))] $n$ 為 number of data sample $x$ 令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$ cost function 對 $log(odds)$ 微分取極值： \\begin{aligned}& \\cfrac{d}{d \\log(odds)}\\sum^n_{i=1} -[y_i\\log(odds) + \\log(1 + \\exp(log(odds)))] \\\\ & = \\cfrac{d}{d \\log(odds)}\\sum^{n^{(1)}}_i -(\\log(odds) + \\log(1 + exp(log(odds)))) - \\sum^{n^{(0)}}_j (0 * \\log(odds) + \\log(1 + \\exp(\\log(odds)))) \\\\& = \\cfrac{d}{d\\log(odds)} -n^{(1)} \\times (\\log(odds) + \\log(1 + \\exp(\\log(odds)))) - n^{(0)} \\times \\log(1 + \\exp(\\log(odds))) \\\\ & =0\\end{aligned} \\begin{aligned} & n^{(1)} \\times(-1 + \\cfrac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) + n^{(0)} \\times(\\cfrac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) \\\\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\\\ & = - n^{(1)} + n p \\\\ &= 0 \\end{aligned}移項得到 $p$ p = \\cfrac{n^{(1)}}{n} \\log(odds) = \\cfrac{p}{1-p} = \\cfrac{n^{(1)}}{n^{(0)}}故得證，給定 $\\text{loss function } = -[y \\log(p) + (1-y)\\log(1-p)]$， 能使 $argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma)$ 的 $\\gamma$ 為 log(odds)= \\cfrac{n^{(1)}}{n^{(0)}} \\therefore F_0(x) = \\cfrac{n^{(1)}}{n^{(0)}}Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？ 問題來自 Step 2 - (A) 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum -[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)] $y$ 為 target label $p = P(Y=1|x)$ 表正類的 probability 注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy 我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x) \\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - p)) \\\\ &= -(-(\\text{observed} - \\text{predict probability})) \\\\ &= - \\text{negative residual} \\\\ & = \\text{residual} \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual Q: leaf node 的輸出公式怎麼來的？ 問題來自 Step 2-(C) 在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\\gamma_{jm}$，使的 cost function 最小 \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma) \\\\ &= argmin_\\gamma \\sum_{x_i \\in R_{j, m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + e^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $j$ 表 leaf node index $m$ 表第 $m$ 步 $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合 將 loss function 以 $\\log(odds)$ 表示後的 objective function \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + e^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $-[y \\log(p) + (1-y)\\log(1-p)] = -[y\\log(odds) + \\log(1 + e^{\\log(odds)})]$ $F_{m-1}(x)$ 輸出為 $\\log(odds)$ cost function 對 $\\gamma$ 微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近 loss function 處理 f(x) \\approx f(a) + f'(a)(x-a) + \\cfrac{1}{2}f''(a)(x-a)^2讓 2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \\gamma)$ 在 $F_{m-1}(x)$ 處展開 L(y_i, F_{m-1}(x_i) + \\gamma) \\approx L(y_i, F_{m-1}(x_i) ) + \\cfrac{d}{d (F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))\\gamma + \\cfrac{1}{2} \\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\\gamma^2將 cost function 對 $\\gamma$ 微分取極值，求 $\\gamma_{j,m}$ \\sum_{x_i \\in R_{jm}} \\cfrac{d}{d\\gamma} L(y_i, F_{m-1}(x_i), \\gamma) \\approx \\sum_{x_i \\in R_{jm}} (\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\\gamma) = 0移項得到 $\\gamma$ \\gamma = \\cfrac{\\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))} = \\cfrac{ \\sum_{x_i \\in R_{jm} } g_i}{ \\sum_{x_i \\in R_{jm} } h_i}分子是 derivative of Loss function ; 分母是 second derivative of loss function 分子部分: \\begin{aligned} & \\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\\\& = \\sum \\cfrac{d}{d(F_{m-1}(x_i))} \\ y_i \\times [F_{m-1}(x_i) ] - \\log(1 + \\exp(F_{m-1}(x_i) )) \\\\ &= \\sum (y_i - \\cfrac{\\exp(F_{m-1}(x_i) )}{1 + \\exp(F_{m-1}(x_i) )} ）\\\\& = \\sum_{x_i \\in R_{jm}} (y_i -p_i) \\end{aligned} $F_{m-1}(x_i)$ 是 $m-1$ 步時 $classifier$ 輸出的 $\\log(odds)$ 分子部分為 $\\text{summation of residual}$ 分母部分 \\begin{aligned}& \\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\\\ & = \\sum_{x_i \\in R_{jm} } \\cfrac{d^2}{d \\, \\ (F_{m-1}(x_i))^2} \\, -[y_i \\times F_{m-1}(x_i) - \\log(1 + \\exp(F_{m-1}(x_i)))] \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)}-[y_i - \\cfrac{\\exp(F_{m-1}(x_i) )}{1 + \\exp(F_{m-1}(x_i) )}] \\\\ & =\\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \\exp(F_{m-1}(x_i)))^{-1} \\times \\exp(F_{m-1}(x_i))] \\\\ & = \\sum_{x_i \\in R_{jm} }-[(1 + e^{F_{m-1}(x_i)})^{-2} \\exp(F_{m-1}(x_i))\\times \\exp(F_{m-1}(x_i)) - (1+ \\exp(F_{m-1}(x_i)))^{-1} \\times \\exp(F_{m-1}(x_i)) ] \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\quad + \\quad \\cfrac{\\exp(F_{m-1}(x_i))}{1+ \\exp(F_{m-1}(x_i))} = \\sum_{x_i \\in R_{jm} }\\cfrac{-\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\ + \\ \\cfrac{-\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\times \\cfrac{(1 + \\exp(F_{m-1}(x_i)))}{1 + \\exp(F_{m-1}(x_i))} \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp(2 * F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} + \\cfrac{\\exp(F_{m-1}(x_i)) + \\exp(2 * F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} = \\sum_{x_i \\in R_{jm} }\\cfrac{\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))^2} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))} \\times \\cfrac{1}{(1 + \\exp(F_{m-1}(x_i)))} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp(\\log(odds)_i)}{1 + \\exp(\\log(odds)_i)} \\times \\cfrac{1}{1 + \\exp(\\log(odds)_i)} \\\\ &= \\sum_{x_i \\in R_{jm} } p_i \\times (1-p_i) \\end{aligned}綜合分子分母，能使 $F_m(x)$ cost function 最小化的 tree $f_m(x)$ 第 $j$ 個 leaf node 輸出為 \\gamma_{jm}= \\cfrac{\\sum_{x_i \\in R_{jm})} (y_i - p_i)}{\\sum_{x_i \\in R_{jm} }(p_i \\times (1- p_i))} = \\cfrac{\\text{summation of residuals }}{\\text{summantion of (previous probability $\\times$ (1 - previoous probability))}}寫在最後Data Sample learning by doing it 123456789101112import pandas as pddata = [ (True, 12, 'blue', True), (True, 87, 'gree', True), (False, 44, 'blue', False), (True, 19, 'red', False), (False, 32, 'green', True), (False, 14, 'blue', True)]columns = ['like_popcorn', 'age', 'favorite_color', 'love_troll2']target = 'love_troll2'df = pd.DataFrame.from_records(data, index=None, columns=columns) Reference Gradient Boost Part 3 (of 4): Classification Gradient Boost Part 4 (of 4): Classification Details Gradient Boosting In Classification: Not a Black Box Anymore! https://blog.paperspace.com/gradient-boosting-for-classification/ statquest 整理 StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s The Logit and Sigmoid Functions https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/ Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979","link":"/GBDT-Classifier-step-by-step/"},{"title":"透視 XGBoost(4) 神奇 optimization 在哪裡？","text":"XGBoost 不只在算法上的改造，真正讓 XGBoost 大放異彩的是與工程優化的結合，這讓 XGBoost 在工業上可以應用於大規模數據的處理。 Approximate Greedy Algorithm Weighted Quantile Sketch &amp; Parallel Learning Sparsity-Aware Split Finding System Design Approximate Greedy Algorithm XGBoost 有兩種 split finding 策略，Exact Greedy and Approximate Greedy data sample 量小的情況下， XGB tree 在建立分支時，會逐一掃過每個可能分裂點 (已對特徵值排序) 計算 similarity score 跟 gain， 即是 greedy algorithm。 greedy algorithm 會窮舉所有可能分裂點，因此能達到最準確的結果，因為他把所有可能的結果都看過一遍後選了一個最佳的。 但如果 data sample 不僅數量多 (row)，特徵維度也多 (columns)，採用 greedy algorithm，雖然準確卻也沒效率。 因此在數據量大的情況下，XGBoost 只會選出多個切分點，稱為 candidate splits ，這樣就能大幅降低計算量，此即 Approximate Greedy Algorithm，candidate splits 越多也意味更加耗時。 怎麼找 Candidate Splits ? XGBoost proposes candidate splitting points according to percentiles of feature distribution XGBoost 原則上是利用 data sample 的 feature 分佈的 quantiles 尋找 candidate splits，為什麼說是原則上呢，因為跟一般的 quantiles 有點不同，這留到 Weighted Quantile Sketch 章節再細說。 $\\phi$ - Quantiles 計算123data: [39. 21. 24. 61. 81. 11. 89. 56. 12. 51.]sort: [11. 12. 21. 24. 39. 51. 56. 61. 81. 89.]rank: [ 1. 2. 3. 4. 5. 6. 7. 8. 9. 10.] 以上總共 $N=10$ 筆 data 則 $\\phi = 0.1$ ⇒ 0.1 quantile ⇒ $0.1*10 = 1$ ⇒ rank = 1 ⇒ 11，故 0.1 quantile = 11 $\\phi = 0.5$ ⇒ 0.5 quantile ⇒ $0.5 * 10 = 5$ ⇒ rank = 5 ⇒ 39， 故 0.5 quantile = 39 可以發現 $\\phi N$ 就是在找 rank 與其對應的 data 值，下面是一個 quantiles = [0.1, 0.3, 0.5, 0.6, 0.9] 的例子 123quantiles [0.1, 0.3, 0.5, 0.6, 0.9]rank query [1.0, 3.0, 5.0, 6.0, 9.0]data query [11, 21, 39, 51, 81] 但 $\\phi$ - Quantiles 在數據量大時難以對所有 data sample 排序，更遑論找確切的 candidate splits。 所以 XGBoost 採用了 $\\epsilon \\text{-approximate} \\ \\phi \\text{-quantiles}$ 的思想來得到近似的 candidate splite ，容許 $\\phi N$ 找出的 rank 有一定誤差 $\\varepsilon N$。 $\\epsilon \\text{-approximate} \\ \\phi \\text{-quantiles}$$\\epsilon \\text{-approximate}$ 容許找出的 rank 落在一定範圍內： ⁍ $\\varepsilon=0.1$ , 0.1 quantile ⇒ {11, 12} $\\varepsilon =0.1$ , 0.2 quantile ⇒ {11, 12, 21} $\\varepsilon =0.1$ , 0.3 quantile ⇒ {12, 21, 24} 換句話說，在區間內的值都可以作為 candidate splits，當 $\\varepsilon =0.1$ quantiles = [0.1, 0.3, 0.5, 0.6, 0.9] 下，可以接受的 rank 與對應的 data 值： 123456789101112131415quantile: 0.1 query rank: [1, 2] query data: [11, 12]quantile: 0.3 query rank: [2, 3, 4] query data: [12, 21, 24]quantile: 0.5 query rank: [4, 5, 6] query data: [24, 39, 51]quantile: 0.6 query rank: [5, 6, 7] query data: [39, 51, 56]quantile: 0.9 query rank: [8, 9, 10] query data: [61, 81, 89] ε-Approximate Quantile Summary An ε-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of εN 因爲在區間內的值都可以作為 candidate splits，可以看出 quantiles 之間 query 出來的 rank 會有 overlapping，於是我們可以只選取特定幾個 quantiles，每個 quantile 只需要保存 $[\\lfloor (\\phi -\\epsilon)\\times N \\rfloor , \\lfloor (\\phi + \\epsilon) \\times N\\rfloor ]$ 中的代表的 rank 值 (最小值 and 最大值) 即可。 例如 $\\varepsilon = 0.1$ ，quantiles = [0.1, 0.3, 0.5, 0.6, 0.9] ，分別選定 rank 區間內的最大值做為 summary: 123quantiles: [0.1, 0.3, 0.5, 0.6, 0.9]rank summary: [2, 4, 6, 7, 10]query data: [12, 24, 51, 56, 89] 在使用時也很簡單： 找 0.1 quantile ⇒ rank =2 ⇒ 12 ⇒ 0.1 quantile = 12 找 0.8 quantile ⇒ rank = $0.8 * 10 = 8$ ⇒ 與 8 最接近的 rank 為 7 ⇒ 56 ⇒ 0.8 quantile = 56 因此在 data samples 很大的情況下，一台運算機器無法將所有 data samples 存放進 memory 時，可以透過近似的方式找出 quantile 代表的值做為 candidate split。 Weighted Quantile Sketch &amp; Parallel Learning上面提到 XGBoost 在數據量大的時候，會採用 Approximate Greedy Algorithm 選出多個 candidate splits ，加快運算效率。 問題是分佈運算 (Parallel Learning) 時要怎麼 approximate quantiles？ Parallel Learning 下的 quantiles簡單來說， data samples 會被拆成很多份在不同的運算單元計算某個特徵值的 gain and similarity score，但計算過程需要全局 data samples 的特徵 distribution 才有辦法算出 approximate quantiles ，因此各個運算單元會彙整 (merge) 出一張 approximate histogram，然後在這張 histogram 上計算 $\\varepsilon$-approximate quantiles 什麼是 Weighted Quantile Sketch?每筆 data sample 都帶有 weight，XGBoost 在 計算 approximate quantiles 時，盡量確保每個 quantiles 之間的 $\\text{sum of data weight}$ 是差不多的的，而一般的 quantiles 是確保 quantiles 之間的 data 數量相同，此即 weighted quantile 下面這張 weighted quantile sketch 示意圖，展示每個 quantile 的 sum of data weight 皆等於 10， 示意圖，參考就好 為什麼需要 Weighted Quantile Sketch？ Put observation with low confidence into quantiles with fewer observation 以分類問題當例子，假設我們有六筆 data samples 如下 如果按照數量決定 quantiles，紅框內的兩個 data sample 不管怎樣都會被分在一起分不開。 再看 classification leaf node 的 output value 公式 \\text{Output Value} = \\cfrac{(\\sum \\text{residual}_i)}{\\sum[(\\text{previous probability}_i) (1 - \\text{previous probability}_i)] + \\lambda}兩者的 residual 一正一負，剛好會互相 cancel，這對於 predicted probability 的收斂有負面影響。 如果依照 weight 來切分，盡可能使每個 quantile 的 sum of weight 相等， 就有機會將兩者分開到不同 leaf node 下 data sample $x_i$ 的 weight 應該要反映出 $F(x_i)$ 的預測值的 confidence，在分類問題中，predicted probability 在 0.5 附近時，代表 classifier 其實不確定 data sample $x_i$ 屬於 positive or negative， 所以 XGBoost 利用 weight quantile 加大 $x_i$的 weight 讓 XGB tree $f$ 在分裂時更可以考慮到 $x_i$。 Weight Quantile 的實際作用為讓 low confidence 的 data sample 有更高的 weight，以便在計算 candidate split 時， low confidence 的 data sample 能跟 hight confidence 的 data sample 分開來 split 那每筆 data 的 weight 怎麼來的？每筆 data 的 weight 其實就是通用 objective function 裡的 second derivative of the loss function $h_i$ 參見 透視 XGBoost(3) 蘋果樹下的 objective function \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned} $g_i$ 為 first derivative of loss function related to data sample $x_i$ : $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, F_{m-1}(x_i))$ $h_i$ 為 second derivative of loss function related to data sample $x_i$: $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$ $T_m$ is the number of leaf in tree $f_m$ $\\tau$ 表對 $T_m$ 的 factor $\\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$ 第 $j$ 個 leaf node 的輸出值 Proof: 從 objective function 開始推導 \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)}& = [ \\sum_i^n( g_if_m(x_i) + \\cfrac{1}{2}h_if_m(x_i)^2) ] + \\Omega(f_m) \\\\ & = \\sum^n_{i=1}\\cfrac{1}{2} h_i(f_m(x_i) + \\cfrac{g_i}{h_i})^2 + \\Omega(f_m) - \\cfrac{1}{2}\\sum^n_{x}\\cfrac{g_i^2}{h_i} \\\\ & = \\sum^n_{i=1}\\cfrac{1}{2} h_i(f_m(x_i) + \\cfrac{g_i}{h_i})^2 + \\Omega(f_m) + constant \\end{aligned} regularization term $\\Omega({f_m} ) =\\tau T_m + \\cfrac{1}{2}\\lambda \\sum^T_{j} \\gamma_{m,j}^2$ $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, F_{m-1}(x_i))$，已知項 $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$ ，已知項 仔細觀察，each data sample is weighted by $h_i$ \\cfrac{1}{2} h_i(f_m(x_i) -(-\\cfrac{g_i}{h_i}))^2 \\hAar \\text{weight}(x - b)^2 因為是求 weight ，係數項只是 scalar，所有 data sample 都乘一樣的值，相當於不用乘 論文中這一段堆導應該是筆誤了，少了一個負號 故得證 each data sample $x_i$’s weight is actually its second derivative of the loss function $h_i$ \\text{data sample weight } = h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, F_{m-1}(x_i))Regression 下 data sample weight 已知 data sample $x_i$’s weight is actually its second derivative of the loss function，來求 regression 的 data sample weight 。 Regression 的 loss function 通常是 square error ， $l(y_i, \\hat{y_i}) = \\cfrac{1}{2}(y_i - \\hat{y_i})^2$ 我們已知 square error 的 $h_i = \\cfrac{d}{d \\ \\hat{y}_i} -(y_i - \\hat{y_i}) = 1$ 所以事實上，loss function 為 square error 的 regression 所有 data sample weight 都是 1 Classification 下 data sample weightclassification 在 cross entropy/logit loss 作為 loss function 時，second derivative of the loss function $h_i$: h_i = \\cfrac{d^2}{d (F_{m-1}(x_i))^2}l(y_i, F_{m-1}(x_i) = p_i \\times (1-p_i) previous probability $p_i$ 表 data sample $x_i$ 在 $F_{m-1}(x_i)$ 輸出的 probability classification 的 sample weight 是個開口向下的拋物線，兩側越往 $p_i=0.5$ 靠近 sample weight 越大。這很好理解，當 classifier 預測出的 probability 為 0.5 時，對這筆 data sample 是 low confidence 的，所以得加大其 weight 凸顯他。 Sparsity-Aware Split FindingXGBoost 有兩種 missing values (NAs) 處理機制，一種是最簡單的，直接指定 leaf node 方向，EX: missing value 直接放到 leaf node 。 另一種是 learning from data，訓練時對於特徵中的 missing values (NAs)，在分裂計算 gain 時處理。 XGBoost 會分別考慮將 missing values 放到 left leaf node and right leaf node 的情形下計算 $\\text{Gain}_{Left}, \\text{Gain}_{Right}$ 選擇能讓 gain 最大的那一側做為 missing values 所在 leaf node ，在 inference 階段遇到此特徵的 missing value 時將之歸入 EX： 假設我們有數筆 complete data 和兩筆含有 missing values 的 data 分別計算 $Gain_{Left}$ and $Gain_{Right}$ 後選擇 $\\max(\\text{Gain}_{Left}, \\text{Gain}_{Right})$ 最大的那邊作為 missing values (NAs) 的歸屬 leaf node System DesignXGBoost 一系列 System 上的 optimization 都是圍繞著 Memory Block 展開的 Column Block for Parallel Learningblock 是個連續的 memory 區塊，XGBoost 將 data sample 以 CSC (Compressed Sparse Column) format 存進 block 中，這麼做的好處是 XGBoost 的 input 常常為大規模的 sparse data samples，使用 CSC 可以以 column 為 index 僅存放非 0 元素，有效減少 data sample size in memory 。 CSC 因為是 columns index ，所以在 training tree 時，能快速定位到確切的 column 在建 XGB tree $f_m(x)$ 時須依據不同 column (X feature column) 的 feature value sorting data，才能 split data ，有了 block 之後，只需一開始對全局所有 columns 的 feature value sorting 一次，即可在之後建任何 XGB tree $f_m(x)$ 時使用 在 exact greedy algorithm，全局只有一個 block 計算 split; 在 approximate algorithms 中，因爲 data samples size 過大導致一運算單元容不下，所以 data samples 會根據 rows 切分成多個 blocks 分散到多個運算單元。 也因為根據 row 切分成不同 blocks，所以在 multi blocks 之上運算的 Weighted Quantile Sketch 也能 parallel for split finding Time Complexity of Split Finding若不事先對 block 內的 columns sorting，則整體 time complexity 為 O(Kd \\| \\text{x}\\|_0 \\log n) $K$ it total tree in XGBoost $d$ is the maximum depth of the tree $| \\text{x}|_0$ number of non-missing entries $| \\text{x}|_0 \\log n$ related to sorting algorithm ，每一棵樹的每一層都要 sorting 一次的意思 若事先對 block columns sorting 則 Exact greedy algorithm time complexity 為 O(Kd \\|\\text{x}\\|_0 + \\|\\text{x}\\|_0 \\log n) $O(| \\text{x}|_0 \\log n)$ 為事先 sorting $O(Kd |\\text{x}|_0 )$ 為建 K 個 tree Approximate algorithm 的 time complexity 為 O(Kd \\|\\text{x}\\|_0 + \\|\\text{x}\\|_0 \\log B) 單建一顆 tree 的 complexity 為 $| \\text{x} |_0 \\log q$，$q$ is the number of proposal split candidates in dataset $B$ is the maximum number of rows in each block。因爲 parallel computing 所以 $O$ 只記入最耗時的那個 block 綜合以上，事先對 block 內的 columns sorting 加上 approximate algorithm 的 proposal candidate split，可以大幅降低 time complexity。 Cache-Aware Access使用 memory block structure 對 columns sorting，對於 split finding 能有效降低 time complexity，但同時會造成 CPU cache $g_i$ $h_i$ 時的 cache miss rate 上升。 $g_i$ first derivative of loss function $h_i$ second derivative of loss function $g_i \\ h_i$的計算跟當前 $x_i$ 的 loss 有關，所以當 split finding 由小到大取時，$g_i \\ h_i$ 在 memory 中實際上是無序的存放的，得透過 pointer 對應之，CPU 要 cache 時，不容易 copy 需要的 $g_i \\ h_i$ 到 cache，造成 cache missing 在 exact greedy algorithm，使用 cache-aware prefetching algorithm，使用另一個 thread 提前 access (prefetch) $g_i \\ h_i$ 到 memory buffer 中讓 CPU cache 住。當 training thread 要提取時，直接就可以從 cache 拿，就算沒 cache 住，在 memory 中也是連續空間。 在 multi block 下的 approximate greedy algorithm 得對 each block size 合理設置，也就是 each block 下的 row number (data sample 數) 有關。 a block size 過大 cache 會裝不下，造成 cache missing ; a block size 過小會使 total block number 過多，使的 parallel 運算沒效率，是個 trade-off 的參數。 經過實驗，block size $2^{16}$ 是個相對合適的設置。 Blocks for out-of-core Computation當 data samples 大到所有的計算單元的 memory 也無法容納時，XGBoost 會使用 out-of-core。我們知道 XGBoost 以 block 為單位切分 data sample，無法被 memory 裝下的 blocks 會被成塊且連續的存入 hard disk。 計算時也會開一條 thread 去 hard disk pre-fetch blocks into memory，這樣 XGBoost 就不會消耗過多時間再 I/O 上，可以一邊訓練一邊讀取 block。 Block Compression存放在 hard disks 上的 out-of-core blocks 會被壓縮，減少讀取 I/O 的耗時。 壓縮方式大致上為保留 column index，對 row index 進行壓縮，只保留 beginning index of row in block，然後 利用一個 16bit 的 integer 儲存 row index 的 offset，類似 C 裡面的 pointer 跟 array 的關係。 Block Sharding當有多個 hard disks 可供存放時，會將 data 劃分到多個 hard disks 上，好處是 pre-fetch thread 就可以同時讀取多個 hard disks 提高 I/O throughput 。 Reference Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. https://doi.org/10.1145/2939672.2939785 Why XGBoost Is So Effective? https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289 Weighted Quantile Sketch ε-approximate quantiles http://www.mathcs.emory.edu/~cheung/Courses/584/Syllabus/08-Quantile/Greenwald.html xgboost之分位点算法 https://datavalley.github.io/2017/09/11/xgboost源码之分位点 XGBoost解读(2)—近似分割算法 https://yxzf.github.io/2017/04/xgboost-v2 Need help understanding xgboost’s approximate split points proposal https://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal Regression prediction intervals with XGBoost https://towardsdatascience.com/regression-prediction-intervals-with-xgboost-428e0a018b block blocks-for-out-of-core-computation https://www.hrwhisper.me/machine-learning-xgboost/#blocks-for-out-of-core-computation XGBoost Part 4 (of 4): Crazy Cool Optimizations","link":"/XGBoost-cool-optimization/"},{"title":"透視 XGBoost(2) 圖解 Classification","text":"TL;DR XGBoost 與 GBDT 相比，同樣都是擬合上一步 $F_{m-1}(x)$ 預測值的 residual 。不同的是 XGBoost 在擬合 residual 時更為聰明有效，他有自己衡量分裂增益 gain 的方式去擬合 residual 建立 XGB tree $f_m(x)$ Gain = Left_{\\text{similarity}} + Right_{\\text{similarity}} - Root_{\\text{similarity}} \\tag{1} 分裂增益 gain ，的背後原理可以透過 XGBoost 通用 objective function 並填入 classification/regression/rank/etc.. 各自的 loss function 後推導出的 $\\text{first derivative of loss function} = g_i \\quad \\text{and} \\quad \\text{second derivative of loss function} = h_i$ XGBoost 的 通用 objective function 由兩部份組成 first part is loss function，根據不同任務有不同的 loss function second part is regularization term 防止 overfitting，限制 leaf node 輸出值大小 和 leaf nodes 的總數 從 Gradient Boosting 說起 XGBoost 跟 gradient boosting algorithm 框架一樣，皆是依序建立多棵樹 $f_1(x), f_2(x), ….,f_M(x)$ 組成模型 F_m(x) = F_0(x) + \\nu\\sum^m_{i=1} f_i(x) = F_{m-1} + \\nu f_{m-1}(x) 其中第 $m$ 步的 tree $f_m(x)$ 是擬合模型 $F_{m-1}(x)$ 預測值 $\\text{predicted }$ 與 真實值 $\\text{observed}$ 的 $\\text{residual}$ $\\nu$ 為 learning rate 算法差別主要體現在 objective function 的設計 Step 2 (B) (C) 建樹，GBDT 是建一顆傳統 regression tree $f_m(x)$ 去擬合 $\\text{residual}$; XGBoost 有自己衡量分裂 gain 的方式去擬合 residual 建立 XGB tree $f_m(x)$ 可以說 XGBoost 用一種比較精準的方式去擬合 residual 建立子樹 $f_m(x)$ 以下章節主要分兩大塊 XGBoost for Classification： 藉由 classification 介紹 XGBoost 如何 train 一棵 XGB tree，以圖文方式說明 XGB tree 如何擬合目標到剪枝。此章節不涉及公式證明，只有少量運算，適合快速理解 XGB 訓練流程 XGBoost Classification Math Background：此章節深入討論在前一章節中用到的公式原理，並給予證明，適合深入理解 XGBoost 為何 work 篇幅關係 XGBoost 的優化手段放在 透視 XGBoost(4) 神奇 optimization 在哪裡？ XGBoost for Classification前置 background如同在 GBDT - classification 裡一樣 一步步透視 GBDT Classifier 模型輸出 $F_m(X)$ 為 $\\log(odds)$，但衡量 residual 時與 probability 有關，兩者互相轉換的公式 probability $p$ → $\\log(odds)$ \\log(odds) = \\cfrac{p}{(1-p)} $\\log(odds)$ → $p$ p = \\cfrac{\\exp^{\\log(odds)}}{1 + \\exp^{\\log(odds)}}Data之後用到的例子，數據如下 四筆 data samples，目標是用 Drug Dosage 預測藥物是否有效用 (Drug Effectiveness) 如何建一顆 XGB tree $f_m(x)?$Classification problem 上，XGBoost 會在所有的特徵值中選取最佳分裂點，最終建成一顆 binary tree $f_m(x)$ 建樹的過程涉及 Fitting Target 擬合目標 分裂好壞的恆量，如 CART 用 gini gain 衡量分類樹 Pruning 剪枝 Output Value 決定每個 leaf node 的唯一輸出 $f_m(x)$ 擬合的目標是什麼？$f_m(x)$ 擬合的目標是 $\\text{residual}$ ，利用 data sample x 的所有特徵建一顆特殊的 $\\text{regression tree}$ 去擬合 $\\text{residual}$ \\text{residual = observed - predicted}classification 的 residual 計算與 probability 有關，observed 即 Drug Effectiveness False → 0 True → 1 ; predicted 為模型輸出的 probability $\\sigma(F_m(x))$ 以 $m=1$ 舉例 XGBoost 預設的初始值 $F_0(x)$ probability，預設為 0.5，$\\log(odds)$ = 0 ep_0_pre 表 $F_0(x)$ 輸出的 $\\log(odds)$ ep_0_prob 表 $F_0(x)$ 輸出的 probability ep_1_residual 表 $m=1$ 時 observed y 與 ep_0_prob 的 residual ep_1_residual 即 $m=1$ 時，XGB tree $f_1(x)$ 要擬合的目標 建 tree 時如何衡量分裂點好壞？建分支時依序在特徵 $\\text{Drug Dosage}$ 的 data sample value $\\text{[3, 8, 12, 17]}$ 中尋找最優分裂點切分 residual $\\text{[-0.5, 0.5, 0.5, -0.5]}$ 決定分裂點的優劣取決於 gain Gain = Left_{\\text{similarity}} + Right_{\\text{similarity}} - Root_{\\text{similarity}} \\tag{1}分裂 node ，會產生 left child node $Left$ 與 right child node $Right$，分別計算三者的 similarity score \\text{Similarity Score} = \\cfrac{(\\sum \\text{residual}_i)^2}{ \\sum[\\text{previous probability}_i \\times (1- \\text{previous probability}_i)]+ \\lambda} \\tag{2} $\\lambda$ is a regularization parameter， 跟之後的 pruning 有關，這邊先假設 0 $\\text{previous probability}_i$ 為上一步模型輸出的 probability $\\sigma(F_{m-1}(x))$ 注意！！ 排序的是 $\\text{Drug Dosage}$，分裂點依序在 $\\text{Drug Dosage}$ 中找，但被分開到左右子樹的是 $\\text{residual}$ 計算 Gain以 $\\text{Dosage &lt; 15}$ 做為分裂點計算 gain，令 $\\lambda =0$ root similarity $\\cfrac{(-0.5 + 0.5 + 0.5 -0.5)^2}{\\text{whatever you are}} =0$ left node similarity:$\\cfrac{(-0.5 + 0.5 + 0.5)^2}{(0.5 \\times (1-0.5)) +(0.5 \\times (1-0.5)) + (0.5 \\times(1-0.5)) + 0} = 0.33$ right node similarity: $\\cfrac{0.5^2}{0.5 \\times (1-0.5) + 0} = 1$ $\\text{Gain } = 0.33 + 1 -0 = 1.33$ 上面只是示範如何計算 gain ，依序將 $\\text{[3, 8, 12, 17]}$ 可能分裂點計算過 gain 後選取最大的 gain 即最優分裂點 當然這只是第一個分支，往下還有 第二個分支 第三個分支 … etc 假設最終我們找到的樹結構為： 建完一棵樹後，接下來 XGBoost 有多個手段對樹結構 pruning 如何 Pruning - Cover Cover is related to the minimum number of Residuals in a leaf XGBoost 其中一個 pruning 方法叫 cover，在 classification 中 cover 的運算為 式(2) similarity score 分母中的前項 \\text{Cover} = \\sum[\\text{previous probability}_i \\times (1- \\text{previous probability}_i)]然後會有個 $\\text{Cover}_{threshold}$ 的值決定是否 pruning 此 leaf node 例如，把 $\\text{Cover}_{threshold}$設為 1 時，所有小於 threshold 的 leaf node 都會被 pruning： XGBoost 規定一顆 XGB tree $f(x)$ 不能只有 root，所以會保留一個分岔 $\\text{Cover}_{threshold}$ 在 XGBoost 參數中叫 min_child_weight 如何 Pruning - $\\tau$此法 對 new tree $f_m(x)$ 的 pruning 建立在 gain 上，透過設定一個 gain threshold $\\tau$ 決定是否剪枝 若 $\\tau = 2$，則 分裂點 $＂Dosage &lt; 5＂$ 的 $gain = 2.66 &gt; 2 = \\tau$，不會被剪枝 $\\textit{Dosage &lt; 15}$ 的 $gain = 1.33 &lt; 3 = \\tau$ ，理應要被 pruning，但因爲子節點沒被剪枝，父節點 $\\textit{Dosage &lt; 15}$ 也保留 若 $\\tau =3$，則 分裂點 $＂Dosage &lt; 5＂$ 的 $gain = 2.66 &lt; 3 = \\tau$ ，會被剪枝 $\\textit{Dosage &lt; 15}$ 的 $gain = 1.33 &lt; 3 = \\tau$，理應要被 pruning，但因為 tree 需要一個根節點，所以保留 P.S. gain threshold $\\tau$ 在 XGBoost 叫 min_split_loss 如何決定 leaf node 的 output value?classification 中決定每個 leaf node 的 output value 公式跟式(2) similarity score 很像，差別在分子部份的 $\\text{sum of residuals}$ 是否取平方 \\text{Output Value} = \\cfrac{(\\sum \\text{residual}_i)}{ \\sum[\\text{previous probability}_i \\times (1- \\text{previous probability}_i)]+ \\lambda} \\tag{3} $\\lambda$ is a regularization parameter，這邊先假設 0 計算每個 leaf node output value 後 如何整合多棵樹 $f_1(x), f_2(x),…,f_m(x)$ 的輸出$F_M(x)$ 的計算與 GBDT 一樣 F_M(x) = F_0(x) + \\nu\\sum^M_{i=1} f_i(x) = F_{M-1} + \\nu f_m(x) 得注意的是，$F_m(x)$ 輸出的是 $\\log(odds)$ ，得通過 $p = \\sigma(\\log(odds))$ ，才會是 predicted probability。 假設只建立一顆 XGB tree $f_1(x)$， $\\text{learning rate = 0.3}$ 則每個 data samples 新的 prediction 為 F_1(x) = F_0(x) + 0.3* f_1(x) ep_0_prob: $\\sigma(F_0(x))$ 算出的 probability of Drug Effectiveness ep_0_pre: $F_0(x)$ 輸出的 predicted log odds ep_1_leaf_node_output: XGB tree $f_1(x_i)$ 對每個 data sample $x_i$ 的輸出值。公式請見式(3) ep_1_pre: $F_1(x)$ 輸出的 predicted log odds, $F_1(x) = F_0(x) + \\nu f_1(x)$ ep_1_prob $\\sigma(F_1(x))$ 算出的 probability of Drug Effectiveness 再次提醒 $F_m(x)$ 輸出 $log(odds)$ $F_m(x)$ 輸出的 probability 為 $\\sigma(F_m(x))$ $\\text{residual }$ 的計算與 probability 有關 XGBoost Classification Math BackgroundRegression 的 Similarity Score 怎麼來的？ 式 (2) similarity score 怎麼得出的 XGBoost 的通用 Objective Function \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned}\\tag{4} $g_i$ 為 first derivative of loss function related to data sample $x_i$ : $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, \\hat{y_i})$ $h_i$ 為 second derivative of loss function related to data sample $x_i$: $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, \\hat{y_i})$ $T_m$ is the number of leaf in tree $f_m$ $\\tau$ 表對 $T_m$ 的 factor $\\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$ 第 $j$ 個 leaf node 的輸出值 $R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合 optimal output $\\gamma_{j,m}^*$ of leaf node $j$ \\gamma_{j,m}^*=- \\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} \\tag{5}詳細推導參閱 透視 XGBoost(3) 蘋果樹下的 objective function Classification 的 Similarity Score式 (2) classification similarity score 如下 \\text{Similarity Score} = \\cfrac{(\\sum \\text{residual}_i)^2}{ \\sum[\\text{previous probability}_i \\times (1- \\text{previous probability}_i)]+ \\lambda} \\tag{2}XGBoost 分裂的目的是要使 式(4) objective function 越小越好，怎麼評判分裂前後收益？式(1) 評價了分裂後 left/right leaf node 可以得到的＂score＂與分裂前 root node 的 ＂score＂ 差值計算分裂前後增益 gain Gain = Left_{\\text{similarity}} + Right_{\\text{similarity}} - Root_{\\text{similarity}} \\tag{1}所以 similarity score 肯定得跟 objective function 有關，才能正確的恆量 gain，但 similarity score 是 ＂score “ 越大越好， objective function 是要 minimize 的，得越小越好，兩者如何扯上關係？ 觀察一下 objective function \\begin{aligned} \\mathcal{\\tilde{L}}^{(m)} &= \\sum^{T_m}_{j=1}[(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] + \\tau T_m \\end{aligned} \\tag{4} 因為我們要衡量單個 node 得 gain，式 (4)中，跟某個 node 直接相關的 part 為 summation of all leaf node 裡面的 equation [(\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2] \\tag{6} $R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合 而 式(5) 給出了單個節點 optimal output value $\\gamma_{j,m}^*$ \\gamma_{j,m}^*=- \\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} \\tag{5}將 式(5) 代入 式(6) 可以得出 式(6)的極小值 \\begin{aligned} (\\sum_{i \\in R_{m,j}}g_i) \\gamma_{j,m} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)\\gamma_{j,m}^2 &= -（\\sum_{i \\in R_{j,m}}g_i)\\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} + \\cfrac{1}{2}(\\sum_{i \\in R_{j,m} }h_i + \\lambda)(\\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda})^2 \\\\ & = -\\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} + \\cfrac{1}{2} \\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} \\\\ & = -\\cfrac{1}{2}\\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} \\end{aligned} \\tag{7}因爲要的是 ＂score＂ ，所以讓 loss 對 x 軸做對稱，拿掉係數項 $\\frac{1}{2}$ \\text{Similarity Score} = \\cfrac{(\\sum g_i)^2}{\\sum h_i + \\lambda} \\tag{8} $g_i$ 為 first derivative of loss function related to data sample $x_i$ : $g_i = \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, \\hat{y_i})$ $h_i$ 為 second derivative of loss function related to data sample $x_i$: $h_i = \\cfrac{d^2}{d \\ F_{m-1}^2} l(y_i, \\hat{y_i})$ 拿掉係數項沒影響，原因是計算 gain (式 (1)) 時只需要相對 (relative) 的值 式 (8) 中的 $g_i, h_i$ 分別表 loss function $l(y,\\hat{y})$ 的 first derivative and second derivative classification 常用的 loss function 為 cross entropy/logistic loss \\begin{aligned} l(y_i,\\hat{y_i}) &= -[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)] \\\\ &=-y_i \\log(odds) + \\log(1 + \\exp(\\log(odds)) \\\\ &= -y_i F_{m-1}(x_i) + \\log(1 + \\exp(F_{m-1}(x_i))) \\end{aligned}\\tag{9} $F_{m-1}(x_i)$ 輸出的是 log(odds) $p_i$ 為 $F_{m-1}(x_i)$ 輸出的 probability , $p_i= \\sigma(F_{m-1}(x_i)) = \\cfrac{\\exp(F_{m-1}(x_i))}{1 + \\exp(F_{m-1}(x_i))}$ 將式 (9) 代入 $g_i, h_i$，得到 cross entropy/logistic loss 下的 $g_i, h_i$ \\begin{aligned} g_i &= \\cfrac{d}{d \\ F_{m-1}} \\ l(y_i, \\hat{y_i}) \\\\ &= -y_iF_{m-1}(x_i) + \\log(1 + \\exp(F_{m-1}(x_i)) \\\\ &= -y_i + \\cfrac{\\exp(F_{m-1}(x_i))}{1 + \\exp(F_{m-1}(x_i))} \\\\ & = -(y_i - p_i) = \\text{negatvie residual} \\\\ h_i &= \\cfrac{d^2}{d^2 \\ F_{m-1}} l(y_i, \\hat{y_i}) \\\\ &= \\cfrac{d^2}{d F_{m-1}^2} [-y_i F_{m-1}(x_i) + \\log(1 + \\exp(F_{m-1}(x_i)))] \\\\ &= \\cfrac{d}{d F_{m-1}} [-y_i + \\cfrac{\\exp(F_{m-1}(x_i))}{1 + \\exp(F_{m-1}(x_i))}] \\\\ &= \\cfrac{\\exp(F_{m-1}(x_i))}{(1 + \\exp(F_{m-1}(x_i)))} \\times \\cfrac{1}{(1 + \\exp(F_{m-1}(x_i)))} \\\\ &= p_i \\times (1-p_i) \\end{aligned} \\tag{10}代回 式(8)，得證 \\begin{aligned} \\text{Similarity Score} &= \\cfrac{(\\sum_{i \\in R_{j,m}} g_i)^2}{\\sum_{i \\in R_{j,m}} h_i + \\lambda} \\\\ & = \\cfrac{(\\sum-(y_i-p_i))^2}{(\\sum p_i (1-p_i)) + \\lambda} \\\\ &= \\cfrac{(\\sum \\text{residual}_i)^2}{(\\sum \\text{previous probability}_i \\times (\\text{1 - preveious probability}_i) ) + \\lambda} \\end{aligned}Regression Leaf output 公式怎麼來的？ 式(3) each leaf node 的 output 公式怎麼得出的？ \\text{Output Value} = \\cfrac{(\\sum \\text{residual}_i)}{ \\sum[\\text{previous probability}_i \\times (1- \\text{previous probability}_i)]+ \\lambda} \\tag{3}從通用 objective function 的 optimal output value $\\gamma_{j,m}^*$ 可求出 \\gamma_{j,m}^*=- \\cfrac{\\sum_{i\\in R_{j,m}} g_i}{\\sum_{i \\in R_{j,m}}h_i + \\lambda} \\tag{5}將 式 (10)的 $g_i$ $h_i$ 代入式 (5)，得證 \\gamma_{j,m}^*=\\cfrac{\\sum_{i\\in R_{j,m}} (y_i - p_i)}{\\sum_{i \\in R_{j,m}}(p_i \\times (1- p_i)) + \\lambda} = \\cfrac{(\\sum \\text{residual}_i)}{ \\sum[\\text{previous probability}_i \\times (1- \\text{previous probability}_i)]+ \\lambda} $p_i$ 為 $F_{m-1}(x_i)$ 輸出的 probability , $p_i= \\sigma(F_{m-1}(x_i)) = \\cfrac{\\exp(F_{m-1}(x_i))}{1 + \\exp(F_{m-1}(x_i))}$ 關於 Cover Cover is related to the minimum number of Residuals in a leaf 在 ＂ 如何 Pruning - Cover＂ 這章節提到 classification 中 cover 的運算為 式(2) similarity score 分母中的前項 \\text{Cover} = \\sum[\\text{previous probability}_i \\times (1- \\text{previous probability}_i)]實際上就是 式(8) similarity score 中分母部份的 summation of $h_i$ \\text{Cover} = \\sum_{i \\in R_{j,m}} h_iReference Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. https://doi.org/10.1145/2939672.2939785 What makes “XGBoost” so Extreme? https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb XGBoost-from-scratch-python Boosting algorithm: XGBoost https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d https://www.hrwhisper.me/machine-learning-xgboost/ XGBoost Part 2 (of 4): Classification XGBoost Part 3 (of 4): Mathematical Details My post 一步步透視 GBDT Regression Tree 一步步透視 GBDT Classifier 透視 XGBoost(1) 圖解 Regression 透視 XGBoost(3) 蘋果樹下的 objective function 透視 XGBoost(4) 神奇 optimization 在哪裡？","link":"/XGBoost-for-classification/"},{"title":"推薦系統中的瑞士刀 Factorization Machine","text":"多才多藝的 Factorization Machine在推薦領域，現今各種 DNN 模型當道，有個不怎麼深的模型卻小巧而美，既能做召回又能做排序，計算複雜度上又沒 DNN 耗時又耗錢 他 就是現今各種應用在推薦系統 DNN 的前輩 Factorization Machine (FM)，這麼多才多藝的模型你值得擁有。 我們都知道 DNN 的本質是特徵的高階交叉，這點在 FM 上就能初見端倪： y(x) = w_0 + \\sum^n_{i=1}w_ix_i + \\sum^{n-1}_{i=1}\\sum^n_{j=i+1}x_ix_j \\tag{1}FM 前兩項為 LR: $w_0 + \\sum^n_{i=1}w_ix_i$，末項為二階特徵交叉項 \\sum^{n-1}_{i=1} \\sum^n_{j=i+1}x_ix_j，這形式像極了 wide &amp; deep，根本是 wide &amp; deep 的前身，linear 項負責 memorization; cross 項負責 generalization。 FM 既能做召回又能做排序，甚至能在多路召回後對上萬個 entity 粗排打分，到底是怎麼做到的呢？ 讓我們往下繼續看下去 特徵 label 化首先在推薦領域中，特徵大部分都可被離散化/ label 化，也就是所謂的分桶 label，離散化後的特徵 $x$ 非 0 即 1，所以式 (1) 可以簡化成 y(x) = w_0 + \\sum^n_{i=1}w_i + \\sum^{n-1}_{i=1}\\sum^n_{j=i+1} \\tag{2}Label 化例子： gender 這個 feature 有三個取值 Men Women Unknown label 化後取值為 0 ~ 2 1234'Men': 0,'Women': 1,'Unknown': 2 事實上， gender 的 label 化 跟 3 dimensions 的 one-hot encoder 是一體兩面的 12340: [1, 0, 0],1: [0, 1, 0],2: [0, 0, 1] label 化，相當於 one-hot encoder 的壓縮，省去零值的儲存，因爲 matrix multiplication 時零值對結果沒影響 ，計算時只需從 weight 中取出 label 所對應的 index 維度即可。 推薦系統中，特徵維度上百萬甚至上千萬指的是 one-hot encode 展開來後的維度，實際上運算的時候不需要這麼多，因爲特徵非常非常稀疏。 FM 向量召回Math Background先把 式(2) 中的二階項的 hidden vector $v$ 拆解成 item hidden vector $v^{item}$ 跟 user hidden vector $v^{user}$ 的關係 \\begin{aligned} &\\sum^{n-1}_{i=1}\\sum^n_{j=i+1} \\\\ &= \\color{red}{\\sum_{i=1}^{n^{user } -1} \\sum_{j=i + 1}^{n^{user}} < v^{user}_i, v^{user}_j>} + \\color{blue}{ \\sum_{i=1}^{n^{item } -1} \\sum_{j= i + 1}^{n^{item}} } + \\color{green}{} \\\\ &= \\color{red}{\\sum_{i=1}^{n^{user } -1} \\sum_{j= i + 1}^{n^{user}} < v^{user}_i, v^{user}_j>}+ \\color{blue}{ \\sum_{i=1}^{n^{item } -1} \\sum_{j= i + 1}^{n^{item}} } + \\color{green} {} \\\\ & = \\text{a number} \\end{aligned} \\tag{3} $V^{user}_u$ 為 user-Id $u$ 的所有 user features’ hidden vector 逐位和 12345''' @ one_user_hidden_vectors shape: [number of user features, hidden vector dimension]''' sum_of_one_user_hidden_vector = np.sum(one_user_hidden_vectors, axis=0) # shape: [hidden vector dimension] $V^{item}_i$ 為 item-Id $i$ 所有的 item features’ hidden vector 逐位和 $n^{user} + n^{item} = n$ 式(3) 白話說就是三個部分的加法 \\color{red}{\\text{cross prod sum of user hidden vector } v^{user} } + \\color{blue}{\\text{cross prod sum of item hidden vector } v^{item}} + \\color{green}{\\text{inner product of } V^{user}_u \\text{and } V^{item}_i }以同樣方式分解式 (2)中的一階項 \\begin{aligned} \\sum^n_{i=1}w_i &= \\color{red} {\\sum w^{user} }+ \\color{blue}{\\sum w^{item} }\\\\ &= \\color{red}{ W^{user}} + \\color{blue} {W^{item} } \\end{aligned} \\tag{4} $w^{user}$ 為與 user features 對應的 weight $w^{item}$ 為與 item features 對應的 weight $W^{user}$ 為所有 user features 對應的 weights 和 $W^{item}$ 為所有 item features 對應的 weights 和 用 式(3) 式(4) 改寫式(2)： \\begin{aligned} \\tilde{y} &= \\not{x_0} \\quad + \\\\ & \\color{red} {W^{user} }+ \\color{blue} {W^{item} }+ \\\\ & \\color{red}{\\sum_{i=1}^{n^{user } -1} \\sum_{j= i + 1}^{n^{user}} < v^{user}_i, v^{user}_j>}+ \\color{blue}{ \\sum_{i=1}^{n^{item } -1} \\sum_{j= i + 1}^{n^{item}} } + \\color{green} {} \\end{aligned} \\tag{5} bias 大家都一樣，要的只是相對的 score，所以可以去掉 $V^{user}_{u}$ 為 user-id $u$ 的 user features’s hidden vector 逐位和 $V^{item}_i$ 為 item-Id $i$ 所有的 item features’ hidden vector 逐位和 推薦系統在做向量召回時，通常是以 cosine similarity 做為 score 衡量兩個向量的 similarity，式 (5) 可以轉化成兩個 vector 的 inner product，即 \\text{similarity score} = E^{user}_u \\cdot {E^{item}_i} \\tag{6} $E^{user}_u$ 為 user-id $u$ 的 embedding $E^{item}_i$ 為 item-id $i$ 的 embedding 式(5) 推導到 式(6) 非常 trikcy，不過看下圖也就一目了然了 user embedding vector $E^{user}_u$ 的維度為 $dim(V^{user}_u) + 2$，在 $V^{user}_{u}$左側 concatenate 2 維 item embedding vector $E^{item}_{i}$ 的維度為 $dim(V^{item}_i) + 2$，在 $V^{item}_i$ 左側 concatenate 2 維 計算向量內積 ，式 (7) 神奇的跟 式 (5) 相等！ \\begin{aligned} \\text{similarity score} & = \\\\ & = \\color{red}{W^{user} + \\sum_{i=1}^{n^{user } -1} \\sum_{j= i + 1}^{n^{user}} < v^{user}_i, v^{user}_j>} + \\color{blue}{W^{item} + \\sum_{i=1}^{n^{item } -1} \\sum_{j= i + 1}^{n^{item}} } + \\color{green}{}\\\\ &= \\tilde{y} \\end{aligned} \\tag{7}總結一下，FM 的 output score $y$ 可以拆解成用兩個 embedding vector $E^{user}_u$ $E^{item}_i$ inner product 表示，這特性用在召回時非常方便。 使用說明書我們在線下訓練完 FM 之後，分別將 user Embedding $E^{user}$ , item Embedding $E^{item}$ 存入 Faiss 或是任何其他相似的 vector similarity search engine 到了線上，觸發 user-id $u$ 的推薦: I2I 取回 user-id $u$ 的歷史交互 item-ids $I$ 對每個 item-id $i \\in I$，取回其 item embedding $E^{item}_i$ 將 $E^{item}_i$ 拿去 faiss search 相似的 item-ids，完成 I2I 召回 U2I 取回 user-id $u$ 的 user embedding $E^{user}_u$ 將 $E^{user}_u$拿去 faiss search 相似的 item-ids 完成 U2I 召回 上面的 I2I 可以做成 offline 版本，預先將所有 item-id $i$ 的 TopK I2I 算出後存進 redis ，線上只要根據 item-id $i$ 取出對應的 item-ids 即可 Training TipsFM 用在召回任務時，用意是學出更好的 user and item embedding，所以在 objective function 的選擇上使用 logit loss 不是不行，但有更好的作法。 Tomas Mikolov 在他 word2Vec 裡採用的 Negative Sampling 優化 softmax 的運算瓶頸，Negative Sampling (NCE) 可以從 cross entropy 一步步推導出來 參見 Word2Vec (3):Negative Sampling 背後的數學 對比於 Hierarchical Softmax 本質上還是學出 likelihood of correct words，Negative Sampling 對於學到更好的 representation of word distribution 有奇效。也因為 Negative Sampling 能學出更好的 word distribution， google 2016 年的 youtube net、 airbnb 2018 年的 listings embedding 都有類似的做法，所以在召回問題上 objective function 可以採用 NCE 優化之。 FM 排序FM 用在排序時可分成 粗排 (coarse rank) 和精排 (fine rank)，兩者運算過程一樣，只不過粗排用的特徵數量應遠少於精排，因為粗排得對多路召回的上萬個 entity 打分截斷。 使用方式線上觸發 user-id $u$ 的推薦： 取回 user-id $u$ 的 user embedding $E^{user}_u$ 和待排序 items $I$ 的 item embeddings $E^{item}_{I}$ 將 $E^{user}_u$ 和所有 $E^{item}_I$ inner product 計算出 score $\\text{S}_I^u$，根據式 (7) 兩者相等 對 $S^u_{I}$ 排序截斷 topK Some Tips在排序時，若無依賴線上 context 的特徵，從 DB 取出 $E^{user}_u$ $E^{item}_i$ 按照 式(6) 計算 score 即可。 若有依賴 context 的特徵，得按照 式 (5) 計算 score 。 例如: 若 item 側有個 feature 為 “多路召回中來自哪路”，這是線上才能得知的 context 訊息，無法離線算好，所以得將 FM model 式 (1) 的所有 $\\theta$ 都存下來帶到線上，照著 式 (5) 計算 score。 另外，當我們在計算 cross prod sum of hidden vectors 時，別忘了有複雜度更低的計算方式 \\sum^{n-1}_{i=1}\\sum^n_{j=i+1} = \\frac{1}{2} \\sum_{f=1}^k{\\left( \\left(\\sum_{i=1}^n{v_{i,f}x_i}\\right)^2 - \\sum_{i=1}^nv_{i,f}^2 x_i^2 \\right)}Implement FM by Pytorchdataset 用 movie len ml-1m MovieLens Bucketize and Label Features上面說過，在推薦系統中的特徵都得分桶 label 化， movie len 的 features 都已分桶過的，所以我們只需要 label 化即可 P.S. Sklearn 中有現成的 sklearn.preprocessing.LabelEncoder 和 sklearn.preprocessing.KBinsDiscretizer 可以用 label encoder >folded12123456789101112131415161718192021222324252627282930313233343536373839404142class LabelEncoder(BaseEstimator, TransformerMixin): def __init__(self, columns_to_encode: List[str]): self.columns_to_encode = columns_to_encode self.unseen = -1 def fit(self, df: pd.DataFrame, y=None): df_ = df[self.columns_to_encode].copy() df_[self.columns_to_encode] = df[self.columns_to_encode].astype( 'str') unique_column_vals = {col: df_[col].unique() for col in self.columns_to_encode} self.encoding_dict_ = OrderedDict() for col in self.columns_to_encode: unique_value = unique_column_vals[col] self.encoding_dict_[col] = {val: idx for idx, val in enumerate(unique_value)} self.encoding_dict_[col][self.unseen] = len(self.encoding_dict_[col]) return self def transform(self, df: pd.DataFrame): try: self.encoding_dict_ except AttributeError: raise NotFittedError( &quot;This LabelEncoder instance is not fitted yet. &quot; &quot;Call 'fit' with appropriate arguments before using this LabelEncoder.&quot; ) df_ = df.copy() filtered_columns = [col for col in self.columns_to_encode if col in df_.columns] df_[filtered_columns] = df_[filtered_columns].astype('str') for col, encoding_map in self.encoding_dict_.items(): original_value = [f for f in encoding_map.keys() if f != self.unseen] if col in filtered_columns: df_[col] = np.where(df_[col].isin( original_value), df_[col], self.unseen) df_[col] = df_[col].apply(lambda x: encoding_map[x]) return df_ Before labeling: After labeling: P.S. userId label 化後的 label 值跟 原本的 Id 會不一樣，很容易搞混。 把 gender ecoding 後的對應關係打出來看看: In: 1id_encoder.encoding_dict_['gender'] Out: 1{-1: 2, 'F': 0, 'M': 1} FM Model自己實現 FM FMmodel >folded121234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class FactorizationMachine(torch.nn.Module): def __init__(self, reduce_sum=True): super().__init__() self.reduce_sum = reduce_sum def forward(self, x): square_of_sum = torch.sum(x, dim=1) ** 2 sum_of_square = torch.sum(x ** 2, dim=1) ix = square_of_sum - sum_of_square if self.reduce_sum: ix = torch.sum(ix, dim=1, keepdim=True) # [b_size, 1] return 0.5 * ix class FactorizationMachineModel(torch.nn.Module): def __init__(self, fields_index: Dict[str, int], fields_dims: Dict[str, int], embed_dim): super(FactorizationMachineModel, self).__init__() self.fields_index = fields_index self.all_fields = [k for k, v in sorted(self.fields_index.items(), key=lambda kv: kv[1])] self.embed_dim = embed_dim self.fields_dims_dict = fields_dims self.fields_dims = np.array([self.fields_dims_dict[f] for f in self.all_fields]) self.fields_offset = torch.tensor((0, *np.cumsum(self.fields_dims)[:-1]), dtype=torch.long).requires_grad_(False).unsqueeze(0) ''' field_offset [1, sum_of_field_dims] field_dims: np.array([10, 20 ,5, 7, 9]) [*np.cumsum(self.field_dims)[:-1]] = [10, 30, 35, 42] ''' self.fields_range = self._gen_fields_range(self.fields_dims) self.embedding = torch.nn.Embedding(self.fields_dims.sum(), embed_dim) torch.nn.init.xavier_uniform_(self.embedding.weight.data) self.fc = torch.nn.Embedding(self.fields_dims.sum(), 1) self.bias = torch.nn.Parameter(torch.zeros((1, ))) self.fm = FactorizationMachine(reduce_sum=True) def _forward_embedding(self, X: torch.Tensor): return self.embedding(X) # [b_size, field_num, embed_dim] def _forward_linear(self, X: torch.Tensor): return torch.sum(self.fc(X), dim=1) + self.bias # [b_size, output_dim] def get_field_vector(self, field_name: str, label: int)-&gt; np.ndarray: assert field_name in self.fields_index assert label &lt; self.fields_dims_dict[field_name] field_index = self.fields_index[field_name] field_range = self.fields_range[field_index] field_vector = self.embedding.weight.data[field_range[0]: field_range[1]] return field_vector[label] def _gen_fields_range(self, fields_dims: np.ndarray)-&gt; List[Tuple[int, int]]: fields_offset = [0, *np.cumsum(fields_dims)] return [(s, e) for s, e in zip(fields_offset, fields_offset[1:])] def forward(self, X: torch.Tensor): # X [b_size, num_fields] if X.get_device() != self.fields_offset.get_device(): self.fields_offset = self.fields_offset.to(X.device) X = X + self.fields_offset X = self._forward_linear(X) + self.fm(self._forward_embedding(X)) # [b_size, 1] return X.squeeze(1) # [b_size] def predict_probs(self, X: torch.Tensor): with torch.no_grad(): out = self.forward(X) return torch.sigmoid(out) self.embedding 存放 hidden vector ，也就是 式(1) 中的 $v$ self.fc, self.bias 分別為 式(1) 中的 bais 跟 weight instance self.fm 計算 cross prod sum of hidden vector: $\\frac{1}{2} \\sum_{f=1}^k{\\left( \\left(\\sum_{i=1}^n{v_{i,f}x_i}\\right)^2 - \\sum_{i=1}^nv_{i,f}^2 x_i^2 \\right)}$ self.fields_offset，存放特徵 field $i$ 在 self.embedding 中對應的起始位置 i.e gender 這個特徵有 3 個取值 Men, Women, unknown 形成一個 field，所以 gender 在 self.embedding 會佔據三個 rows ， self.fields_offset 存放 gender 第一個取值 Men 在 self.embedding 中的 row index 訓練過程請參閱 seed9D/hands-on-machine-learning 組合出 $E^{user}$ $E^{item}$訓練完後，取出 FM model 裡的 weight $W$ 跟 hidden vector $V$，按照 figure 1 所示，組合出 user embedding 跟 item embedding FMEmbedding >folded12123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106from collections import namedtupleID2Vector = namedtuple('id2vector', 'id vector')class FMEmbedding: def __init__(self, id_encoder, user_fields, item_fields, fm_model): self.id_encoder = id_encoder self.user_fields = user_fields self.item_fields = item_fields self.fm_model = fm_model self.fields_dims_dict = fm_model.fields_dims_dict self.fields_index= fm_model.fields_index self.fields_dims = fm_model.fields_dims self.fields_range = self._gen_fields_range(self.fields_dims) self.fields_vectors = self.fm_model.embedding.weight.data.numpy() # hidden vector self.fields_weights = self.fm_model.fc.weight.data.numpy() self.bias = self.fm_model.bias.data.numpy() def get_user_embedding(self, primitive_features: pd.DataFrame, userId_column='userId')-&gt; List[ID2Vector]: assert set(primitive_features.columns) == set(self.user_fields) user_id_features = self.id_encoder.transform(primitive_features).values user_fields_index = [self.fields_index[col] for col in self.user_fields] fields_offset = np.array([0, *np.cumsum(self.fields_dims)[:-1]])[user_fields_index].reshape((1, -1)) user_IDs_features = user_id_features + fields_offset ''' field vector''' user_IDs_vectors = np.take(self.fields_vectors, user_IDs_features, axis=0) # [b_size, user_field_size, vector_dims] user_fields_cross = self._cross_vector(user_IDs_vectors) # [b_size] user_IDs_vectors = user_IDs_vectors.sum(axis=1) # [b_size, vector_dims] '''field weight''' user_fields_weights = np.take(self.fields_weights, user_IDs_features, axis=0).squeeze(2) # [b_size, user_field_size] user_fields_weights = user_fields_weights.sum(axis=1) # [b_size] ''' user embebding composition [1 :: (user_fields_cross + user_fields_weghts):: user_IDs_vectors] ''' ones = np.ones((len(user_fields_weights), 1)) # [b_size] embedding = np.hstack([ones, np.expand_dims(user_fields_cross + user_fields_weights, 1), user_IDs_vectors]) id2vector = [ID2Vector(userId, vector) for userId, vector in zip(primitive_features[userId_column].tolist(), embedding)] return id2vector def get_item_embedding(self, primitive_features: pd.DataFrame, itemId_col='itemId')-&gt; List[ID2Vector]: assert set(primitive_features.columns) == set(self.item_fields) ''' primitive_features may contain unseen itemId ''' item_id_feautres = self.id_encoder.transform(primitive_features).values item_fields_index = [self.fields_index[col] for col in self.item_fields] fields_offset = np.array([0, *np.cumsum(self.fields_dims)[:-1]])[item_fields_index].reshape((1, -1)) item_IDs_features = item_id_feautres + fields_offset ''' field vector ''' item_IDs_vectors = np.take(self.fields_vectors, item_IDs_features, axis=0) # [b_size, item_fields_size, field_vector_dims] item_fields_cross = self._cross_vector(item_IDs_vectors) # [b_size] item_IDs_vectors = item_IDs_vectors.sum(axis=1) # [b_size, field_vector_dims] '''field weight''' item_fields_weights = np.take(self.fields_weights, item_IDs_features, axis=0).squeeze(2) # [b_size, item_field_size] item_fields_weights = item_fields_weights.sum(axis=1) # [b_size] # '''bias'''# bias = np.full((len(item_fields_weights), 1), self.bias.data) ''' item embedding composition: [(item_fields_weights + item_fileds_cross ):: 1 :: item field vector] ''' ones = np.ones((len(item_fields_weights), 1)) # [b_size, 1] sum_ = np.expand_dims(item_fields_weights + item_fields_cross, 1) embedding = np.hstack([sum_, ones, item_IDs_vectors]) id2vector = [ID2Vector(itemId, vector) for itemId, vector in zip(primitive_features[itemId_col].tolist(), embedding) ] return id2vector def _cross_vector(self, vectors: np.ndarray)-&gt; int: ''' @vectors: [b_size, vector_dims] ''' square_of_sum = np.sum(vectors, axis=1) ** 2 sum_of_square = np.sum(vectors ** 2, axis=1) reduce_sum = np.sum(square_of_sum - sum_of_square, axis=1) # [b_size, 1] return 0.5 * reduce_sum def _get_field_vector(self, field_name: str, label: int)-&gt; np.ndarray: assert field_name in self.fields_index assert label &lt; self.fields_dims_dict[field_name] field_index = self.fields_index[field_name] field_range = self.fields_range[field_index] field_vector = self.fields_vectors[field_range[0]: field_range[1]] return field_vector[label] def _gen_fields_range(self, fields_dims: np.ndarray)-&gt; List[Tuple[int, int]]: fields_offset = [0, *np.cumsum(fields_dims)] return [(s, e) for s, e in zip(fields_offset, fields_offset[1:])] function get_user_embedding 組合出 user embedding \\text{concat}(1, \\color{red}{W^{user} + \\sum_{i=1}^{n^{user } -1} \\sum_{j= i + 1}^{n^{user}} < v^{user}_i, v^{user}_j>}, \\color{green}{V^{user}_u}) function get_item_embedding 組合出 item embedding \\text{concat}(\\color{blue}{W^{item} + \\sum_{i=1}^{n^{item } -1} \\sum_{j= i + 1}^{n^{item}} }, \\color{black}{1}, \\color{green}{V^{item}_i})print 一個 item embedding 看看 In: 12item_id_2_vector = fm_embedding.get_item_embedding(item_df, 'movieId')print(item_id_2_vector[0]) Out: 12345678910111213id2vector(id=1, vector=array([-1.6513232 , 1. , -0.15136771, -0.19989893, -0.17711505, -0.27605495, -0.17580783, -0.20358004, 0.19178246, 0.28852561, 0.19627182, -0.21493186, 0.22243072, 0.24396233, -0.17809424, 0.19624405, 0.21878579, 0.20799968, 0.12430456, 0.18400647, -0.18915175, 0.17806746, 0.21123466, -0.15553948, -0.20221886, -0.23133394, -0.19063245, 0.25346702, -0.19234048, -0.20784694, -0.12557872, -0.25455537, -0.15617739, -0.22168253, -0.14932276, 0.25438485, 0.19298089, -0.23894864, -0.26424453, -0.18523659, 0.20755866, -0.17146595, -0.1505574 , 0.26266149, 0.12615746, -0.16710922, -0.19842891, 0.20556726, 0.16274993, 0.14940131, 0.16785385, 0.24925581])) print 一個 user embedding 看看 In: 12user_id_2_vector = fm_embedding.get_user_embedding(user_df, 'userId')print(user_id_2_vector) Out: 12345678910111213id2vector(id=1,vector=array([ 1. , 1.57251287, 0.31802261, -0.05311833, -0.51956671, -0.41219246, -0.24985576, -0.65933889, 0.59215838, 0.41595197, 0.11497089, -0.69019455, 0.24329846, 0.45048714, -0.05286196, 0.2054476 , 0.30616254, 0.30125472, 0.40432882, 0.26946735, -0.28497586, 0.23863903, 0.37228948, -0.47429329, -0.21513283, -0.37172595, -0.29311907, 0.32599026, -0.37352464, -0.65939361, -0.13792202, -0.29631072, -0.70424175, -0.03108542, -0.47497728, 0.65669644, -0.0536177 , -0.46591866, 0.04575365, -0.47661048, -0.23746635, -0.48891699, -0.39626658, 0.34011322, 0.42570654, -0.53802925, -0.32868454, 0.31107759, 0.65648586, 0.36503255, 0.22789076, 0.56083 ])) 塞進 Faiss將每個 user-id embedding $E^{user}$ 和 iterm-id embedding $E^{item}$ 分別塞入 Faiss 建立 index Item embedding: 1234567891011''' Offline push item embedding into faiss'''all_item_id, item_embedding = zip(*item_id_2_vector)all_item_id = np.array(all_item_id)item_embedding = np.array(item_embedding)movie_embedding_faiss_index = faiss.IndexFlatIP(item_embedding.shape[1])movie_id_faiss_index = faiss.IndexIDMap(movie_embedding_faiss_index)movie_id_faiss_index.add_with_ids(item_embedding.astype('float32'), np.array(all_item_id)) user embedding: 12345678910''' Offline push user embedding into faiss'''all_user_id, user_embedding = zip(*user_id_2_vector)all_user_id = np.array(all_user_id)user_embedding = np.array(user_embedding)user_embedding_faiss_index = faiss.IndexFlatIP(user_embedding.shape[1])user_id_faiss_index = faiss.IndexIDMap(user_embedding_faiss_index)user_id_faiss_index.add_with_ids(user_embedding.astype('float32'), np.array(all_user_id)) IndexFlatIP 為 brute force 的 inner product，Faiss 還有其他更快的索引方式，但就得犧牲 式(6) $E^{iterm}$ $E^{user}$ inner product 的完備性。 應用I2IOnline Computing Similarityonline 版的 I2I，預先將所有 item embedding $E^{item}$ 存進 Faiss，到線上實時計算 similarity score。 首先取回 trigger item-ids 的 vectors 1234567'''online fetch movie ids and their corresponding vectors which user have interacted'''item_vector_provider = VectorProvider(item_id_2_vector)user_seen_movie_id = [10 , 20, 50, 70]vectors = np.array([item_vector_provider[id_] for id_ in user_seen_movie_id]) item_vector_provider 為 item-id $i$ 的 vector 查詢工具 user_seen_movie_id 為 trigger item-ids，來自 user 曾經互動過的 items 送入 faiss 查詢 I2I 12each_match_num = 20sim_score, movie_ids = movie_id_faiss_index.search(vectors.astype('float32'), each_match_num) print 看看所有召回的 item-ids In: 1print(movie_ids.flatten()) Out: 12345678array([3120, 2688, 3113, 3596, 1850, 2283, 2175, 2379, 64, 2372, 437, 3835, 2149, 2950, 3623, 315, 2589, 3616, 1686, 1752, 1322, 3667, 1490, 1324, 1335, 1853, 3574, 3313, 2534, 1989, 1170, 1595, 2298, 867, 2555, 244, 2818, 220, 2816, 473, 557, 2999, 3245, 1842, 2839, 3881, 1850, 1039, 1316, 701, 2909, 3679, 83, 1177, 2175, 3828, 3849, 1294, 814, 1225, 3667, 244, 1490, 2382, 2365, 2898, 1556, 3666, 2298, 1978, 387, 1853, 3407, 1980, 3041, 3220, 3042, 1720, 148, 884]) Offline Computing SimilarityOffline 版的 I2I 為離線計算好所有 item-ids 的 I2I 存進 DB，線上使用時直接取用 計算所有 I2I，每個 triggerＩ 召回 30 個 I 12each_match_num = 30sim_score, match_movie_ids = movie_id_faiss_index.search(item_embedding.astype('float32'), each_match_num) 結構化，方便持久化儲存 1234567key_2_I = []for trigger_id, match_ids, match_scores in zip(all_item_id, match_movie_ids, sim_score): k2i = {} k2i['trigger_key'] = trigger_id pairs = [{'key':id_, 'score': s } for id_, s in zip(match_ids, match_scores) if i != trigger_id] k2i['pairs'] = pairs key_2_I.append(k2i) print 其中一個 trigger key 的結構看看 In: 1print(key_2_I[0]) Out: 123456789101112131415161718192021222324252627282930313233{'trigger_key': 1,'pairs': [{'key': 2562, 'score': 8.487403}, {'key': 3359, 'score': 7.370528}, {'key': 853, 'score': 7.3140664}, {'key': 1910, 'score': 7.243814}, {'key': 3778, 'score': 7.064466}, {'key': 2819, 'score': 6.9514284}, {'key': 475, 'score': 6.9262433}, {'key': 551, 'score': 6.886018}, {'key': 1262, 'score': 6.7728105}, {'key': 1206, 'score': 6.7252}, {'key': 1268, 'score': 6.680213}, {'key': 265, 'score': 6.674436}, {'key': 722, 'score': 6.644379}, {'key': 3281, 'score': 6.6103525}, {'key': 2299, 'score': 6.536831}, {'key': 1983, 'score': 6.5208454}, {'key': 1188, 'score': 6.499143}, {'key': 1730, 'score': 6.414803}, {'key': 240, 'score': 6.3780065}, {'key': 2757, 'score': 6.29891}, {'key': 2731, 'score': 6.23072}, {'key': 3534, 'score': 6.217514}, {'key': 746, 'score': 6.1960998}, {'key': 326, 'score': 6.194806}, {'key': 3296, 'score': 6.143187}, {'key': 1058, 'score': 6.070843}, {'key': 2503, 'score': 6.015757}, {'key': 1187, 'score': 5.9964414}, {'key': 1218, 'score': 5.899559}, {'key': 2782, 'score': 5.8903465}] } 存進 DB (i.e redis) 線上 I2I 使用 U2IU2I 為利用 user embedding $E^{user}_u$ 在 Faiss 內搜尋與之相似的 item Embedding $E^{item}_i$ 首先取回 user-id $u$ 的 embedding 123user_vector_provider = VectorProvider(user_id_2_vector)query_user_id = 500u_embedding = user_vector_provider[query_user_id] 將 u_embedding 丟進 Faiss 找相似的 item 123456''' excute u2i'''topk = 20sim_scores, match_movie_ids = movie_id_faiss_index.search(np.expand_dims(u_embedding.astype('float32'), 0), topk)''' now we have socre and match item ids''' print item-ids 看看 In: 1print(match_movie_ids.flatten()) Out: 12array([ 557, 3245, 2999, 2839, 1842, 755, 3881, 1316, 2503, 2909, 3338, 1664, 701, 406, 3828, 682, 2833, 2811, 3517, 53]) 排序因為特徵不涉及 context 類型的，所以直接取回 user embedding $E^{user}_u$ 與多個 item embedding $E^{item}$ 內積計算 score 即可 拿回待排序 item-ids 12345678''' suppose we have the following match items from different methods'''uid = 1000match_from_A = np.random.choice(np.array(list(item_vector_provider.ID2Vector.keys())), size=30)match_from_B = np.random.choice(np.array(list(item_vector_provider.ID2Vector.keys())), size=30)match_from_C = np.random.choice(np.array(list(item_vector_provider.ID2Vector.keys())), size=30)all_match = np.hstack([match_from_A, match_from_B, match_from_C]) 取回 user embedding 與所有待排序 item-ids 的 embedding 1234567''' online vector provider'''item_vector_provider = VectorProvider(item_id_2_vector)user_vector_provider = VectorProvider(user_id_2_vector)u_vector = user_vector_provider[uid]i_vectors = np.array([item_vector_provider[movie_id] for movie_id in all_match]) inner product 計算 score 12scores = np.dot(i_vectors, u_vector)predicted_prob = sigmoid(scores) 實際上只需要 scores 即可，sigmoid operation 為多餘的，兩者排序結果一樣，除非需要 probability 做其他操作 12345In: print((all_match[np.argsort(predicted_prob)[::-1]] == all_match[np.argsort(scores)[::-1]]).all())out: True 查看排序後的分數 1234567891011121314151617181920212223242526272829In: sorted_index = np.argsort(scores)[::-1] for idx in sorted_index: print(&quot;id:{}, score: {}&quot;.format(all_match[idx], scores[idx]))Out:id:669, score: 3.925238274386397id:942, score: 3.5017253691160786id:3030, score: 3.2159240692623685id:962, score: 2.5550285960726438id:3578, score: 2.2348229702588718id:1226, score: 2.1864282823175265id:3057, score: 2.0258533377047634id:128, score: 1.9924200765247109id:3559, score: 1.86036771697748id:3147, score: 1.8530369719991735id:1301, score: 1.721844059286982id:2843, score: 1.6223211152117063id:2686, score: 1.4088124542846256id:106, score: 1.3856411763378178id:2117, score: 1.2064026627971622id:1625, score: 1.0726163382326743id:3746, score: 0.9731542416184148id:3504, score: 0.9079088224687981id:373, score: 0.8813322676009746id:3101, score: 0.8557414634608994id:2236, score: 0.8039020559275596id:3405, score: 0.7829994401182859id:1210, score: 0.7523971549813273id:3255, score: 0.7294679680393682 Last but not Least本篇便於理解使用 python + pytorch 實現思路，實際上在工業界 offline 應該是在 spark 上處理數據和訓練 FM ; online 的推薦服務為 Java 或其他。 在 spark 上得自己實現 FM model，個人實現,因為用在公司生產上就不能貼了， 貌似 spark 3.0 有提供了，不過版本更新一向令人頭大。 Reference 推荐系统召回四模型之：全能的FM模型 https://zhuanlan.zhihu.com/p/58160982 推荐系统召回四模型之二：沉重的FFM模型 https://zhuanlan.zhihu.com/p/59528983 https://github.com/rixwew/pytorch-fm/ Factorization machine implemented in PyTorch https://www.kaggle.com/gennadylaptev/factorization-machine-implemented-in-pytorch https://www.zhihu.com/question/362190044/answer/945591801 notebook seed9D/hands-on-machine-learning","link":"/pratical-FM-model/"},{"title":"內容推薦 (1) 關鍵詞識別","text":"背景從內容召回說起電商推薦系統多路召回中，通常會有一路召回商品叫內容召回 $\\text{content I2I}$ content I2I 與其他根據用戶行為的召回不同， 最簡單的 content I2I 可以根據 A 商品 title 內容找出其他與 A 相似的 title 的商品，這給了他很強的推薦解釋性 對於商品冷啟動或者系統冷啟動來說，content I2I 可以在沒有用戶交互訊息時，就可以進行推薦，也不失為一種冷啟動方案。 在萬物皆可 Embedding 的今天，content I2I 只要把所有商品的 title 送進 word2Vec 硬 train 一發也就完事了 當然要是這麼簡單，也就不會有這篇了 推薦系統相關文章 一言難盡的商品 title我司的電商商品 title 為中翻英淘寶商品 title 而來，基本上毫無文法可言 如果用 word2Vec 硬做一發，再以 doc2vector 的思路融合成 sentence vector ，肯定會加入某些糟糕詞彙的 vector。 諸如此類的怪異詞彙： EX: “real time” (應該是實時發貨？) , liu haichang(劉海夾??), two yuan(兩元？), yiwu(義烏？) 為了讓 vector 能更好的表達句子 title，加上組內對於商品關鍵字有需求，於是就有了以下商品 title 挖掘出中 產品詞 與 標籤詞的識別任務 原理關鍵詞識別先解釋一下，什麼是產品詞，什麼是標籤詞 以下是我自己的定義： 所有商品都會有自己的 tilte，但肯定會有一個 “產品詞” 去描述這商品到底是賣什麼，他可以是單詞，稱為 unigram，也可以複合詞，bigram or trigram…… unigram:shirt , blouse bigram: apron dress, bermuda shorts trigram: buckle strap shoes, denim mini dress “ 標籤詞 “ (label words)，定義比較空泛， 狹義一點指那些可以用來形容商品或能凸顯商品特色的詞 unigram: denim, hipster bigram: chinese tunic, cotton padded trigram: deep v collar 廣義一點，也可以包含產品詞，最終還得看業務需求，標籤詞他還能在細分出 ＂屬性詞＂(propery) 領口：高領， 低領，V 領，深 V .. 材質：棉，麻 … 不管怎樣，如果沒有人工去蒐集出詞彙，那就得靠機器自己挖掘詞彙字典。 問題來了，在我們的商品池中的商品 title，基本上沒什文法可言，詞彙也是中翻英出來的。 如何找出有意義的詞彙組成的 ＂產品詞＂或＂標籤詞＂就是問題的核心了 EX:＂long＂,＂sleeved＂ 分別看沒什意義，但合起來變成 bigram words： ＂long sleeved” 就有意義 那要怎麼衡量 words 跟 words 之間的組合程度呢？ 就是 熵 entropy了 Entropy 識別關鍵字Entropy在 information theory 中 ，entropy 被用來衡量系統內的不確定性程度 H(X) = -\\sum_x p(x)\\log p(x)＂不確定性＂(uncertainty) 跟 information 豐富程度是一體兩面的 entropy 越高，代表不確定性越高，代表能提供的 information 也更多 舉例來說，如果明天會下雨的 probability 為 0.5，天氣系統的 entropy (以 2 為底)就是 -\\cfrac{1}{2} \\log_2\\cfrac{1}{2} - -\\cfrac{1}{2} \\log_2\\cfrac{1}{2} = 1如果明天會下雨的 probability 為 1，那天氣系統的 entropy 就是 -1log_21 = 0代表天氣系統完全沒有不確定性，明天肯定會下雨，對我們無法提供任何 information 我們可以利用 entropy 來衡量 : 內聚力：word 跟 word 之間的連結緊密程度，由互消息衡量之 豐富度：words 本身的自由運用程度，由 left entropy / right entropy 衡量之 互消息 (MI - Mutual Information)先上公式 I(X,Y) =\\sum_{y \\in Y}\\sum_{x \\in X} p(x,y) \\log(\\cfrac{p(x,y)}{p(x) p(y)})再上圖 看圖就可以直觀明白，$I(X,Y)$ 可以用來衡量兩個事件彼此的關聯性，直觀上互消息可以用來衡量兩個 word 之間的依賴程度。 $\\text{PMI}$ (point-wise mutual information) 也可以用來來衡量兩個 word 的相關性，他被視為簡化版的 $\\text{MI}$ PMI(x,y) = \\log \\cfrac{p(x,y)}{p(x)p(y)}word A 跟 word B 的 PMI(A， B) 或 MI(A，B）value 越高，代表 A, B 越相互依賴，組成一個 term 的可能性越越大 但從公式上不難看出，MI 是 weighted 過後的 PMI。在實務上，PMI 傾向給 “those word only occur together” 組成的 bigram 較高的分數 ; 而 MI 傾向給 high frequency bigram 更高的分數 EX:在商品 title 中有個詞 ＂small fresh＂ joint probability $\\text{p(“small”, “fresh”)} = 0.002$ $\\text{p(“small”)} = 0.0058$ $\\text{p(“fresh”)}= 0.0035$ $\\text{PMI(“small”, “fresh”)} = 4.59$ $\\text{MI(“small”, “fresh”)} =0.009$ 有另一個詞 ＂fresh loos＂ join probability $\\text{p(“fresh loose”)} = 0.0001$ $\\text{p(“fresh”) =0.0035}$ $\\text{p(“loose”)} = 0.0024$ $\\text{PMI(“fresh”, “loose”)} = 2.47$ $\\text{MI(“fresh”, “loose”)} =0.00024$ 顯然，對於 fresh 這個 word 而言，＂small fresh＂比 ＂fresh loose＂成詞程度較高。 左右熵左右熵代表了 word 本身可以自由運用的程度 我們知道，一個 word A 可以跟左邊的 word L，也可以跟右邊的 word R 組合，而左右熵就是來衡量 word A 組成 phase 的豐富程度 H_L(W) = -\\sum_{l \\in L}p(\\text{l::w | w}) \\ \\log_2 p(\\text{l::w| w}) \\\\ H_R(W) = -\\sum_{r \\in R}p(\\text{r::w | w}) \\ \\log_2 p(\\text{r::w| w})EX： 假設 ＂skirt ＂ 這個產品詞在池子中的鄰字組合計數如下 12345678910111213141516171819202122def cal_information(x): return - x * math.log(x)def cal_entropy(freqDict): total_count = sum(list(freqDict.values())) informations = [cal_information(fre / total_count) for fre in freqDict.values()] return sum(informations)skirt_left = { &quot;long skirt&quot;: 500, &quot;midi skirt&quot;: 1000, &quot;pegged skirt&quot;: 100, &quot;pleated skirt&quot;: 300, &quot;prairie skirt&quot;: 20 &quot;printed skirt&quot;: 400, &quot;sarong skirt&quot;: 80, &quot;trumpet skirt&quot;: 600 }skirt_right = { &quot;skirt suit&quot;: 500, &quot;skirt dress&quot;: 1000} 則 skirt 的 left entropy 為 E_L(\\text{\"skirt\"}) = 1.82right entropy為 E_R(\\text{\"skirt\"}) =0.63顯然對 “skirt” 而言，左側語境比右側豐富 Normalize Entropy &amp; PMI &amp; MIPMI, MI 與 entropy 的值域是個相對 unbound 的值，造成在使用時比較難拿捏 threshold，得來回比對數值決定成詞標準，解決方法是 normalize 值到固定範圍內: Normalizing PMI into (1, -1) \\text{PMI}_n(x, y) = (\\ln\\cfrac{p(x,y)}{p(x)p(y)}) / -\\ln p(x,y) Normalizing MI into (0, 1) \\text{MI}*n(X,Y) = \\cfrac{\\sum*{x,y} p(x,y) \\ln \\frac{p(x,y)}{p(x) p(y)}}{-\\sum_{x,y}p(x,y)\\ln p(x,y)} Normalizing entropy into (0, 1) H_n(X) = -\\sum_x \\frac{p(x) \\log p(x)}{\\log n}論文研究顯示 Normalized MI &amp; PMI 為對角線趨勢，但依然會有一定失真，所以在使用上得自行拿捏 一般來說 MI 偏向 high frequency，NMI 會稍微將高頻詞 push down ，低頻詞 pull up PMI 偏向 low frequency，NPMI 稍微降低低頻詞的 rank Score 成詞分數有了度量語境豐富度跟詞彙內聚力的工具後，得進一步定出一個 score 代表＂成詞程度＂，score 越高，代表這個詞成為有意義詞的可能性相對較高。 先定義一個 candidate phrase 的抽象表達，方便我們計算其成詞 score。 我們的 Candidate phrase 可以是以下這些組合： unigram candidate (special case) [unigram] , ex: [skirt] bigram candidate [unigram] :: [unigram], ex: [long :: skirt] trigram candidate [unigram] :: [bigram], ex: [casual] :: [long skirt] [bigram] :: [unigram] ex: [flower printed] :: [shirt] 拆分成 [Left] :: [Right] 的形式方便我們泛化處理 candidate phrase 接下來利用定義好的 candidate phrase 來計算成詞 score 這裡給出一個最簡單的 score 計算： \\text{score} = \\text{(PMI or MI)} - \\min(Right_{\\text{left_entropy}}, Left_{\\text{right_entropy}}) + \\min(\\text{right_entropy}, \\text{left_entropy}) $\\min(Right_{\\text{left_entropy}}, Left_{\\text{right_entropy}})$ 分別表示，Left side 與 Right side 各自的語境豐富程度，通常取 min 後的的值越大，代表 Left side 或 Right side 有一側傾向與其他詞結合，candidate 越不可能成詞 $\\min(\\text{right_entropy}, \\text{left_entropy})$ 表示 candidate 左右兩側語境豐富成度，越大代表 candidate 越可能成詞 Label Score &amp; Product Score有了以上的 background 是時候來說說產品詞跟標籤詞的特性了，大致上 產品詞在 candidate 會出現在 right side，其左側自由度較高: left_entropy &gt; right_entropy 標籤詞在 candidate 會出現在 left side，其右側自由度較高: right_entropy &gt; left_entropy 顯然只有成詞分數 score 不足以將產品詞和標籤詞分離，所以每個 candidate phrase，會針對 label 跟 product 特性計算 label score 跟 product score。 先上圖： Right Phrase，表 corpus 內出現在 candidate right side 的 phrase 集合 EX : short sleeve right side unigram 集合 Light Phrase，表 corpus 內出現在 candidate left side 的 phrase 集合 計算 Right Phrases 集合內每一個 phase 對 candidate phrase 的 sum of information，代表從所有 right phrases 的角度來看 candidate phrase 的豐富度，值越高代表 candidate 的 right phrases 組成越豐富，其成為 label 的機會越高。 我們希望單個 $\\text{phrase}_i$ 對 candidate phrase $C$ 的 conditional probability $p(\\text{phrase}_i::C|\\text{phrase}_i)$ 不要太高也不要太低，此時算出的 $\\text{information} = - p(\\text{phrase}_i::C|\\text{phrase}_i) \\log p(\\text{phrase}_i::C|\\text{phrase}_i)$ 恰好是最大 \\begin{aligned} I_{L,C} &= \\sum_{phase_i \\in C_{L}} - p(\\text{phrase}_i::C|\\text{phrase}_i) \\log p(\\text{phrase}\\_i::C|\\text{phrase}_i)\\\\ I_{R,C} &= \\sum_{phrase_j \\in C*{R}} - p(C::\\text{phrase}_j|\\text{phrase}_j) \\log p(C::\\text{phrase}_j|\\text{phrase}_j) \\end{aligned} $I_{L,C}$ 表 left phrases 對 candidate 的 sum of information $C_L$ 表 candidate 的 left phrase 集合 有了 left / right phrases information，一個簡單的 label score and product score 計算如下 \\begin{aligned} \\text{label score} &= (\\text{right_entropy - left_entropy}) + (I_{R,C} - I_{L,C}) \\\\ \\text{product score} & = (\\text{left_entropy - right_entropy}) + (I_{L,C} - I_{R,C})\\end{aligned}P.S. 上面分數計算只是提供一個計算思路，實際使用還是得資料分析 EngineeringData Structurecandidate phrase 的需要計算的值有 candidate 的 left entropy and right entropy candidate 的 PMI/MI 中的 joint probability / frequency left component 的 entropy ; right component 的 entropy right phrase information and left phrase information … etc 為了方便計算 candidate 需要兩種 Tries ，一個存 corpus 內所有 sentence 的 prefix tries，另一個存 corpus 內所有 reversed title 的 reversed tries (叫 suffix tries 怕有歧義) 以 title = &quot;Masks Scarf Cashmere Sweater Cap&quot; 為例 首先將 title 所有可能 ngram 取出: 123456789101112131415['Masks', 'Scarf', 'Cashmere', 'Sweater', 'Cap']['Masks', 'Scarf', 'Cashmere', 'Sweater']['Masks', 'Scarf', 'Cashmere']['Masks', 'Scarf']['Masks']['Scarf', 'Cashmere', 'Sweater', 'Cap']['Scarf', 'Cashmere', 'Sweater']['Scarf', 'Cashmere']['Scarf']['Cashmere', 'Sweater', 'Cap']['Cashmere', 'Sweater']['Cashmere']['Sweater', 'Cap']['Sweater']['Cap'] build prefix tries: 123456789101112131415[['Masks'], ['Masks', 'Scarf'], ['Masks', 'Scarf', 'Cashmere'], ['Masks', 'Scarf', 'Cashmere', 'Sweater'], ['Masks', 'Scarf', 'Cashmere', 'Sweater', 'Cap'], ['Scarf'], ['Scarf', 'Cashmere'], ['Scarf', 'Cashmere', 'Sweater'], ['Scarf', 'Cashmere', 'Sweater', 'Cap'], ['Cashmere'], ['Cashmere', 'Sweater'], ['Cashmere', 'Sweater', 'Cap'], ['Sweater'], ['Sweater', 'Cap'], ['Cap']] build reversed tries: 123456789101112131415[['Cap'], ['Cap', 'Sweater'], ['Cap', 'Sweater', 'Cashmere'], ['Cap', 'Sweater', 'Cashmere', 'Scarf'], ['Cap', 'Sweater', 'Cashmere', 'Scarf', 'Masks'], ['Sweater'], ['Sweater', 'Cashmere'], ['Sweater', 'Cashmere', 'Scarf'], ['Sweater', 'Cashmere', 'Scarf', 'Masks'], ['Cashmere'], ['Cashmere', 'Scarf'], ['Cashmere', 'Scarf', 'Masks'], ['Scarf'], ['Scarf', 'Masks'], ['Masks']] 當我們的 candidate phrase = [&quot;Cashmere&quot; :: &quot;Sweater&quot;] 時， 透過 prefix tries 即可找出 [&quot;Cashmere&quot; :: &quot;Sweater&quot;] 右側所有的 phrase node 透過 reversed tries 即可找出 [&quot;Cashmere&quot; :: &quot;Sweater&quot;] 左側所有的 phrase node 啟發式辨識流程 用 entropy 辨識產品詞與標籤詞本質上是 unsupervised learning 的做法，如果以 threshold 卡 score, label score, product score 判斷結果，肯定事倍工半。辨識過程中加入 clustering/grouping 輔助判斷，多迭代幾遍後就能搜集到高度置信的結果。 Grouping把 candidate phrase 當成 data sample $x$ 的話，其包含的特徵有 自身統計類：frequency ，probability … etc 自身 entropy related PMI/MI，NPMI/NMI，left entropy / right entropy left / right component related: PMI/MI ，entropy to left/entropy to right left phrase/right phrase related: sum of information ，deviation，diversity，total frequency，average frequency，total phrase …etc score 類：成詞 score ，label score，product score 挑出 data samples 裡有鑑別度的特徵丟進 cluster 算法中初步分成四群： group A: 獨立成詞 一些用法固定的詞彙 ex : “big code”(這應該是想表示大碼？)，”united state” group B: label 詞 符合 label 詞的特性，右側自由度高 ex: “short sleeved” group C: 右側 product 詞 符合 product 詞的特性，左側自由度高 ex: “lace blouse” group D: 無用詞 沒什意義的詞，本身成詞 score 不高 ex: “lace long” 然後在每個 group 中，分別挑出多個 high confidence and typical data samples ，跟其餘的 data sample 做 KNN/Kmean，來回個幾次做 semi-supervised 。 Exploration隨著置信的 data sample 越多，可以考慮訓練 decision tree，辨識新的 candidate phrases。 也可以利用 word2Vec 強大的相近詞搜索相似的 產品詞/標籤詞 挖掘辨識新的產品詞/標籤詞 這兩個做法建立在手頭上的詞彙已能很好區分出產品詞和標籤詞的情況下，例如用 word2Vec 找相近產品詞有奇效： In: 1w2v_model.wv.most_similar(&quot;flight_jacket&quot;, topn=10) out: 12345678910[('bomber_jacket', 0.8251528739929199), ('flight_suit', 0.8104218244552612), ('coach_jacket', 0.7058290243148804), ('workwear_jacket', 0.7037896513938904), ('jacket', 0.7035773992538452), ('ma_1', 0.6985215544700623), ('baseball_uniform', 0.6333736181259155), ('ma1_pilot', 0.6201080679893494), ('jackets', 0.608674168586731), ('denim_jacket', 0.6048851013183594)] Some Tips candidate phrase proposal：可以透過 TF-IDF, frequency, student-t, PMI 先行召回一批 candidate 再開始辨識 一次處理一種 ngram 辨識過程中加入字典輔助 product words 黑白字典 label words 黑白字典 善用 bigram / trigram 可以由其他 phrase 組合出，可以省去很多計算量 產品詞組成 unigram 產品詞 [unigram], ex： skirt bigram 產品詞 [unigram label] :: [unigram product] , ex: long skirt [unigram] :: [unigram], ex: phone shell (手機殼 …) trigram 產品詞 [bigram label] :: [unigram product], ex: long sleeved blouse [unigram label] :: [bigram product], ex: little black dress 標籤詞組成 unigram 標籤詞 [unigram], ex: slim bigram 標籤詞 [unigram] :: [unigram], ex: v neck, high cut trigram 標籤詞 [unigram label] :: [bigram label], ex: half high collar [bigram label] :: [unigram], ex: deep v collar Reference data mining basing on entropy Language Models – handling unseen sequences &amp; Information Theoryhttps://www3.cs.stonybrook.edu/~ychoi/cse628/lecture/03-ngram.pdf 反作弊基于左右信息熵和互信息的新词挖掘 https://zhuanlan.zhihu.com/p/25499358 基于互信息和左右信息熵的短语提取识别 http://www.hankcs.com/nlp/extraction-and-identification-of-mutual-information-about-the-phrase-based-on-information-entropy.html Normalization entropy Efficiency (normalized entropy) https://en.wikipedia.org/wiki/Entropy_(information_theory)#Efficiency_(normalized_entropy)#Efficiency_(normalized_entropy)) Bouma, G. (2009). Normalized ( Pointwise ) Mutual Information in Collocation Extraction. Proceedings of German Society for Computational Linguistics (GSCL 2009), 31–40. [https://math.stackexchange.com/questions/395121/how-entropy-scales-with-sample-size","link":"/recognize-keywords-by-entorpy/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"recommendation system","slug":"recommendation-system","link":"/tags/recommendation-system/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"word embedding","slug":"word-embedding","link":"/tags/word-embedding/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"推薦系統","slug":"推薦系統","link":"/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"recommendation system","slug":"recommendation-system","link":"/categories/recommendation-system/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"}]}