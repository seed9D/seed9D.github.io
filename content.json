{"pages":[{"title":"about","text":"","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"一步步透視 GBDT Regression Tree","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x) $ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 結論說完了，想看數學原理請移駕 背後的數學，想看例子從無到有生出 GBDT 的請到 Algorithm - step by step ，想離開得請按上一頁。 GBDT 簡介GBDT-regression tree 簡單來說，訓練時依序建立 trees $\\{ f_1(x), f_2(x), …. , f_M(x)\\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。 所以 GBDT - regression tree 的訓練是 sequentially ，無法以並行訓練加速。 我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下: F_m(x) = F_0(x) + \\nu\\sum^m_{i=1}f_i(x) = F_{m-1}(x) + \\nu f_m(x) $\\nu$ 為 learning rate GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。 差多遠即 residual 的概念： \\textit{residual = observed - predicted}因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual GBDT Algorithm - step by stepAlgorithm 參考了 statQuest 對 GBDT 的講解，連結放在 reference，必看！ GBDT-regression tree 擬合 algorithm： Input Data and Loss Function input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data 接下來都用這組簡單的數據，其中 Target $y_i$ : weight which is what we want to predict $x_i$: features 的組成有 身高，喜歡的顏色，性別 目標是用 $x_i$ 的 height, favorite color, gender 來預測 $y_i$ 的 weight loss function 為 square error L(y_i, F(x)) = \\cfrac{1}{2}(\\textit{observed - predicted}) ^2 = \\cfrac{1}{2}(y_i^2 - F(x))^2 square error commonly use in Regression with Gradient Boost $\\textit{observed - predicted}$ is called $residual$ $y_i$ are observed value $F(x)$: the function which give us the predicted value 也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出 F_m(x) = F_{m-1}(x) + \\nu f_m(x)F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x) Step 1 Initialize Model with a Constant Value初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可 取 weight 平均得到 $F_0(x) = 71.2 = \\textit{average weight}$ Step 2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At epoch m=1 (A) Calculate Residuals of $F_{0}(x)$epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals ​ residuals = \\textit{observed weight - predicted weight} 而 $F_0(x) = \\textit{average weight = 71.17}$ 計算 residual 後: epoch_0_prediction 表 $F_0(x)$ 輸出 epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$ (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual 用 columns $\\textit{height, favorite, color, gender}$ 預測 $residuals$ 來建新樹 $f_1(x)$ 建樹的過程為一般的 regression tree building 過程，target 就是 residuals。 假設我們找到分支結構是 綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$ 特定的 leaf node 下。 epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值 直覺地對每個 leaf node 內的 data sample $x$ weight 值取平均，得到輸出值 epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值 (D) Update $F_1(x)$ with New Tree $f_1(x)$此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$ 現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \\textit{learning rate } \\times f_1(x)$ 假設 $\\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下 epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值 At epoch m=2(A) Calculate Residuals of $F_{1}(x)$m = 2 新的 residual between observed weight and $F_1(x)$如下 epoch_1_prediction 為 $F_1(x)$ 的輸出 epoch_2_residual 為 observed weight 與 predicted weight $F_1(x)$ 的 residual (B) Construct New Regression Tree $f_2(x)$建一顆新樹擬合 epoch 2 (A) 得出的 residual epoch_2_residual 為 $f_2(x)$ 要擬合的 target 假設 $f_2(x)$ 擬合後樹結構長這樣 epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_2(x)$決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個 leaf node 下的 data sample $x$ 取平均 epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是 residual epoch_2_residual (D) Update $F_2(x)$ with New Tree $f_2(x)$到目前為止我們建立了兩顆 $tree$ $f_1(x), f_2(x)$，假設 $\\textit{learning rate = 0.1}$，則 $F_2(x)$ 為 F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))每個 data sample 在 $F_2(x)$ 的 predict 值如下圖： weight: out target value epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值 epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual epoch_2_prediction: $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出 Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ Step 3 輸出模型 $F_M(x)$，把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$ 與上一步 predicted value $\\hat{y}$ $F_{m-1}(x)$ 的 $residual$ GBDT Regression 背後的數學Q: Loss function 為什麼用 mean square error ?​ 選擇 $\\cfrac{1}{2}(\\textit{observed - predicted}) ^2$ 當作 loss function 的好處是對 $F(X)$ 微分的形式簡潔 \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\\textit{negative residual}$ ​ $-(y_i - F(X)) = \\textit{-(observed - predicted) = negative residual } $ 而我們知道 $F(X)$ 在 loss function $L(y_i, F(X))$ 的 gradient 就是 $\\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$ Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？問題來自 step 1 我們要找的是能使 cost function $\\sum^n_{i=1}L(y_i, \\gamma)$ 最小的那個輸出值 $\\gamma$ 做為 $F_0(X)$。 $F_0(x) = argmin_r \\sum^n_{i=1} L(y_i,\\gamma)$ $F_0(x)$ 初始化的 function，其值是常數 $\\gamma$ refers to the predicted values $n$ 是 data sample 數 Proof: 已知 ​ \\cfrac{d \\ L(y_i, F(x))}{d \\ F(x)} = \\cfrac{d }{d \\ F(X)} \\ \\cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X)) 所以 cost function 對 $\\gamma $ 微分後，是所有 sample data 跟 $\\gamma$ 的 negative residual 和 為 0 ​ \\sum^n_{i=1}L(y_i, \\gamma) = -(y_1 - \\gamma) - (y_2 - \\gamma)- ... -(y_n - \\gamma) = 0 移項得到 $\\gamma$ ​ \\gamma = \\cfrac{y_1 + y_2 + .... + y_n}{n} 正是所有 target value 的 mean，故得證 ​ F_0(x) = \\gamma = \\textit{the average of targets value} Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？問題來自 step 2A 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum \\cfrac{1}{2}(y_i^2 - F(x))^2我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x)\\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} \\ L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - F_{m-1}(x))) \\\\ &= -(-(observed - predicted )) \\\\ &= - \\textit{negative residual} \\\\ & = residual \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual ​ $f_m(x)$ = $\\textit{gradient decent}$ = $\\textit{negative gradient}$ = $residual$ GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\\textit{gradient decent}$ 的方向得到新的 $F(x)$ 這也正是為什麼叫做 gradient boost 。 by the way，step 2-(A) compute residuals: r_{im} = -[\\cfrac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \\ \\textit{for i = 1,....,n} $i$: sample number $m$: the tree we are trying to build Q：個別 leaf node 的輸出為什麼是取平均 ？問題來自 step 2C 在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\\gamma$ 使 cost function 最小 $\\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma)$ $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合 $F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$ 上的輸出值為 $\\gamma_{j,m}$ \\gamma_{j,m} = argmin_\\gamma \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2直接對 $\\gamma$ 微分 \\begin{aligned}\\cfrac{d}{d \\gamma} \\ \\sum_{x_i \\in R_{j,m}} \\cfrac{1}{2}(y_i - F_{m-1}(x_i) - \\gamma)^2 = \\ \\sum_{x_i \\in R_{j,m}}-(y_i - F_{m-1}(x_i) - \\gamma) = 0\\end{aligned}移項 \\gamma = \\cfrac{1}{n_{jm}} \\sum_{x_i \\in R_{j,m}}y_i - F_{m-1}(x_i) $n_{jm}$ is the number of data sample in leaf node $j$ at step $m$ 白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均 事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $ 都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。 Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？問題來自 step 2D \\begin{aligned}F_m(x) & = F_{m-1}(x) + \\nu f_m(x) \\\\ &= F_{m-1}(x) + \\nu \\sum^{J_m}_{j=1}\\gamma_{jm}I(x \\in R_{jm})\\end{aligned} $F_{m-1}(x) = \\nu\\sum^{m-1}_{i=1} f_i(x)$ $\\nu$ learning rate $J_m$ m 步 的 leaf node 總數 $\\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出 $F_m(x)$ 展開來就是 F_m(x) = F_0(x) + \\nu(f_1(x) + f_2(x)+ ...+f_m(t))寫在最後 seeing is believing 太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙 人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶 learning by doing it 所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的 1234567891011import pandas as pddata = [ (1.6, 'Blue', 'Male', 88), (1.6, 'Greem', 'Female', 76), (1.5, 'Blue', 'Female', 56), (1.8, 'Red', 'Male', 73), (1.5, 'Green', 'Male', 77), (1.4, 'Blue', 'Female', 57) ]columns = ['height', 'favorite_color', 'gender', 'weight']df = pd.DataFrame.from_records(data, index=None, columns=columns) always get your hands dirty ReferenceMainGradient Boost Part 1 (of 4): Regression Main Ideas Gradient Boost Part 2 (of 4): Regression Details ccd comment: 上面兩個必看 A Step by Step Gradient Boosting Decision Tree Example https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/ 真 step by step Gradient Boosting Decision Tree Algorithm Explained https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4 including sklearn 實作 Other Gradient Boosting Decision Tree http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法 提升树算法理论详解 https://hexinlin.top/2020/03/01/GBDT/ 梯度提升树(GBDT)原理小结 https://www.cnblogs.com/pinard/p/6140514.html","link":"/GBDT-Rregression-Tree-Step-by-Step/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/ckk9zuhsn0003k0e2aler1a0b/"},{"title":"一步步透視 GBDT Classifier","text":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) = F_0(x) + \\nu\\sum^M_{i=1}f_i(x)$ 訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}$ 的方向 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ GBDT classifier 常用的 loss function 為 cross entropy classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$ 跟 $probability$ 有關，得將 $F(x)$ 通過 $\\textit{sigmoid function }$ 獲得 probability $p = \\sigma(F(x))$ GBDT 簡介在 一步步透視 GBDT Regression Tree 直接進入正題吧 GBDT Algorithm - step by stepGBDT classification tree algorithm 跟 regression tree 並無不同 Input Dat and Loss Function Input Data $\\{(x_i, y_i)\\}^n_{i=1}$ and a differentiable Loss Function $L(y_i, F(x))$ Data target $y_i$: who loves Troll2 features of $x_i$: “likes popcorn”, “Age”, “favorite” Our goal is using $x_i$ to predict someone like Trolls 2 or not loss function 為 cross entropy \\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​值得注意的是，GBDT - classifier $F(x)$ 輸出的是 $log(odds)$ 而不是 $probability$ 要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\\textit{sigmoide function}$ \\textit{The probability of Loving Troll 2 } = \\sigma(F(x)) = p $log(odds)$ 轉換成 $probability$ 公式 p = \\cfrac{\\exp^{log(odds)}}{1 + exp^{log(odds)}} Step 1 Initial model with a constant value $F_0(X)$ 初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可 計算完，得到 $F_0(x) = 0.69$，每個 data point 的初始 prediction 都一樣就是 $F_0(x)$。 $F_0(x)$ 是 $\\log(odds)$ 若要計算 probability of loving Troll 2 呢？ $\\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。 ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$ ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving Troll 2 Step2For $m=1$ to $M$，重複做以下的事 (A) calculate residuals of $F_{m-1}(x)$ (B) construct new regression tree $f_m(x)$ (C) compute each leaf node’s output of new tree $f_m(x)$ (D) update $F_m(x)$ with new tree $f_m(x)$ At Epoch m = 1(A) Calculate Residuals of $F_{0}(x)$classification 問題中 residual 為 predicted probability 與 observed label $y$ 之間的差距 $residual = observed - \\textit{predicted probability}$ true label 為 1 false label 為 0 注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$ 輸出的是 $log(odds)$ 計算各 data sample 的 residual 後： ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$ ep_0_prob 表 $F_0(x)$ predicted probability，$\\sigma(F_0(x))$ ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual (B) Construct New Regression Tree $f_1(x)$此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$ 建樹為一般 fit regression tree 的過程，criterion 為 mean square error，假設找到的樹結構為 可以看到綠色為 leaf node，所有的 data sample $x$ 都被歸到特定 leaf node 下 ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index (C) Compute Each Leaf Node’s Output of New Tree $f_1(x)$現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下 \\cfrac{\\sum residual_i}{\\sum [\\textit{previous probability} \\times \\textit{(1 - previous probability)}]} 分子是 each leaf node 下的 data sample $x$ 的 residual 和 分母的 previous probability 為 $m -1$ 步 GBDT 輸出的 probability $p = \\sigma(F(x))$ 。在這個 epoch 是指 $F_0(x)$ 經過計算後，每個 leaf node 輸出 ep_0_prob 表 $\\sigma(F_0(x))$ 計算出的 probability of loving Troll2 ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob ep_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值 (D) update $F_1(x)$ with new tree $f_1(x)$現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 F_1(x) = F_0(x) + \\textit{learning rate} \\times f_1(x)輸出的 probability 為 $\\sigma(F_1(x))$ 令 $\\textit{learnign rate = 0.8}$，得到 epoch 2 每個 data sample 的 $\\log(odds)$ prediction 與 probability prediction ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$ ep_1_prob 為 $F_1(x)$ 輸出的 probability $\\sigma(F_1(x))$ At Epoch m = 2(A) Calculate Residuals of $F_1(x)$計算上一步 $\\textit{residual of } F_1(X)$ residual = observed - \\textit{predicted probability} ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\\sigma(F_1(x))$ ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$ (B) Construct New Regression Tree $f_2(x)$用 data sample x 的 columns “like_popcor”, “age”, “favorite_color” 擬合 ep_2_residual build a new tree $f_2(x)$ 假設得到 $f_2(x)$ 的樹結構： 每個 data sample 對應的 leaf index ep_2_leaf_index 表 data sample 對應到 $f_2(x)$ 上的 leaf node index (D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$計算 $f_2(x)$ 下每個 leaf node 的輸出: 對應到 data sample 上: ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node 下會有一樣的值 Update $F_2(x)$ with New Tree $f_2(x)$到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為 $F_2(x) = F_0(x) + \\nu(f_1(x) + f_2(x))$ $\\nu$ 為 learning rate，假設為 0.8 GBDT 輸出的 probability 為 $\\sigma(F_2(x))$，計算 epoch 2 的 prediction of probability of loving troll2: love_toll2: our target ep_0_pre 表 $F_0(x)$ ep_1_leaf_output 表 data sample x​ 在第一顆樹 $f_1(x)$ 的輸出值 ep_2_leaf_output 表 data sample x 在第二顆樹 $f_2(x)$ 的輸出值 ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$ ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\\sigma(F_2(x))$ Step 3 Output GBDT fitted model Output GBDT fitted model $F_M(x)$ 把 $\\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$ F_M(x) = F_0(x) + \\nu\\sum^{M}_{m=1}f_m(x) $F_M(x)$ 的每棵樹 $f_m(x)$ 都是去 fit $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$ residual = observed - \\textit{predicted probability}所以 $F_m(x)$ 又可以寫成 F_m(x) = F_{m-1}(x) + \\nu f_m(x)這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住： $F_m(x)$ 輸出 $log(odds)$ $residual$ 的計算與 probability 有關 GBDT Classifier 背後的數學Q: 為什麼用 cross entropy 做為 loss function ?在分類問題上，我們預測的是 $\\textit{The probability of loving Troll 2}$ $P(Y|x)$，$\\textit{}$ 以 $maximize$ $\\textit{log likelihood}$ 來解 $P(Y|x)$。 令 GBDT - classification tree 的 probability prediction 為 $P(Y| x) = \\sigma(F(x))$，則 objective function 為 \\textit{log (likelihood of the obersved data given the prediction) } \\\\= \\sum_{i=1}^N [y_i log(p) + (1-y_i)log(1-p)] $p = P(Y=1|x)$，表 the probability of loving movie Troll 2 $y_i$ : observation of data sample $x_i$ loving Troll 2 or not $y \\in \\{1, 0\\}$ 而 $\\textit{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成 \\textit{objective function} = - \\sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)所以 $\\textit{loss function} = -[y log(p) + (1-y)log(1-p)]$ 把 loss function 用 $odds$ 表示： \\begin{aligned} -[y log(p) + (1-y)log(1-p)] & = -ylog(p)-(1-y)log(1-p) \\\\ &= -ylog(p)-log(1-p) + ylog(1-p) \\\\ &= -y[log(p) - log(1-p)] - log(1-p) \\\\ & = -ylog(odds) - log(1-p) \\\\ &= -ylog(odds) + log(1 + \\exp^{log(odds)}) \\end{aligned} 第三個等號 到 第四個等號用到 $odds=\\cfrac{p}{1-p}$ 第四個等號 到 第五個等號用到 $p = \\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}$ 這個結論 $log(1-p) = log(1- \\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}) = log(\\cfrac{1}{1 + \\exp^{log(odds)}}) = -log(1 + \\exp^{log(odds)})$ 把 loss function 表示成 odds 的好處是， $-ylog(odds) + log(1 + \\exp^{log(odds)})$ 對 $log(odds)$ 微分形式很簡潔 \\cfrac{d}{d \\ log(odds)} -ylog(odds) + log(1 + \\exp^{log(odds)}) = -y -\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}} = -y + ploss function 對 $log(odds)$ 的微分，既可以以 $log(odds)$ 表示，也可以以 probability $p$ 表示 以 $log(odds)$ 表示： $\\cfrac{d}{d log(odds)}L(y_i, p) = -y -\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}$ 以 $p$ 表示：$\\cfrac{d}{d log(odds)}L(y_i, p) = -y + p$ 用 $p$ 表示時，loss function 對 $log(odds)$ 的微分 -y + p = \\textit{ -(observed - predicted) = negative residual}Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\\cfrac{count(true)}{count(false)})$ ? 來自 Step 1 的問題 根據選定的 loss function \\textit{loss function} = -[y log(p) + (1-y)log(1-p)] $P(Y=1|x) = p$ 為出現正類的 probability $y \\in \\{1, 0\\}$ 將 loss function 以 $log(odds)$ 表示 -[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \\exp^{log(odds)})]$F_0(x)$ 為能使 $\\textit{cost function}$ 最小的 $log(odds): \\gamma$ F_0(x) = argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma) = argmin_\\gamma \\sum^n_{i=1} -[y_ilog(odds) + log(1 + \\exp^{log(odds)})] $n$ 為 number of data sample $x$ 令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$ cost function 對 $log(odds)$ 微分取極值： \\begin{aligned}& \\cfrac{d}{d log(odds)}\\sum^n_{i=1} -[y_ilog(odds) + log(1 + \\exp^{log(odds)})] \\\\ & = \\cfrac{d}{d log(odds)}\\sum^{n^{(1)}}_i -(log(odds) + log(1 + exp^{log(odds)})) - \\sum^{n^{(0)}}_j (0 * log(odds) + log(1 + \\exp^{log(odds)})) \\\\& = \\cfrac{d}{dlog(odds)} -n^{(1)} \\times (log(odds) + log(1 + exp^{log(odds)})) - n^{(0)} \\times log(1 + \\exp^{log(odds)}) \\\\ & =0\\end{aligned} \\begin{aligned} & n^{(1)} \\times(-1 + \\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}) + n^{(0)} \\times(\\cfrac{\\exp^{log(odds)}}{1 + \\exp^{log(odds)}}) \\\\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\\\ & = - n^{(1)} + n p \\\\ &= 0 \\end{aligned}移項得到 $p$ p = \\cfrac{n^{(1)}}{n} log(odds) = \\cfrac{p}{1-p} = \\cfrac{n^{(1)}}{n^{(0)}}故得證，給定 $\\textit{loss function } = -[y log(p) + (1-y)log(1-p)]$， 能使 $argmin_{\\gamma}\\sum^n_{i=1} L(y_i, \\gamma)$ 的 $\\gamma$ 為 log(odds)= \\cfrac{n^{(1)}}{n^{(0)}} \\therefore F_0(x) = \\cfrac{n^{(1)}}{n^{(0)}}Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？ 問題來自 Step 2 - (A) 在 $m$ 步的一開始還沒建 新 tree $f_m(x)$ 時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 \\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \\sum -[y log(p) + (1-y)log(1-p)] $y$ 為 target label $p = P(Y=1|x)$ 表正類的 probability 注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy 我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$ 的 cost function $\\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？ 觀察 $F_m(x) = F_{m-1}(x) + \\nu f_m(x)$，是不是很像 gradient decent update 的公式 F_m(x) = F_{m-1}(x) + \\nu f_m(x) \\hAar F_m(x) = F_{m-1}(x) - \\hat{\\nu} \\cfrac{d}{d \\ F(x)} L(y, F(x))讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是 \\begin{aligned}f_m(x) &= - \\cfrac{d}{d \\ F_{m-1}(x)} \\ L(y, F_{m-1}(x)) \\\\ &= -(-(y - p)) \\\\ &= -(-(observed - \\textit{predict probability})) \\\\ &= - \\textit{negative residual} \\\\ & = residual \\end{aligned}新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual Q: leaf node 的輸出公式怎麼來的？ 問題來自 Step 2-(C) 在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\\gamma_{jm}$，使的 cost function 最小 \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} L(y_i, F_{m-1}(x_i) + \\gamma) \\\\ &= argmin_\\gamma \\sum_{x_i \\in R_{j, m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + \\exp^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $j$ 表 leaf node index $m$ 表第 $m$ 步 $\\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值 $R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合 將 loss function 以 $log(odds)$ 表示後的 objective function \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + \\exp^{F_{m-1}(x_i) + \\gamma }) \\end{aligned} $-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \\exp^{log(odds)})]$ $F_{m-1}(x)$ 輸出為 $log(odds)$ cost function 對 $\\gamma$ 微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近 loss function 處理 \\begin{aligned}\\gamma_{j,m} &= argmin_\\gamma \\sum_{x_i \\in R_{j,m}} -y_i \\times [F_{m-1}(x_i) + \\gamma] + log(1 + \\exp^{F_{m-1}(x_i) + \\gamma }) \\end{aligned}讓 2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \\gamma)$ 在 $F_{m-1}(x)$ 處展開 L(y_i, F_{m-1}(x_i) + \\gamma) \\approx L(y_i, F_{m-1}(x_i) ) + \\cfrac{d}{d (F_{m-1}(x))} L(y_i, F_{m-1}(x_i))\\gamma + \\cfrac{1}{2} \\cfrac{d^2}{d (F_{m-1}(x) )^2}L(y_i, F_{m-1}(x_i))\\gamma^2將 cost function 對 $\\gamma$ 微分取極值，求 $\\gamma_{j,m}$ \\sum_{x_i \\in R_{jm}} \\cfrac{d}{d\\gamma} L(y_i, F_{m-1}(x_i), \\gamma) \\approx \\sum_{x_i \\in R_{jm}} (\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\\gamma) = 0移項得到 $\\gamma$ \\gamma = \\cfrac{\\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))}分子是 derivative of Loss function ; 分母是 second derivative of loss function 分子部分: \\begin{aligned} & \\sum_{x_i \\in R_{jm} }-\\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\\\& = \\sum \\cfrac{d}{d(F_{m-1}(x_i))} \\ y_i \\times [F_{m-1}(x_i) ] - log(1 + \\exp^{F_{m-1}(x_i) }) \\\\ &= \\sum (y_i - \\cfrac{\\exp^{^{F_{m-1}(x_i) }}}{1 + \\exp^{^{F_{m-1}(x_i) }}} ）\\\\& = \\sum_{x_i \\in R_{jm}} (y_i -p_i) \\end{aligned} $F_{m-1}(x_i)$ 是 $m-1$ 步時 $classifier$ 輸出的 $log(odds)$ 分子部分為 $\\textit{summation of residual}$ 分母部分 \\begin{aligned}& \\sum_{x_i \\in R_{jm} }\\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\\\ & = \\sum_{x_i \\in R_{jm} } \\cfrac{d^2}{d \\, \\ (F_{m-1}(x_i))^2} \\, -[y_i \\times F_{m-1}(x_i) - log(1 + \\exp^{F_{m-1}(x_i)})] \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)}-[y_i - \\cfrac{\\exp^{^{F_{m-1}(x_i) }}}{1 + \\exp^{^{F_{m-1}(x_i) }}}] \\\\ & =\\sum_{x_i \\in R_{jm} } \\cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \\exp^{F_{m-1}(x_i)})^{-1} \\times \\exp^{F_{m-1}(x_i)}] \\\\ & = \\sum_{x_i \\in R_{jm} }-[(1 + \\exp^{F_{m-1}(x_i)})^{-2} \\exp^{F_{m-1}(x_i)}\\times \\exp^{F_{m-1}(x_i)} - (1+ \\exp^{F_{m-1}(x_i)})^{-1} \\times \\exp^{F_{m-1}(x_i)} ] \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} \\quad + \\quad \\cfrac{\\exp^{F_{m-1}(x_i)}}{1+ exp^{F_{m-1}(x_i)}} = \\sum_{x_i \\in R_{jm} }\\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} \\ + \\ \\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} \\times \\cfrac{(1 + \\exp^{F_{m-1}(x_i)})}{1 + \\exp^{F_{m-1}(x_i)}} \\\\&= \\sum_{x_i \\in R_{jm} } \\cfrac{-\\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} + \\cfrac{\\exp^{F_{m-1}(x_i)} + \\exp^{2 * F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} = \\sum_{x_i \\in R_{jm} }\\cfrac{\\exp^{F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})^2} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp^{F_{m-1}(x_i)}}{(1 + \\exp^{F_{m-1}(x_i)})} \\times \\cfrac{1}{(1 + \\exp^{F_{m-1}(x_i)})} \\\\ &= \\sum_{x_i \\in R_{jm} } \\cfrac{\\exp^{log(odds)_i}}{1 + \\exp^{log(odds)_i}} \\times \\cfrac{1}{1 + \\exp^{log(odds)_i}} \\\\ &= \\sum_{x_i \\in R_{jm} } p_i \\times (1-p_i) \\end{aligned}綜合分子分母，能使 $F_m(x)$ cost function 最小化的 tree $f_m(x)$ 第 $j$ 個 leaf node 輸出為 \\gamma_{jm}= \\cfrac{\\sum_{x_i \\in R_{jm})} (y_i - p_i)}{\\sum_{x_i \\in R_{jm} }(p_i \\times (1- p_i))} = \\cfrac{\\textit{summation of residuals }}{\\textit{summantion of (previous probability $\\times$ (1 - previoous probability))}}寫在最後Data Sample learning by doing it 1234567891011import pandas as pddata = [ (True, 12, 'blue', True), (True, 87, 'gree', True), (False, 44, 'blue', False), (True, 19, 'red', False), (False, 32, 'green', True), (False, 14, 'blue', True)]columns = ['like_popcorn', 'age', 'favorite_color', 'love_troll2']target = 'love_troll2' ReferenceGradient Boost Part 3 (of 4): Classification Gradient Boost Part 4 (of 4): Classification Details Gradient Boosting In Classification: Not a Black Box Anymore! https://blog.paperspace.com/gradient-boosting-for-classification/ statquest 整理 StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s The Logit and Sigmoid Functions https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/ Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979","link":"/GBDT-Classifier-step-by-step/"}],"tags":[{"name":"ML","slug":"ML","link":"/tags/ML/"}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"}]}