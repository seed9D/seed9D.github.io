<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>seed9D&#39;s blog</title>
  
  
  <link href="https://seed9d.github.io/atom.xml" rel="self"/>
  
  <link href="https://seed9d.github.io/"/>
  <updated>2021-02-21T10:09:04.000Z</updated>
  <id>https://seed9d.github.io/</id>
  
  <author>
    <name>seed9D</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>實作 wide &amp; deep 從訓練到推薦排序</title>
    <link href="https://seed9d.github.io/wide-deep-in-recommendation/"/>
    <id>https://seed9d.github.io/wide-deep-in-recommendation/</id>
    <published>2021-02-20T15:29:06.000Z</published>
    <updated>2021-02-21T10:09:04.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Memorization-amp-Generalization&quot;&gt;&lt;a href=&quot;#Memorization-amp-Generalization&quot; class=&quot;headerlink&quot; title=&quot;Memorization &amp;amp; Generalization&quot;&gt;&lt;/a&gt;Memorization &amp;amp; Generalization&lt;/h1&gt;&lt;p&gt;在推薦系統中，如果要選第一個深度排序模型從傳統機器學習接軌到 DNN，那首推 google 在 2016 年提出的  Wide &amp;amp; Deep ，Wide &amp;amp; Deep 名稱來自其由一個 shallow 的 model 與一個 deep  的  network 組合而成。&lt;/p&gt;
&lt;p&gt;在進入到 DNN 前，我們手上肯定有個正在線上運行的傳統排序模型 ex: FM, GBDT …，還有積累了很久的有效特徵組合，如果直接進入 DNN，這些經驗和積累打水漂不說，還得花很長的時間 tune 模型。&lt;/p&gt;
&lt;p&gt;有了 Wide &amp;amp; Deep，我們只需將正在 serving 的那個排序模型的特徵放在 wide side 使用; 另外在建一個  deep side 的深度模型，兩個模型 joint training  即可無縫接軌到 DNN。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ZlRSMGf.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;那 wide 跟  deep network 分別代表什麼呢 ？&lt;/p&gt;
&lt;p&gt;Google 在 2016 的論文中總結出：Wide 負責 memorization , Deep 負責 Generalization , Wide &amp;amp; Deep 是 Memorization &amp;amp; Generalization 的體現，實際上也是推薦領域中經典的 Exploitation &amp;amp; Exploration  問題。&lt;/p&gt;</summary>
    
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/categories/recommendation-system/"/>
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/tags/recommendation-system/"/>
    
    <category term="pytorch" scheme="https://seed9d.github.io/tags/pytorch/"/>
    
    <category term="推薦系統" scheme="https://seed9d.github.io/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/"/>
    
    <category term="deep learning" scheme="https://seed9d.github.io/tags/deep-learning/"/>
    
    <category term="ranking" scheme="https://seed9d.github.io/tags/ranking/"/>
    
  </entry>
  
  <entry>
    <title>實時推薦策略流程</title>
    <link href="https://seed9d.github.io/realtime-recommendataion-system-architecture/"/>
    <id>https://seed9d.github.io/realtime-recommendataion-system-architecture/</id>
    <published>2021-02-19T15:52:12.000Z</published>
    <updated>2021-02-19T16:13:03.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;一個完整的工業級的推薦系統會分成兩大部分&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Online serving&lt;/li&gt;
&lt;li&gt;Offline data processing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;此篇著重在電商推薦系統在 online serving 的算法策略架構&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;架構概述&quot;&gt;&lt;a href=&quot;#架構概述&quot; class=&quot;headerlink&quot; title=&quot;架構概述&quot;&gt;&lt;/a&gt;架構概述&lt;/h1&gt;&lt;p&gt;Online serving 為當 user request 進來時算法的策略流程，什麼階段該做什麼事。一般來說 online serving 又可大致上分成三個 stages&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Match： 從百萬級的商品庫中召回數千或數百的商品 &lt;ul&gt;
&lt;li&gt;此階段大致上, 模糊地召回 user 感興趣的商品&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rank：對召回的數千或數百商品進行打分，決定針對 user 個性化排序召回的商品&lt;ul&gt;
&lt;li&gt;通常會用模型 ranking，模型會綜合考慮 user 的喜好以及商品本身的特徵進行打分&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Re rank：根據業務邏輯重新排序 ranking 結果&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/xn0ymDY.png&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;上面三個 stages 是一個推薦流程的抽象表示，實際上每個 stage 內還會細分出工程和業務考量的模組。&lt;/p&gt;</summary>
    
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/categories/recommendation-system/"/>
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/tags/recommendation-system/"/>
    
    <category term="推薦系統" scheme="https://seed9d.github.io/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/"/>
    
  </entry>
  
  <entry>
    <title>推薦系統中的瑞士刀 Factorization Machine</title>
    <link href="https://seed9d.github.io/pratical-FM-model/"/>
    <id>https://seed9d.github.io/pratical-FM-model/</id>
    <published>2021-02-18T16:00:42.000Z</published>
    <updated>2021-03-02T16:07:50.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;多才多藝的-Factorization-Machine&quot;&gt;&lt;a href=&quot;#多才多藝的-Factorization-Machine&quot; class=&quot;headerlink&quot; title=&quot;多才多藝的 Factorization Machine&quot;&gt;&lt;/a&gt;多才多藝的 Factorization Machine&lt;/h1&gt;&lt;p&gt;在推薦領域，現今各種 DNN 模型當道，有個不怎麼深的模型卻小巧而美，既能做召回又能做排序，計算複雜度上又沒 DNN 耗時又耗錢&lt;/p&gt;
&lt;p&gt;他 就是現今各種應用在推薦系統 DNN 的前輩 Factorization Machine (FM)，這麼多才多藝的模型你值得擁有。&lt;/p&gt;
&lt;p&gt;我們都知道 DNN 的本質是特徵的高階交叉，這點在 FM 上就能初見端倪：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(x) = w_0 + \sum^n_{i=1}w_ix_i + \sum^{n-1}_{i=1}\sum^n_{j=i+1}&lt;v_i, v_j&gt;x_ix_j \tag{1}&lt;/script&gt;&lt;p&gt;FM 前兩項為 LR:   $w_0 + \sum^n_{i=1}w_ix_i$，末項為二階特徵交叉項 &lt;script type=&quot;math/tex&quot;&gt;\sum^{n-1}_{i=1} \sum^n_{j=i+1}&lt;v_i, v_j&gt;x_ix_j&lt;/script&gt;，這形式像極了 wide &amp;amp; deep，根本是 wide &amp;amp; deep  的前身，linear 項負責  memorization; cross 項負責 generalization。&lt;/p&gt;
&lt;p&gt;FM 既能做召回又能做排序，甚至能在多路召回後對上萬個 entity 粗排打分，到底是怎麼做到的呢？&lt;/p&gt;
&lt;p&gt;讓我們往下繼續看下去&lt;/p&gt;</summary>
    
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/categories/recommendation-system/"/>
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/tags/recommendation-system/"/>
    
    <category term="推薦系統" scheme="https://seed9d.github.io/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/"/>
    
  </entry>
  
  <entry>
    <title>透視 XGBoost(0) 總結篇</title>
    <link href="https://seed9d.github.io/what-make-XGBoost-so-effective/"/>
    <id>https://seed9d.github.io/what-make-XGBoost-so-effective/</id>
    <published>2021-02-17T15:20:35.000Z</published>
    <updated>2021-02-17T16:09:55.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;此篇總結 透視 XGBoost 系列，建議系列閱讀順序為&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;/xgboost-for-regression/&quot; title=&quot;透視 XGBoost(1) 圖解 Regression&quot;&gt;透視 XGBoost(1) 圖解 Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/XGBoost-for-classification/&quot; title=&quot;透視 XGBoost(2) 圖解 Classification&quot;&gt;透視 XGBoost(2) 圖解 Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/XGBoost-General-Objective-Function/&quot; title=&quot;透視 XGBoost(3) 蘋果樹下的 objective function&quot;&gt;透視 XGBoost(3) 蘋果樹下的 objective function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/XGBoost-cool-optimization/&quot; title=&quot;透視 XGBoost(4) 神奇 optimization 在哪裡？&quot;&gt;透視 XGBoost(4) 神奇 optimization 在哪裡？&lt;/a&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://seed9d.github.io/categories/Machine-Learning/"/>
    
    
    <category term="ML" scheme="https://seed9d.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>透視 XGBoost(4) 神奇 optimization 在哪裡？</title>
    <link href="https://seed9d.github.io/XGBoost-cool-optimization/"/>
    <id>https://seed9d.github.io/XGBoost-cool-optimization/</id>
    <published>2021-02-17T02:00:16.000Z</published>
    <updated>2021-02-17T16:08:46.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;XGBoost 不只在算法上的改造，真正讓 XGBoost  大放異彩的是與工程優化的結合，這讓 XGBoost 在工業上可以應用於大規模數據的處理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approximate Greedy Algorithm&lt;/li&gt;
&lt;li&gt;Weighted Quantile Sketch &amp;amp; Parallel Learning&lt;/li&gt;
&lt;li&gt;Sparsity-Aware Split Finding&lt;/li&gt;
&lt;li&gt;System Design&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Approximate-Greedy-Algorithm&quot;&gt;&lt;a href=&quot;#Approximate-Greedy-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;Approximate Greedy Algorithm&quot;&gt;&lt;/a&gt;Approximate Greedy Algorithm&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;XGBoost 有兩種 split finding  策略，Exact Greedy and Approximate Greedy&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;data sample 量小的情況下， XGB tree 在建立分支時，會逐一掃過每個可能分裂點 (已對特徵值排序) 計算 similarity score  跟 gain， 即是 greedy  algorithm。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/UKj8mpu.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;greedy algorithm 會窮舉所有可能分裂點，因此能達到最準確的結果，因為他把所有可能的結果都看過一遍後選了一個最佳的。&lt;/p&gt;
&lt;p&gt;但如果 data sample  不僅數量多 (row)，特徵維度也多 (columns)，採用 greedy algorithm，雖然準確卻也沒效率。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://seed9d.github.io/categories/Machine-Learning/"/>
    
    
    <category term="ML" scheme="https://seed9d.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>透視 XGBoost(3) 蘋果樹下的 objective function</title>
    <link href="https://seed9d.github.io/XGBoost-General-Objective-Function/"/>
    <id>https://seed9d.github.io/XGBoost-General-Objective-Function/</id>
    <published>2021-02-16T16:17:27.000Z</published>
    <updated>2021-03-05T13:39:04.000Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;h1 id=&quot;XGBoost-General-Objective-Function&quot;&gt;&lt;a href=&quot;#XGBoost-General-Objective-Function&quot; class=&quot;headerlink&quot; title=&quot;XGBoost General</summary>
        
      
    
    
    
    <category term="Machine Learning" scheme="https://seed9d.github.io/categories/Machine-Learning/"/>
    
    
    <category term="ML" scheme="https://seed9d.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>透視 XGBoost(2) 圖解 Classification</title>
    <link href="https://seed9d.github.io/XGBoost-for-classification/"/>
    <id>https://seed9d.github.io/XGBoost-for-classification/</id>
    <published>2021-02-16T15:48:04.000Z</published>
    <updated>2021-02-17T16:08:57.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;XGBoost 與  GBDT 相比，同樣都是擬合上一步 $F_{m-1}(x)$ 預測值的 residual 。不同的是 XGBoost 在擬合 residual 時更為聰明有效，他有自己衡量分裂增益 gain 的方式去擬合  residual 建立  XGB tree $f_m(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gain = Left_{\text{similarity}} + Right_{\text{similarity}}  - Root_{\text{similarity}} \tag{1}&lt;/script&gt;&lt;ul&gt;
&lt;li&gt;分裂增益 gain ，的背後原理可以透過 XGBoost  通用 objective function 並填入 classification/regression/rank/etc.. 各自的 loss function  後推導出的 $\text{first derivative of loss function} = g_i \quad \text{and} \quad \text{second derivative of loss function} = h_i$&lt;/li&gt;
&lt;li&gt;XGBoost 的 通用 objective function 由兩部份組成&lt;ul&gt;
&lt;li&gt;first part is loss function，根據不同任務有不同的 loss function &lt;/li&gt;
&lt;li&gt;second part is regularization term 防止 overfitting，限制 leaf node 輸出值大小 和 leaf nodes 的總數&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://seed9d.github.io/categories/Machine-Learning/"/>
    
    
    <category term="ML" scheme="https://seed9d.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>透視 XGBoost(1) 圖解 Regression</title>
    <link href="https://seed9d.github.io/xgboost-for-regression/"/>
    <id>https://seed9d.github.io/xgboost-for-regression/</id>
    <published>2021-02-16T15:32:16.000Z</published>
    <updated>2021-02-17T16:09:03.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/aaIKrZN.png&quot; alt=&quot;XGBoost&quot;&gt;&lt;/p&gt;
&lt;p&gt;XGboost 是 gradient boosting machine 的一種實作方式，xgboost 也是建一顆新樹 $f_m(x)$ 去擬合上一步模型輸出 $F_{m-1}(x)$ 的 $\text{residual}$&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
F_m(x) &amp; = F_{m-1}(x) + f_m(x)  
\\
F_{m-1}(x) &amp;= F_0(x) + \sum_{i=0}^{m-1}f_{m-1}(x)
\end{aligned}&lt;/script&gt;&lt;p&gt;不同的是，Xgboost 做了大量運算和擬合的優化，這讓他比起傳統 GBDT 更為高效率與有效&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://seed9d.github.io/categories/Machine-Learning/"/>
    
    
    <category term="ML" scheme="https://seed9d.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>內容推薦 (3):主題卡片推薦</title>
    <link href="https://seed9d.github.io/topic-recommendation/"/>
    <id>https://seed9d.github.io/topic-recommendation/</id>
    <published>2021-02-10T15:35:59.000Z</published>
    <updated>2021-02-10T15:53:06.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;什麼是主題卡片推薦？&quot;&gt;&lt;a href=&quot;#什麼是主題卡片推薦？&quot; class=&quot;headerlink&quot; title=&quot;什麼是主題卡片推薦？&quot;&gt;&lt;/a&gt;什麼是主題卡片推薦？&lt;/h1&gt;&lt;p&gt;主題推薦，就是給定一個概念，然後推薦系統圍繞著這個主題將將相關聯的商品推薦給用戶。&lt;/p&gt;
&lt;p&gt;“主題” 是個超越類目, 產品詞或標籤的存在，他可以是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;旅行出行&lt;/li&gt;
&lt;li&gt;文青&lt;/li&gt;
&lt;li&gt;寬鬆顯瘦&lt;/li&gt;
&lt;li&gt;愛車人&lt;/li&gt;
&lt;li&gt;追星族&lt;/li&gt;
&lt;li&gt;愛美&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;等於是把商品池中的類目關係解構又重構出一個新的子集。&lt;/p&gt;
&lt;p&gt;以用戶端接觸的 UI 來看，主題卡片推薦由兩個構成： Feed 流卡片推薦與 Landing Page 主題推薦。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/7NNjttp.png&quot; style=&quot;zoom: 33%;&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/categories/recommendation-system/"/>
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/tags/recommendation-system/"/>
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="推薦系統" scheme="https://seed9d.github.io/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/"/>
    
  </entry>
  
  <entry>
    <title>內容推薦 (2) Title Embedding with Keyword</title>
    <link href="https://seed9d.github.io/title-embedding-with-keywords/"/>
    <id>https://seed9d.github.io/title-embedding-with-keywords/</id>
    <published>2021-02-09T15:49:29.000Z</published>
    <updated>2021-02-10T15:36:50.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;在前篇 &lt;a href=&quot;/recognize-keywords-by-entorpy/&quot; title=&quot;內容推薦 (1) 關鍵詞識別&quot;&gt;內容推薦 (1) 關鍵詞識別&lt;/a&gt; 中，我們利用 entropy 從商品池的 title  中辨識出 product word  &amp;amp; label word  &lt;/p&gt;
&lt;p&gt;此篇，我們將利用已經辨識出的 product word &amp;amp; label word 回頭對商品池中的商品 title 做 embedding&lt;/p&gt;
&lt;p&gt;當然你也可以直接將所有 title 送進 Word2Vec 硬 train 一發，然後對 title 內的所有的 word vectors 取平均得到 title vector。&lt;/p&gt;</summary>
    
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/categories/recommendation-system/"/>
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/tags/recommendation-system/"/>
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="推薦系統" scheme="https://seed9d.github.io/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/"/>
    
  </entry>
  
  <entry>
    <title>內容推薦 (1) 關鍵詞識別</title>
    <link href="https://seed9d.github.io/recognize-keywords-by-entorpy/"/>
    <id>https://seed9d.github.io/recognize-keywords-by-entorpy/</id>
    <published>2021-02-09T15:18:55.000Z</published>
    <updated>2021-02-10T15:36:59.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;h2 id=&quot;從內容召回說起&quot;&gt;&lt;a href=&quot;#從內容召回說起&quot; class=&quot;headerlink&quot; title=&quot;從內容召回說起&quot;&gt;&lt;/a&gt;從內容召回說起&lt;/h2&gt;&lt;p&gt;電商推薦系統多路召回中，通常會有一路召回商品叫內容召回 $\text{content I2I}$&lt;/p&gt;
&lt;p&gt;content I2I 與其他根據用戶行為的召回不同， 最簡單的 content I2I 可以根據 A 商品 title 內容找出其他與 A 相似的 title 的商品，這給了他很強的推薦解釋性&lt;/p&gt;
&lt;p&gt;對於商品冷啟動或者系統冷啟動來說，content I2I 可以在沒有用戶交互訊息時，就可以進行推薦，也不失為一種冷啟動方案。&lt;/p&gt;
&lt;p&gt;在萬物皆可 Embedding 的今天，content I2I 只要把所有商品的 title 送進 word2Vec  硬 train 一發也就完事了&lt;/p&gt;
&lt;p&gt;當然要是這麼簡單，也就不會有這篇了&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/hz7T95F.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/categories/recommendation-system/"/>
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/tags/recommendation-system/"/>
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="推薦系統" scheme="https://seed9d.github.io/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/"/>
    
  </entry>
  
  <entry>
    <title>Thompson Sampling 推薦系統中簡單實用的 Exploring Strategy</title>
    <link href="https://seed9d.github.io/Implement-Thompson-Sampling-in-Recommendation-System/"/>
    <id>https://seed9d.github.io/Implement-Thompson-Sampling-in-Recommendation-System/</id>
    <published>2021-02-02T15:35:16.000Z</published>
    <updated>2021-02-02T16:38:19.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Exploring-and-Exploiting&quot;&gt;&lt;a href=&quot;#Exploring-and-Exploiting&quot; class=&quot;headerlink&quot; title=&quot;Exploring and Exploiting&quot;&gt;&lt;/a&gt;Exploring and Exploiting&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Exploring and Exploiting (EE)&lt;/strong&gt; 是推薦系統中歷久不衰的議題，如何幫助用戶發現更多感興趣的 entity 以及基於已有對用戶的認知推薦他感興趣的 entity，在推薦系統的實務上都得考慮。&lt;/p&gt;
&lt;p&gt;具象化這個問題：在推薦系統中有$\text{}$ $\text{category A, category B, category C, category D, category E}$ 等五大類的 entity 集合，今天有個新用戶 $U$來了，我們要如何&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;知道他對哪個種類的 entity 比較感興趣？&lt;/li&gt;
&lt;li&gt;人的興趣可以分成長期興趣跟短期興趣，在電商場景中，用戶短期興趣指他有立即需求的商品，我們如何快速抓到他的意圖，調整推薦系統的響應？&lt;/li&gt;
&lt;li&gt;推薦哪些類目能帶給他意料之外的驚喜 ? 那些他沒預期，但我們推薦給他，能讓他感到滿意的 category。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Multi-armed bandit problem, K-armed bandit problem (MAP) 中的 Thompson Sampling，簡單又實用&lt;/p&gt;</summary>
    
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/categories/recommendation-system/"/>
    
    
    <category term="recommendation system" scheme="https://seed9d.github.io/tags/recommendation-system/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax</title>
    <link href="https://seed9d.github.io/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/"/>
    <id>https://seed9d.github.io/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/</id>
    <published>2021-01-31T15:25:31.000Z</published>
    <updated>2021-02-10T04:43:51.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;CBOW-with-Hierarchical-Softmax&quot;&gt;&lt;a href=&quot;#CBOW-with-Hierarchical-Softmax&quot; class=&quot;headerlink&quot; title=&quot;CBOW with Hierarchical Softmax&quot;&gt;&lt;/a&gt;CBOW with Hierarchical Softmax&lt;/h1&gt;&lt;p&gt;CBOW  的思想是用兩側 context words 去預測中間的 center word&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(center|context;\theta)&lt;/script&gt;</summary>
    
    
    
    <category term="NLP" scheme="https://seed9d.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="word embedding" scheme="https://seed9d.github.io/tags/word-embedding/"/>
    
    <category term="pytorch" scheme="https://seed9d.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling</title>
    <link href="https://seed9d.github.io/Pytorch-Implement-Skipgram-with-Negative-Sampling/"/>
    <id>https://seed9d.github.io/Pytorch-Implement-Skipgram-with-Negative-Sampling/</id>
    <published>2021-01-31T15:15:36.000Z</published>
    <updated>2021-02-10T05:14:48.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;Skipgram-with-Negative-Sampling&quot;&gt;&lt;a href=&quot;#Skipgram-with-Negative-Sampling&quot; class=&quot;headerlink&quot; title=&quot;Skipgram with Negative Sampling&quot;&gt;&lt;/a&gt;Skipgram with Negative Sampling&lt;/h1&gt;&lt;p&gt;skipgram  的思想是用中心詞 center word 去預測兩側的 context words &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(context|center; \theta)&lt;/script&gt;</summary>
    
    
    
    <category term="NLP" scheme="https://seed9d.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="word embedding" scheme="https://seed9d.github.io/tags/word-embedding/"/>
    
    <category term="pytorch" scheme="https://seed9d.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec (4):Pytorch 實作 Word2Vec with Softmax</title>
    <link href="https://seed9d.github.io/Pytorch-Implement-Naive-Word2Vec-with-Softmax/"/>
    <id>https://seed9d.github.io/Pytorch-Implement-Naive-Word2Vec-with-Softmax/</id>
    <published>2021-01-31T15:03:30.000Z</published>
    <updated>2021-02-10T04:43:46.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;用  pytorch  實現最簡單版本的  CBOW 與 skipgram，objective function 採用 minimize negative log likelihood with softmax&lt;/p&gt;</summary>
    
    
    
    <category term="NLP" scheme="https://seed9d.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="word embedding" scheme="https://seed9d.github.io/tags/word-embedding/"/>
    
    <category term="pytorch" scheme="https://seed9d.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec (3):Negative Sampling 背後的數學</title>
    <link href="https://seed9d.github.io/negative-sampling-in-word2vec/"/>
    <id>https://seed9d.github.io/negative-sampling-in-word2vec/</id>
    <published>2021-01-24T15:05:45.000Z</published>
    <updated>2021-02-10T04:43:56.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;以下用 Skip-gram 為例&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/YeDgnQ9.png&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="NLP" scheme="https://seed9d.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="word embedding" scheme="https://seed9d.github.io/tags/word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec (2):Hierarchical Softmax 背後的數學</title>
    <link href="https://seed9d.github.io/hierarchical-softmax-in-word2vec/"/>
    <id>https://seed9d.github.io/hierarchical-softmax-in-word2vec/</id>
    <published>2021-01-24T11:51:54.000Z</published>
    <updated>2021-02-10T04:44:07.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;以-CBOW-為例&quot;&gt;&lt;a href=&quot;#以-CBOW-為例&quot; class=&quot;headerlink&quot; title=&quot;以 CBOW 為例&quot;&gt;&lt;/a&gt;以 CBOW 為例&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Pbdqtx9.png&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="NLP" scheme="https://seed9d.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="word embedding" scheme="https://seed9d.github.io/tags/word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec (1):NLP Language Model</title>
    <link href="https://seed9d.github.io/NLP-language-model/"/>
    <id>https://seed9d.github.io/NLP-language-model/</id>
    <published>2021-01-24T11:33:05.000Z</published>
    <updated>2021-02-10T04:44:01.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;General-Form&quot;&gt;&lt;a href=&quot;#General-Form&quot; class=&quot;headerlink&quot; title=&quot;General Form&quot;&gt;&lt;/a&gt;General Form&lt;/h1&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_1)&lt;/script&gt;&lt;p&gt;展開來後 $P(w_1, w_2, w_3,…,w_T) = P(w_1)P(x_2|w_1)P(w_3|w_2, w_1)…P(w_T|w_1,…w_{T-1})$&lt;/p&gt;</summary>
    
    
    
    <category term="NLP" scheme="https://seed9d.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec (0):從原理到實現</title>
    <link href="https://seed9d.github.io/word2vec-from-theory-2-implement/"/>
    <id>https://seed9d.github.io/word2vec-from-theory-2-implement/</id>
    <published>2021-01-24T11:11:42.000Z</published>
    <updated>2021-02-10T04:43:29.000Z</updated>
    
    
    <summary type="html">&lt;p&gt;這篇是在 notion 整理的筆記大綱，只提供綱要性的說明&lt;/p&gt;
&lt;h1 id=&quot;預備知識&quot;&gt;&lt;a href=&quot;#預備知識&quot; class=&quot;headerlink&quot; title=&quot;預備知識&quot;&gt;&lt;/a&gt;預備知識&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;language model： NLP 語言模型&lt;/p&gt;
&lt;p&gt;參閱 &lt;a href=&quot;/NLP-language-model/&quot; title=&quot;Word2Vec (1):NLP Language Model&quot;&gt;Word2Vec (1):NLP Language Model&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;huffman tree&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;簡介&quot;&gt;&lt;a href=&quot;#簡介&quot; class=&quot;headerlink&quot; title=&quot;簡介&quot;&gt;&lt;/a&gt;簡介&lt;/h1&gt;&lt;h2 id=&quot;兩種網路結構&quot;&gt;&lt;a href=&quot;#兩種網路結構&quot; class=&quot;headerlink&quot; title=&quot;兩種網路結構&quot;&gt;&lt;/a&gt;兩種網路結構&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/395NfQN.png&quot; alt=&quot;CBOW and skipgram&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="NLP" scheme="https://seed9d.github.io/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://seed9d.github.io/tags/NLP/"/>
    
    <category term="word embedding" scheme="https://seed9d.github.io/tags/word-embedding/"/>
    
  </entry>
  
  <entry>
    <title>一步步透視 GBDT Classifier</title>
    <link href="https://seed9d.github.io/GBDT-Classifier-step-by-step/"/>
    <id>https://seed9d.github.io/GBDT-Classifier-step-by-step/</id>
    <published>2021-01-23T16:09:45.000Z</published>
    <updated>2021-02-09T15:14:50.000Z</updated>
    
    
    <summary type="html">&lt;h1 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$&lt;ul&gt;
&lt;li&gt;$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}$ 的方向&lt;ul&gt;
&lt;li&gt;$F_m(x) = F_{m-1}(x) + \nu f_m(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GBDT classifier 常用的 loss function 為 cross entropy&lt;/li&gt;
&lt;li&gt;classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$  跟 $probability$  有關，得將 $F(x)$ 通過 $\textit{sigmoid function }$ 獲得  probability&lt;ul&gt;
&lt;li&gt;$p = \sigma(F(x))$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GBDT 簡介在 &lt;a href=&quot;https://seed9d.github.io/GBDT-Rregression-Tree-Step-by-Step/#GBDT-%E7%B0%A1%E4%BB%8B&quot;&gt;一步步透視 GBDT Regression Tree&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;直接進入正題吧&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://seed9d.github.io/categories/Machine-Learning/"/>
    
    
    <category term="ML" scheme="https://seed9d.github.io/tags/ML/"/>
    
  </entry>
  
</feed>
