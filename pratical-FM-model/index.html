<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>推薦系統中的瑞士刀 Factorization Machine - seed9D&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="seed9D&#039;s blog"><meta name="msapplication-TileImage" content="/images/seed.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="seed9D&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="多才多藝的 Factorization Machine在推薦領域，現今各種 DNN 模型當道，有個不怎麼深的模型卻小巧而美，既能做召回又能做排序，計算複雜度上又沒 DNN 耗時又耗錢 他 就是現今各種應用在推薦系統 DNN 的前輩 Factorization Machine (FM)，這麼多才多藝的模型你值得擁有。 我們都知道 DNN 的本質是特徵的高階交叉，這點在 FM 上就能初見端倪： y(x"><meta property="og:type" content="article"><meta property="og:title" content="推薦系統中的瑞士刀 Factorization Machine"><meta property="og:url" content="https://seed9d.github.io/pratical-FM-model/"><meta property="og:site_name" content="seed9D&#039;s blog"><meta property="og:description" content="多才多藝的 Factorization Machine在推薦領域，現今各種 DNN 模型當道，有個不怎麼深的模型卻小巧而美，既能做召回又能做排序，計算複雜度上又沒 DNN 耗時又耗錢 他 就是現今各種應用在推薦系統 DNN 的前輩 Factorization Machine (FM)，這麼多才多藝的模型你值得擁有。 我們都知道 DNN 的本質是特徵的高階交叉，這點在 FM 上就能初見端倪： y(x"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://i.imgur.com/cmTDEmU.png"><meta property="og:image" content="https://i.imgur.com/Hd0lkJN.png"><meta property="og:image" content="https://i.imgur.com/p752hZl.png"><meta property="article:published_time" content="2021-02-18T16:00:42.000Z"><meta property="article:modified_time" content="2021-02-19T05:16:55.000Z"><meta property="article:author" content="seed9D"><meta property="article:tag" content="recommendation system"><meta property="article:tag" content="推薦系統"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.imgur.com/cmTDEmU.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://seed9d.github.io/pratical-FM-model/"},"headline":"seed9D's blog","image":["https://i.imgur.com/cmTDEmU.png","https://i.imgur.com/Hd0lkJN.png","https://i.imgur.com/p752hZl.png"],"datePublished":"2021-02-18T16:00:42.000Z","dateModified":"2021-02-19T05:16:55.000Z","author":{"@type":"Person","name":"seed9D"},"description":"多才多藝的 Factorization Machine在推薦領域，現今各種 DNN 模型當道，有個不怎麼深的模型卻小巧而美，既能做召回又能做排序，計算複雜度上又沒 DNN 耗時又耗錢 他 就是現今各種應用在推薦系統 DNN 的前輩 Factorization Machine (FM)，這麼多才多藝的模型你值得擁有。 我們都知道 DNN 的本質是特徵的高階交叉，這點在 FM 上就能初見端倪： y(x"}</script><link rel="canonical" href="https://seed9d.github.io/pratical-FM-model/"><link rel="icon" href="/images/seed.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="seed9D's blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/logo.svg" alt="seed9D&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>推薦系統中的瑞士刀 Factorization Machine</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-02-19</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-02-19</time></span><span class="level-item"><a class="link-muted" href="/categories/recommendation-system/">recommendation system</a></span><span class="level-item">33 minutes read (About 4893 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h1 id="多才多藝的-Factorization-Machine"><a href="#多才多藝的-Factorization-Machine" class="headerlink" title="多才多藝的 Factorization Machine"></a>多才多藝的 Factorization Machine</h1><p>在推薦領域，現今各種 DNN 模型當道，有個不怎麼深的模型卻小巧而美，既能做召回又能做排序，計算複雜度上又沒 DNN 耗時又耗錢</p>
<p>他 就是現今各種應用在推薦系統 DNN 的前輩 Factorization Machine (FM)，這麼多才多藝的模型你值得擁有。</p>
<p>我們都知道 DNN 的本質是特徵的高階交叉，這點在 FM 上就能初見端倪：</p>
<script type="math/tex; mode=display">y(x) = w_0 + \sum^n_{i=1}w_ix_i + \sum^{n-1}_{i=1}\sum^n_{j=i+1}<v_i, v_j>x_ix_j \tag{1}</script><p>FM 前兩項為 LR:   $w_0 + \sum^n_{i=1}w_ix_i$，末項為二階特徵交叉項 <script type="math/tex">\sum^{n-1}_{i=1} \sum^n_{j=i+1}<v_i, v_j>x_ix_j</script>，這形式像極了 wide &amp; deep，根本是 wide &amp; deep  的前身，linear 項負責  memorization; cross 項負責 generalization。</p>
<p>FM 既能做召回又能做排序，甚至能在多路召回後對上萬個 entity 粗排打分，到底是怎麼做到的呢？</p>
<p>讓我們往下繼續看下去</p>
<a id="more"></a>
<h2 id="特徵-label-化"><a href="#特徵-label-化" class="headerlink" title="特徵 label 化"></a>特徵 label 化</h2><p>首先在推薦領域中，特徵大部分都可被離散化/ label 化，也就是所謂的分桶 label，離散化後的特徵 $x$  非 0 即 1，所以式 (1) 可以簡化成</p>
<script type="math/tex; mode=display">y(x) = w_0 + \sum^n_{i=1}w_i + \sum^{n-1}_{i=1}\sum^n_{j=i+1}<v_i, v_j> \tag{2}</script><p>Label  化例子：</p>
<p>gender 這個 feature 有三個取值 Men Women Unknown</p>
<ul>
<li>label 化後取值為 0 ~ 2</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;Men&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"><span class="string">&#x27;Women&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"><span class="string">&#x27;Unknown&#x27;</span>: <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>事實上， gender 的 label 化 跟  3 dimensions 的 one-hot encoder 是一體兩面的</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line"><span class="number">1</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line"><span class="number">2</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>label  化，相當於 one-hot encoder 的壓縮，省去零值的儲存，因爲 matrix  multiplication 時零值對結果沒影響 ，計算時只需從 weight 中取出 label 所對應的 index 維度即可。</p>
<p>推薦系統中，特徵維度上百萬甚至上千萬指的是 one-hot encode 展開來後的維度，實際上運算的時候不需要這麼多，因爲特徵非常非常稀疏。</p>
<h2 id="FM-向量召回"><a href="#FM-向量召回" class="headerlink" title="FM 向量召回"></a>FM 向量召回</h2><h3 id="Math-Background"><a href="#Math-Background" class="headerlink" title="Math Background"></a>Math Background</h3><p>先把 式(2) 中的二階項的 hidden vector $v$ 拆解成 item hidden vector $v^{item}$ 跟 user hidden vector $v^{user}$ 的關係</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\sum^{n-1}_{i=1}\sum^n_{j=i+1}<v_i, v_j> \\ 
&= \color{red}{\sum_{i=1}^{n^{user } -1} \sum_{j=i + 1}^{n^{user}}
< v^{user}_i,  v^{user}_j>} +

\color{blue}{ \sum_{i=1}^{n^{item } -1} \sum_{j= i + 1}^{n^{item}} 
<v^{item}_i  v^{item}_j> } +
\color{green}{<\sum_{i=1}^{n^{user}} v^{user}, \sum_{j= i + 1}^{n^{item}} v^{item}>} 

\\
&= \color{red}{\sum_{i=1}^{n^{user } -1} \sum_{j= i + 1}^{n^{user}}
< v^{user}_i,  v^{user}_j>}+ 
\color{blue}{ \sum_{i=1}^{n^{item } -1} \sum_{j= i + 1}^{n^{item}} 
<v^{item}_i  v^{item}_j> } +
\color{green} {<V^{user}_u, V^{item}_i>} \\
& = \text{a number} 
\end{aligned} \tag{3}</script><ul>
<li><p>$V^{user}_u$ 為  user-Id $u$ 的所有 user features’ hidden vector 逐位和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	@ one_user_hidden_vectors shape: [number of user features, hidden vector dimension]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span> </span><br><span class="line">sum_of_one_user_hidden_vector = np.<span class="built_in">sum</span>(one_user_hidden_vectors, axis=<span class="number">0</span>) <span class="comment"># shape: [hidden vector dimension]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>$V^{item}_i$ 為 item-Id $i$ 所有的 item features’ hidden vector 逐位和</p>
</li>
<li><p>$n^{user} + n^{item} = n$</p>
</li>
</ul>
<p>式(3)  白話說就是三個部分的加法</p>
<script type="math/tex; mode=display">\color{red}{\text{cross prod sum of user hidden vector } v^{user} } + \color{blue}{\text{cross prod sum of item hidden vector } v^{item}} + \color{green}{\text{inner product of } V^{user}_u \text{and } V^{item}_i }</script><p>以同樣方式分解式 (2)中的一階項</p>
<script type="math/tex; mode=display">\begin{aligned}
\sum^n_{i=1}w_i &= \color{red} {\sum w^{user} }+ 
\color{blue}{\sum w^{item} }\\
&= \color{red}{ W^{user}} +  \color{blue} {W^{item} }
\end{aligned} \tag{4}</script><ul>
<li>$w^{user}$ 為與 user features 對應的 weight</li>
<li>$w^{item}$ 為與 item features 對應的 weight</li>
<li>$W^{user}$ 為所有 user features 對應的 weights 和</li>
<li>$W^{item}$ 為所有 item features 對應的 weights 和</li>
</ul>
<p>用 式(3) 式(4) 改寫式(2)：</p>
<script type="math/tex; mode=display">\begin{aligned}
\tilde{y} &= \not{x_0} \quad + \\
&  \color{red}  {W^{user} }+ 
\color{blue} {W^{item} }+ \\
&  \color{red}{\sum_{i=1}^{n^{user } -1} \sum_{j= i + 1}^{n^{user}}
< v^{user}_i,  v^{user}_j>}+ 
\color{blue}{ \sum_{i=1}^{n^{item } -1} \sum_{j= i + 1}^{n^{item}} 
<v^{item}_i  v^{item}_j> } +
\color{green} {<V^{user}_u, V^{item}_i>} 
\end{aligned} \tag{5}</script><ul>
<li>bias 大家都一樣，要的只是相對的 score，所以可以去掉</li>
<li>$V^{user}_{u}$ 為 user-id $u$  的 user features’s hidden vector 逐位和</li>
<li>$V^{item}_i$ 為 item-Id $i$ 所有的 item features’ hidden vector 逐位和</li>
</ul>
<p>推薦系統在做向量召回時，通常是以 cosine similarity 做為 score 衡量兩個向量的 similarity，式 (5) 可以轉化成兩個 vector 的 inner product，即</p>
<script type="math/tex; mode=display">\text{similarity score} = E^{user}_u \cdot {E^{item}_i} \tag{6}</script><ul>
<li>$E^{user}_u$ 為 user-id $u$ 的 embedding</li>
<li>$E^{item}_i$ 為  item-id $i$ 的 embedding</li>
</ul>
<p>式(5) 推導到 式(6) 非常 trikcy，不過看下圖也就一目了然了</p>
<p><img src="https://i.imgur.com/cmTDEmU.png" alt="Figure 1 user embedding and item embedding" style="zoom: 67%;" /></p>
<ul>
<li>user embedding vector $E^{user}_u$  的維度為 $dim(V^{user}_u) + 2$，在 $V^{user}_{u}$左側 concatenate 2 維</li>
<li>item embedding vector $E^{item}_{i}$ 的維度為 $dim(V^{item}_i) + 2$，在 $V^{item}_i$ 左側 concatenate 2 維</li>
</ul>
<p>計算向量內積 <script type="math/tex"><E^{user}_u , E^{item}_{i}></script> ，式 (7) 神奇的跟 式 (5) 相等！</p>
<script type="math/tex; mode=display">\begin{aligned}
\text{similarity score} & = <E^{user}_u , E^{item}_{i}> \\
& =  \color{red}{W^{user} + \sum_{i=1}^{n^{user } -1} \sum_{j= i + 1}^{n^{user}}
< v^{user}_i,  v^{user}_j>} +  \color{blue}{W^{item} + \sum_{i=1}^{n^{item } -1} \sum_{j= i + 1}^{n^{item}} 
<v^{item}_i  v^{item}_j> } + \color{green}{<V^{user}_u, V^{item}_i>}\\
&= \tilde{y}
\end{aligned} \tag{7}</script><p>總結一下，FM 的 output score $y$ 可以拆解成用兩個 embedding vector $E^{user}_u$ $E^{item}_i$  inner product 表示，這特性用在召回時非常方便。</p>
<h3 id="使用說明書"><a href="#使用說明書" class="headerlink" title="使用說明書"></a>使用說明書</h3><p>我們在線下訓練完 FM 之後，分別將 user Embedding $E^{user}$ , item Embedding $E^{item}$ 存入 Faiss 或是任何其他相似的 vector similarity search engine</p>
<p>到了線上，觸發 user-id $u$ 的推薦:</p>
<ul>
<li>I2I<ol>
<li>取回 user-id $u$ 的歷史交互 item-ids $I$</li>
<li>對每個 item-id $i \in I$，取回其 item embedding $E^{item}_i$ </li>
<li>將 $E^{item}_i$ 拿去 faiss search 相似的 item-ids，完成 I2I 召回 </li>
</ol>
</li>
<li>U2I<ol>
<li>取回 user-id $u$ 的 user embedding $E^{user}_u$</li>
<li>將 $E^{user}_u$拿去  faiss search 相似的 item-ids 完成 U2I 召回</li>
</ol>
</li>
</ul>
<p>上面的 I2I 可以做成 offline 版本，預先將所有 item-id $i$ 的 TopK I2I 算出後存進 redis ，線上只要根據 item-id $i$  取出對應的  item-ids 即可</p>
<h3 id="Training-Tips"><a href="#Training-Tips" class="headerlink" title="Training Tips"></a>Training Tips</h3><p>FM 用在召回任務時，用意是學出更好的 user and item embedding，所以在 objective function 的選擇上使用 logit loss 不是不行，但有更好的作法。</p>
<p>Tomas Mikolov 在他 word2Vec 裡採用的 Negative Sampling 優化 softmax 的運算瓶頸，Negative Sampling  (NCE) 可以從 cross  entropy 一步步推導出來  參見 <a href="/negative-sampling-in-word2vec/" title="Word2Vec (3):Negative Sampling 背後的數學">Word2Vec (3):Negative Sampling 背後的數學</a></p>
<p>對比於 Hierarchical Softmax 本質上還是學出 likelihood of correct words，Negative Sampling 對於學到更好的 representation of word distribution 有奇效。也因為 Negative Sampling 能學出更好的 word distribution， google 2016 年的 youtube net、 airbnb 2018 年的 listings embedding  都有類似的做法，所以在召回問題上 objective function 可以採用 NCE 優化之。</p>
<h2 id="FM-排序"><a href="#FM-排序" class="headerlink" title="FM 排序"></a>FM 排序</h2><p>FM 用在排序時可分成 粗排 (coarse rank) 和精排 (fine rank)，兩者運算過程一樣，只不過粗排用的特徵數量應遠少於精排，因為粗排得對多路召回的上萬個 entity 打分截斷。</p>
<h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><p>線上觸發 user-id $u$ 的推薦：</p>
<ol>
<li>取回  user-id $u$ 的  user embedding $E^{user}_u$ 和待排序 items $I$ 的 item embeddings $E^{item}_{I}$</li>
<li>將 $E^{user}_u$ 和所有 $E^{item}_I$ inner product 計算出 score $\text{S}_I^u$，根據式 (7) 兩者相等</li>
<li>對 $S^u_{I}$ 排序截斷 topK</li>
</ol>
<h3 id="Some-Tips"><a href="#Some-Tips" class="headerlink" title="Some Tips"></a>Some Tips</h3><p>在排序時，若無依賴線上 context 的特徵，從 DB 取出  $E^{user}_u$    $E^{item}_i$ 按照  式(6) 計算 score 即可。</p>
<p>若有依賴 context 的特徵，得按照 式 (5) 計算 score 。</p>
<p>例如: 若 item 側有個 feature 為 “多路召回中來自哪路”，這是線上才能得知的 context 訊息，無法離線算好，所以得將 FM model 式 (1) 的所有 $\theta$ 都存下來帶到線上，照著 式 (5)  計算 score。</p>
<p>另外，當我們在計算 cross prod sum of hidden vectors 時，別忘了有複雜度更低的計算方式</p>
<script type="math/tex; mode=display">\sum^{n-1}_{i=1}\sum^n_{j=i+1}  <v_i,v_j> = \frac{1}{2} \sum_{f=1}^k{\left( \left(\sum_{i=1}^n{v_{i,f}x_i}\right)^2 - \sum_{i=1}^nv_{i,f}^2 x_i^2  \right)}</script><h1 id="Implement-FM-by-Pytorch"><a href="#Implement-FM-by-Pytorch" class="headerlink" title="Implement FM by Pytorch"></a>Implement FM by Pytorch</h1><p>dataset 用 movie len ml-1m </p>
<p><a target="_blank" rel="noopener" href="https://grouplens.org/datasets/movielens/">MovieLens</a></p>
<h2 id="Bucketize-and-Label-Features"><a href="#Bucketize-and-Label-Features" class="headerlink" title="Bucketize and Label Features"></a>Bucketize and Label Features</h2><p>上面說過，在推薦系統中的特徵都得分桶 label 化， movie len 的 features  都已分桶過的，所以我們只需要 label 化即可</p>
<p>P.S. Sklearn 中有現成的 sklearn.preprocessing.LabelEncoder 和 sklearn.preprocessing.KBinsDiscretizer 可以用</p>
<figure class="highlight"><figcaption><span>label encoder >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, columns_to_encode: List[<span class="built_in">str</span>]</span>):</span></span><br><span class="line">        self.columns_to_encode = columns_to_encode</span><br><span class="line">        self.unseen = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, df: pd.DataFrame, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        df_ = df[self.columns_to_encode].copy()</span><br><span class="line"></span><br><span class="line">        df_[self.columns_to_encode] = df[self.columns_to_encode].astype(</span><br><span class="line">            <span class="string">&#x27;str&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        unique_column_vals = &#123;col: df_[col].unique()</span><br><span class="line">                              <span class="keyword">for</span> col <span class="keyword">in</span> self.columns_to_encode&#125;</span><br><span class="line"></span><br><span class="line">        self.encoding_dict_ = OrderedDict()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> self.columns_to_encode:</span><br><span class="line">            unique_value = unique_column_vals[col]</span><br><span class="line">            self.encoding_dict_[col] = &#123;val: idx <span class="keyword">for</span> idx, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(unique_value)&#125;</span><br><span class="line">            self.encoding_dict_[col][self.unseen] = <span class="built_in">len</span>(self.encoding_dict_[col])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, df: pd.DataFrame</span>):</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.encoding_dict_</span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">raise</span> NotFittedError(</span><br><span class="line">                <span class="string">&quot;This LabelEncoder instance is not fitted yet. &quot;</span></span><br><span class="line">                <span class="string">&quot;Call &#x27;fit&#x27; with appropriate arguments before using this LabelEncoder.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        df_ = df.copy()</span><br><span class="line">        filtered_columns = [col <span class="keyword">for</span> col <span class="keyword">in</span> self.columns_to_encode <span class="keyword">if</span> col <span class="keyword">in</span> df_.columns]</span><br><span class="line">        df_[filtered_columns] = df_[filtered_columns].astype(<span class="string">&#x27;str&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> col, encoding_map <span class="keyword">in</span> self.encoding_dict_.items():</span><br><span class="line">            original_value = [f <span class="keyword">for</span> f <span class="keyword">in</span> encoding_map.keys() <span class="keyword">if</span> f != self.unseen]</span><br><span class="line">            <span class="keyword">if</span> col <span class="keyword">in</span> filtered_columns:</span><br><span class="line">                df_[col] = np.where(df_[col].isin(</span><br><span class="line">                    original_value), df_[col], self.unseen)</span><br><span class="line">                df_[col] = df_[col].apply(<span class="keyword">lambda</span> x: encoding_map[x])</span><br><span class="line">        <span class="keyword">return</span> df_</span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure>
<p>Before labeling:</p>
<p><img src="https://i.imgur.com/Hd0lkJN.png" style="zoom: 50%;" /></p>
<p>After labeling:</p>
<p><img src="https://i.imgur.com/p752hZl.png" style="zoom:50%;" /></p>
<p><strong>P.S. userId label 化後的 label 值跟 原本的 Id 會不一樣，很容易搞混。</strong></p>
<p>把 gender ecoding  後的對應關係打出來看看:</p>
<p>In:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">id_encoder.encoding_dict_[<span class="string">&#x27;gender&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;-<span class="number">1</span>: <span class="number">2</span>, <span class="string">&#x27;F&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;M&#x27;</span>: <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<h2 id="FM-Model"><a href="#FM-Model" class="headerlink" title="FM Model"></a>FM Model</h2><p>自己實現 FM</p>
<figure class="highlight"><figcaption><span>FMmodel >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FactorizationMachine</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, reduce_sum=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.reduce_sum = reduce_sum</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        square_of_sum = torch.<span class="built_in">sum</span>(x, dim=<span class="number">1</span>) ** <span class="number">2</span></span><br><span class="line">        sum_of_square = torch.<span class="built_in">sum</span>(x ** <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        ix = square_of_sum - sum_of_square</span><br><span class="line">        <span class="keyword">if</span> self.reduce_sum:</span><br><span class="line">            ix = torch.<span class="built_in">sum</span>(ix, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># [b_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * ix </span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FactorizationMachineModel</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, fields_index: Dict[<span class="built_in">str</span>, <span class="built_in">int</span>], fields_dims: Dict[<span class="built_in">str</span>, <span class="built_in">int</span>], embed_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FactorizationMachineModel, self).__init__()</span><br><span class="line">        self.fields_index = fields_index</span><br><span class="line">        self.all_fields = [k <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">sorted</span>(self.fields_index.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>])]</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        </span><br><span class="line">        self.fields_dims_dict = fields_dims</span><br><span class="line">        self.fields_dims = np.array([self.fields_dims_dict[f] <span class="keyword">for</span> f <span class="keyword">in</span> self.all_fields])</span><br><span class="line">        </span><br><span class="line">        self.fields_offset = torch.tensor((<span class="number">0</span>, *np.cumsum(self.fields_dims)[:-<span class="number">1</span>]), dtype=torch.long).requires_grad_(<span class="literal">False</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            field_offset [1, sum_of_field_dims]</span></span><br><span class="line"><span class="string">            field_dims:  np.array([10, 20 ,5, 7, 9])</span></span><br><span class="line"><span class="string">            [*np.cumsum(self.field_dims)[:-1]] = [10, 30, 35, 42]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.fields_range = self._gen_fields_range(self.fields_dims)</span><br><span class="line">        self.embedding = torch.nn.Embedding(self.fields_dims.<span class="built_in">sum</span>(), embed_dim)</span><br><span class="line">        torch.nn.init.xavier_uniform_(self.embedding.weight.data)</span><br><span class="line">        </span><br><span class="line">        self.fc = torch.nn.Embedding(self.fields_dims.<span class="built_in">sum</span>(), <span class="number">1</span>)</span><br><span class="line">        self.bias = torch.nn.Parameter(torch.zeros((<span class="number">1</span>, )))</span><br><span class="line">        </span><br><span class="line">        self.fm = FactorizationMachine(reduce_sum=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward_embedding</span>(<span class="params">self, X: torch.Tensor</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embedding(X) <span class="comment"># [b_size, field_num, embed_dim]</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_forward_linear</span>(<span class="params">self, X: torch.Tensor</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">sum</span>(self.fc(X), dim=<span class="number">1</span>) + self.bias <span class="comment"># [b_size, output_dim]</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_field_vector</span>(<span class="params">self, field_name: <span class="built_in">str</span>, label: <span class="built_in">int</span></span>)-&gt; np.ndarray:</span></span><br><span class="line">        <span class="keyword">assert</span> field_name <span class="keyword">in</span> self.fields_index</span><br><span class="line">        <span class="keyword">assert</span> label &lt; self.fields_dims_dict[field_name]</span><br><span class="line">        field_index = self.fields_index[field_name]</span><br><span class="line">        field_range = self.fields_range[field_index]</span><br><span class="line">        </span><br><span class="line">        field_vector = self.embedding.weight.data[field_range[<span class="number">0</span>]: field_range[<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">return</span> field_vector[label]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_fields_range</span>(<span class="params">self, fields_dims: np.ndarray</span>)-&gt; List[Tuple[int, int]]:</span></span><br><span class="line">        fields_offset = [<span class="number">0</span>, *np.cumsum(fields_dims)]</span><br><span class="line">        <span class="keyword">return</span> [(s, e) <span class="keyword">for</span> s, e <span class="keyword">in</span> <span class="built_in">zip</span>(fields_offset, fields_offset[<span class="number">1</span>:])]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># X [b_size, num_fields]</span></span><br><span class="line">        <span class="keyword">if</span> X.get_device() != self.fields_offset.get_device():</span><br><span class="line">            self.fields_offset = self.fields_offset.to(X.device)</span><br><span class="line">        </span><br><span class="line">        X = X + self.fields_offset</span><br><span class="line">        X = self._forward_linear(X) + self.fm(self._forward_embedding(X)) <span class="comment"># [b_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> X.squeeze(<span class="number">1</span>) <span class="comment"># [b_size]</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_probs</span>(<span class="params">self, X: torch.Tensor</span>):</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            out = self.forward(X)</span><br><span class="line">            <span class="keyword">return</span> torch.sigmoid(out)</span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure>
<ul>
<li>self.embedding 存放 hidden vector ，也就是 式(1) 中的 $v$</li>
<li>self.fc, self.bias 分別為 式(1) 中的 bais 跟  weight</li>
<li>instance <a target="_blank" rel="noopener" href="http://self.fm">self.fm</a> 計算 cross prod sum of hidden vector: $\frac{1}{2} \sum_{f=1}^k{\left( \left(\sum_{i=1}^n{v_{i,f}x_i}\right)^2 - \sum_{i=1}^nv_{i,f}^2 x_i^2  \right)}$</li>
<li>self.fields_offset，存放特徵 field $i$ 在 self.embedding 中對應的起始位置<ul>
<li>i.e gender 這個特徵有 3 個取值 Men, Women, unknown 形成一個 field，所以 gender 在 self.embedding  會佔據三個 rows ， self.fields_offset  存放 gender 第一個取值 Men  在 self.embedding 中的 row index</li>
</ul>
</li>
</ul>
<h2 id="訓練過程"><a href="#訓練過程" class="headerlink" title="訓練過程"></a>訓練過程</h2><p>請參閱 <a target="_blank" rel="noopener" href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Recommendation">seed9D/hands-on-machine-learning</a></p>
<h2 id="組合出-E-user-E-item"><a href="#組合出-E-user-E-item" class="headerlink" title="組合出  $E^{user}$ $E^{item}$"></a>組合出  $E^{user}$ $E^{item}$</h2><p>訓練完後，取出 FM model 裡的 weight $W$ 跟 hidden vector $V$，按照 figure 1 所示，組合出  user embedding  跟 item embedding</p>
<figure class="highlight"><figcaption><span>FMEmbedding >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line">ID2Vector = namedtuple(<span class="string">&#x27;id2vector&#x27;</span>, <span class="string">&#x27;id vector&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FMEmbedding</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, id_encoder, user_fields, item_fields, fm_model</span>):</span></span><br><span class="line">        self.id_encoder = id_encoder</span><br><span class="line">        self.user_fields = user_fields</span><br><span class="line">        self.item_fields = item_fields</span><br><span class="line">        self.fm_model = fm_model</span><br><span class="line">        </span><br><span class="line">        self.fields_dims_dict = fm_model.fields_dims_dict</span><br><span class="line">        self.fields_index=  fm_model.fields_index</span><br><span class="line">        self.fields_dims = fm_model.fields_dims</span><br><span class="line">        self.fields_range = self._gen_fields_range(self.fields_dims)</span><br><span class="line">            </span><br><span class="line">        self.fields_vectors = self.fm_model.embedding.weight.data.numpy() <span class="comment"># hidden vector</span></span><br><span class="line">        self.fields_weights = self.fm_model.fc.weight.data.numpy()</span><br><span class="line">        self.bias = self.fm_model.bias.data.numpy()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_user_embedding</span>(<span class="params">self, primitive_features: pd.DataFrame, userId_column=<span class="string">&#x27;userId&#x27;</span></span>)-&gt; List[ID2Vector]:</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(primitive_features.columns) == <span class="built_in">set</span>(self.user_fields)</span><br><span class="line">        user_id_features = self.id_encoder.transform(primitive_features).values</span><br><span class="line">        user_fields_index = [self.fields_index[col] <span class="keyword">for</span> col <span class="keyword">in</span> self.user_fields]</span><br><span class="line">        </span><br><span class="line">        fields_offset = np.array([<span class="number">0</span>, *np.cumsum(self.fields_dims)[:-<span class="number">1</span>]])[user_fields_index].reshape((<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        user_IDs_features = user_id_features + fields_offset</span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;  field vector&#x27;&#x27;&#x27;</span></span><br><span class="line">        user_IDs_vectors = np.take(self.fields_vectors, user_IDs_features, axis=<span class="number">0</span>) <span class="comment"># [b_size, user_field_size, vector_dims]</span></span><br><span class="line">        user_fields_cross = self._cross_vector(user_IDs_vectors) <span class="comment"># [b_size]</span></span><br><span class="line">        </span><br><span class="line">        user_IDs_vectors = user_IDs_vectors.<span class="built_in">sum</span>(axis=<span class="number">1</span>) <span class="comment"># [b_size, vector_dims]</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;field weight&#x27;&#x27;&#x27;</span></span><br><span class="line">        user_fields_weights = np.take(self.fields_weights, user_IDs_features, axis=<span class="number">0</span>).squeeze(<span class="number">2</span>) <span class="comment"># [b_size, user_field_size]</span></span><br><span class="line">        user_fields_weights = user_fields_weights.<span class="built_in">sum</span>(axis=<span class="number">1</span>) <span class="comment"># [b_size]</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            user embebding composition [1 :: (user_fields_cross + user_fields_weghts):: user_IDs_vectors]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        ones = np.ones((<span class="built_in">len</span>(user_fields_weights), <span class="number">1</span>)) <span class="comment"># [b_size]</span></span><br><span class="line">        </span><br><span class="line">        embedding =  np.hstack([ones, np.expand_dims(user_fields_cross + user_fields_weights, <span class="number">1</span>), user_IDs_vectors])</span><br><span class="line"></span><br><span class="line">        id2vector = [ID2Vector(userId, vector) <span class="keyword">for</span> userId, vector <span class="keyword">in</span> <span class="built_in">zip</span>(primitive_features[userId_column].tolist(), embedding)]</span><br><span class="line">        <span class="keyword">return</span> id2vector</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_item_embedding</span>(<span class="params">self, primitive_features: pd.DataFrame, itemId_col=<span class="string">&#x27;itemId&#x27;</span></span>)-&gt; List[ID2Vector]:</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">set</span>(primitive_features.columns) == <span class="built_in">set</span>(self.item_fields)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            primitive_features may contain unseen itemId</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        item_id_feautres = self.id_encoder.transform(primitive_features).values</span><br><span class="line">        item_fields_index = [self.fields_index[col] <span class="keyword">for</span> col <span class="keyword">in</span> self.item_fields]</span><br><span class="line">        </span><br><span class="line">        fields_offset = np.array([<span class="number">0</span>, *np.cumsum(self.fields_dims)[:-<span class="number">1</span>]])[item_fields_index].reshape((<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        item_IDs_features = item_id_feautres + fields_offset</span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; field vector &#x27;&#x27;&#x27;</span></span><br><span class="line">        item_IDs_vectors = np.take(self.fields_vectors, item_IDs_features, axis=<span class="number">0</span>) <span class="comment"># [b_size, item_fields_size, field_vector_dims]</span></span><br><span class="line">        item_fields_cross = self._cross_vector(item_IDs_vectors) <span class="comment"># [b_size]</span></span><br><span class="line">        item_IDs_vectors = item_IDs_vectors.<span class="built_in">sum</span>(axis=<span class="number">1</span>) <span class="comment"># [b_size, field_vector_dims]</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;field weight&#x27;&#x27;&#x27;</span></span><br><span class="line">        item_fields_weights = np.take(self.fields_weights, item_IDs_features, axis=<span class="number">0</span>).squeeze(<span class="number">2</span>) <span class="comment"># [b_size, item_field_size]</span></span><br><span class="line">        item_fields_weights = item_fields_weights.<span class="built_in">sum</span>(axis=<span class="number">1</span>) <span class="comment"># [b_size]</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#         &#x27;&#x27;&#x27;bias&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment">#         bias = np.full((len(item_fields_weights), 1), self.bias.data)</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            item embedding composition: [(item_fields_weights + item_fileds_cross ):: 1 :: item field vector]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        ones = np.ones((<span class="built_in">len</span>(item_fields_weights), <span class="number">1</span>)) <span class="comment"># [b_size, 1]</span></span><br><span class="line">        sum_ = np.expand_dims(item_fields_weights + item_fields_cross, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        embedding = np.hstack([sum_, ones, item_IDs_vectors])</span><br><span class="line">        id2vector = [ID2Vector(itemId, vector) <span class="keyword">for</span> itemId, vector <span class="keyword">in</span> <span class="built_in">zip</span>(primitive_features[itemId_col].tolist(), embedding) ]</span><br><span class="line">        <span class="keyword">return</span> id2vector</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_cross_vector</span>(<span class="params">self, vectors: np.ndarray</span>)-&gt; int:</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            @vectors: [b_size, vector_dims]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        square_of_sum = np.<span class="built_in">sum</span>(vectors, axis=<span class="number">1</span>) ** <span class="number">2</span></span><br><span class="line">        sum_of_square = np.<span class="built_in">sum</span>(vectors ** <span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line">        reduce_sum = np.<span class="built_in">sum</span>(square_of_sum - sum_of_square, axis=<span class="number">1</span>) <span class="comment"># [b_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * reduce_sum</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_field_vector</span>(<span class="params">self, field_name: <span class="built_in">str</span>, label: <span class="built_in">int</span></span>)-&gt; np.ndarray:</span></span><br><span class="line">        <span class="keyword">assert</span> field_name <span class="keyword">in</span> self.fields_index</span><br><span class="line">        <span class="keyword">assert</span> label &lt; self.fields_dims_dict[field_name]</span><br><span class="line">        </span><br><span class="line">        field_index = self.fields_index[field_name]</span><br><span class="line">        field_range = self.fields_range[field_index]</span><br><span class="line">        </span><br><span class="line">        field_vector = self.fields_vectors[field_range[<span class="number">0</span>]: field_range[<span class="number">1</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> field_vector[label]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gen_fields_range</span>(<span class="params">self, fields_dims: np.ndarray</span>)-&gt; List[Tuple[int, int]]:</span></span><br><span class="line">        fields_offset = [<span class="number">0</span>, *np.cumsum(fields_dims)]</span><br><span class="line">        <span class="keyword">return</span> [(s, e) <span class="keyword">for</span> s, e <span class="keyword">in</span> <span class="built_in">zip</span>(fields_offset, fields_offset[<span class="number">1</span>:])]</span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure>
<ul>
<li>function get_user_embedding 組合出 user embedding</li>
</ul>
<script type="math/tex; mode=display">\text{concat}(1, \color{red}{W^{user} + \sum_{i=1}^{n^{user } -1} \sum_{j= i + 1}^{n^{user}}
< v^{user}_i,  v^{user}_j>}, \color{green}{V^{user}_u})</script><ul>
<li>function get_item_embedding 組合出 item embedding</li>
</ul>
<script type="math/tex; mode=display">\text{concat}(\color{blue}{W^{item} + \sum_{i=1}^{n^{item } -1} \sum_{j= i + 1}^{n^{item}} 
<v^{item}_i  v^{item}_j> }, \color{black}{1}, \color{green}{V^{item}_i})</script><p>print  一個  item embedding 看看</p>
<p>In:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">item_id_2_vector = fm_embedding.get_item_embedding(item_df, <span class="string">&#x27;movieId&#x27;</span>)</span><br><span class="line">print(item_id_2_vector[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">id2vector(</span><br><span class="line"><span class="built_in">id</span>=<span class="number">1</span>, </span><br><span class="line">vector=array([-<span class="number">1.6513232</span> ,  <span class="number">1.</span>        , -<span class="number">0.15136771</span>, -<span class="number">0.19989893</span>, -<span class="number">0.17711505</span>,</span><br><span class="line">       -<span class="number">0.27605495</span>, -<span class="number">0.17580783</span>, -<span class="number">0.20358004</span>,  <span class="number">0.19178246</span>,  <span class="number">0.28852561</span>,</span><br><span class="line">        <span class="number">0.19627182</span>, -<span class="number">0.21493186</span>,  <span class="number">0.22243072</span>,  <span class="number">0.24396233</span>, -<span class="number">0.17809424</span>,</span><br><span class="line">        <span class="number">0.19624405</span>,  <span class="number">0.21878579</span>,  <span class="number">0.20799968</span>,  <span class="number">0.12430456</span>,  <span class="number">0.18400647</span>,</span><br><span class="line">       -<span class="number">0.18915175</span>,  <span class="number">0.17806746</span>,  <span class="number">0.21123466</span>, -<span class="number">0.15553948</span>, -<span class="number">0.20221886</span>,</span><br><span class="line">       -<span class="number">0.23133394</span>, -<span class="number">0.19063245</span>,  <span class="number">0.25346702</span>, -<span class="number">0.19234048</span>, -<span class="number">0.20784694</span>,</span><br><span class="line">       -<span class="number">0.12557872</span>, -<span class="number">0.25455537</span>, -<span class="number">0.15617739</span>, -<span class="number">0.22168253</span>, -<span class="number">0.14932276</span>,</span><br><span class="line">        <span class="number">0.25438485</span>,  <span class="number">0.19298089</span>, -<span class="number">0.23894864</span>, -<span class="number">0.26424453</span>, -<span class="number">0.18523659</span>,</span><br><span class="line">        <span class="number">0.20755866</span>, -<span class="number">0.17146595</span>, -<span class="number">0.1505574</span> ,  <span class="number">0.26266149</span>,  <span class="number">0.12615746</span>,</span><br><span class="line">       -<span class="number">0.16710922</span>, -<span class="number">0.19842891</span>,  <span class="number">0.20556726</span>,  <span class="number">0.16274993</span>,  <span class="number">0.14940131</span>,</span><br><span class="line">        <span class="number">0.16785385</span>,  <span class="number">0.24925581</span>]))</span><br></pre></td></tr></table></figure>
<p>print 一個 user embedding 看看</p>
<p>In:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">user_id_2_vector = fm_embedding.get_user_embedding(user_df, <span class="string">&#x27;userId&#x27;</span>)</span><br><span class="line">print(user_id_2_vector)</span><br></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">id2vector(</span><br><span class="line"><span class="built_in">id</span>=<span class="number">1</span>,</span><br><span class="line">vector=array([ <span class="number">1.</span>        ,  <span class="number">1.57251287</span>,  <span class="number">0.31802261</span>, -<span class="number">0.05311833</span>, -<span class="number">0.51956671</span>,</span><br><span class="line">       -<span class="number">0.41219246</span>, -<span class="number">0.24985576</span>, -<span class="number">0.65933889</span>,  <span class="number">0.59215838</span>,  <span class="number">0.41595197</span>,</span><br><span class="line">        <span class="number">0.11497089</span>, -<span class="number">0.69019455</span>,  <span class="number">0.24329846</span>,  <span class="number">0.45048714</span>, -<span class="number">0.05286196</span>,</span><br><span class="line">        <span class="number">0.2054476</span> ,  <span class="number">0.30616254</span>,  <span class="number">0.30125472</span>,  <span class="number">0.40432882</span>,  <span class="number">0.26946735</span>,</span><br><span class="line">       -<span class="number">0.28497586</span>,  <span class="number">0.23863903</span>,  <span class="number">0.37228948</span>, -<span class="number">0.47429329</span>, -<span class="number">0.21513283</span>,</span><br><span class="line">       -<span class="number">0.37172595</span>, -<span class="number">0.29311907</span>,  <span class="number">0.32599026</span>, -<span class="number">0.37352464</span>, -<span class="number">0.65939361</span>,</span><br><span class="line">       -<span class="number">0.13792202</span>, -<span class="number">0.29631072</span>, -<span class="number">0.70424175</span>, -<span class="number">0.03108542</span>, -<span class="number">0.47497728</span>,</span><br><span class="line">        <span class="number">0.65669644</span>, -<span class="number">0.0536177</span> , -<span class="number">0.46591866</span>,  <span class="number">0.04575365</span>, -<span class="number">0.47661048</span>,</span><br><span class="line">       -<span class="number">0.23746635</span>, -<span class="number">0.48891699</span>, -<span class="number">0.39626658</span>,  <span class="number">0.34011322</span>,  <span class="number">0.42570654</span>,</span><br><span class="line">       -<span class="number">0.53802925</span>, -<span class="number">0.32868454</span>,  <span class="number">0.31107759</span>,  <span class="number">0.65648586</span>,  <span class="number">0.36503255</span>,</span><br><span class="line">        <span class="number">0.22789076</span>,  <span class="number">0.56083</span>   ]))</span><br></pre></td></tr></table></figure>
<h2 id="塞進-Faiss"><a href="#塞進-Faiss" class="headerlink" title="塞進 Faiss"></a>塞進 Faiss</h2><p>將每個 user-id embedding $E^{user}$ 和 iterm-id embedding $E^{item}$ 分別塞入 Faiss 建立 index</p>
<p>Item embedding:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Offline</span></span><br><span class="line"><span class="string">      push item embedding into faiss</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">all_item_id, item_embedding = <span class="built_in">zip</span>(*item_id_2_vector)</span><br><span class="line">all_item_id = np.array(all_item_id)</span><br><span class="line">item_embedding = np.array(item_embedding)</span><br><span class="line"></span><br><span class="line">movie_embedding_faiss_index = faiss.IndexFlatIP(item_embedding.shape[<span class="number">1</span>])</span><br><span class="line">movie_id_faiss_index = faiss.IndexIDMap(movie_embedding_faiss_index)</span><br><span class="line">movie_id_faiss_index.add_with_ids(item_embedding.astype(<span class="string">&#x27;float32&#x27;</span>), np.array(all_item_id))</span><br></pre></td></tr></table></figure>
<p>user embedding:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Offline</span></span><br><span class="line"><span class="string">        push user embedding into faiss</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">all_user_id, user_embedding = <span class="built_in">zip</span>(*user_id_2_vector)</span><br><span class="line">all_user_id = np.array(all_user_id)</span><br><span class="line">user_embedding = np.array(user_embedding)</span><br><span class="line">user_embedding_faiss_index = faiss.IndexFlatIP(user_embedding.shape[<span class="number">1</span>])</span><br><span class="line">user_id_faiss_index = faiss.IndexIDMap(user_embedding_faiss_index)</span><br><span class="line">user_id_faiss_index.add_with_ids(user_embedding.astype(<span class="string">&#x27;float32&#x27;</span>), np.array(all_user_id))</span><br></pre></td></tr></table></figure>
<p>IndexFlatIP 為 brute force 的 inner product，Faiss 還有其他更快的索引方式，但就得犧牲 式(6) $E^{iterm}$ $E^{user}$ inner product 的完備性。</p>
<h1 id="應用"><a href="#應用" class="headerlink" title="應用"></a>應用</h1><h2 id="I2I"><a href="#I2I" class="headerlink" title="I2I"></a>I2I</h2><h3 id="Online-Computing-Similarity"><a href="#Online-Computing-Similarity" class="headerlink" title="Online  Computing Similarity"></a>Online  Computing Similarity</h3><p>online 版的 I2I，預先將所有 item embedding $E^{item}$ 存進 Faiss，到線上實時計算 similarity score。</p>
<ol>
<li>首先取回 trigger item-ids  的 vectors</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">online</span></span><br><span class="line"><span class="string">    fetch movie ids and their corresponding vectors which user have interacted</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">item_vector_provider = VectorProvider(item_id_2_vector)</span><br><span class="line">user_seen_movie_id = [<span class="number">10</span> , <span class="number">20</span>, <span class="number">50</span>, <span class="number">70</span>]</span><br><span class="line">vectors = np.array([item_vector_provider[id_] <span class="keyword">for</span> id_ <span class="keyword">in</span> user_seen_movie_id])</span><br></pre></td></tr></table></figure>
<ul>
<li>item_vector_provider 為 item-id  $i$ 的 vector 查詢工具</li>
<li>user_seen_movie_id 為 trigger item-ids，來自 user 曾經互動過的 items</li>
</ul>
<ol>
<li>送入 faiss 查詢 I2I</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">each_match_num = <span class="number">20</span></span><br><span class="line">sim_score, movie_ids = movie_id_faiss_index.search(vectors.astype(<span class="string">&#x27;float32&#x27;</span>), each_match_num)</span><br></pre></td></tr></table></figure>
<p>print 看看所有召回的  item-ids</p>
<p>In:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(movie_ids.flatten())</span><br></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([<span class="number">3120</span>, <span class="number">2688</span>, <span class="number">3113</span>, <span class="number">3596</span>, <span class="number">1850</span>, <span class="number">2283</span>, <span class="number">2175</span>, <span class="number">2379</span>,   <span class="number">64</span>, <span class="number">2372</span>,  <span class="number">437</span>,</span><br><span class="line">       <span class="number">3835</span>, <span class="number">2149</span>, <span class="number">2950</span>, <span class="number">3623</span>,  <span class="number">315</span>, <span class="number">2589</span>, <span class="number">3616</span>, <span class="number">1686</span>, <span class="number">1752</span>, <span class="number">1322</span>, <span class="number">3667</span>,</span><br><span class="line">       <span class="number">1490</span>, <span class="number">1324</span>, <span class="number">1335</span>, <span class="number">1853</span>, <span class="number">3574</span>, <span class="number">3313</span>, <span class="number">2534</span>, <span class="number">1989</span>, <span class="number">1170</span>, <span class="number">1595</span>, <span class="number">2298</span>,</span><br><span class="line">        <span class="number">867</span>, <span class="number">2555</span>,  <span class="number">244</span>, <span class="number">2818</span>,  <span class="number">220</span>, <span class="number">2816</span>,  <span class="number">473</span>,  <span class="number">557</span>, <span class="number">2999</span>, <span class="number">3245</span>, <span class="number">1842</span>,</span><br><span class="line">       <span class="number">2839</span>, <span class="number">3881</span>, <span class="number">1850</span>, <span class="number">1039</span>, <span class="number">1316</span>,  <span class="number">701</span>, <span class="number">2909</span>, <span class="number">3679</span>,   <span class="number">83</span>, <span class="number">1177</span>, <span class="number">2175</span>,</span><br><span class="line">       <span class="number">3828</span>, <span class="number">3849</span>, <span class="number">1294</span>,  <span class="number">814</span>, <span class="number">1225</span>, <span class="number">3667</span>,  <span class="number">244</span>, <span class="number">1490</span>, <span class="number">2382</span>, <span class="number">2365</span>, <span class="number">2898</span>,</span><br><span class="line">       <span class="number">1556</span>, <span class="number">3666</span>, <span class="number">2298</span>, <span class="number">1978</span>,  <span class="number">387</span>, <span class="number">1853</span>, <span class="number">3407</span>, <span class="number">1980</span>, <span class="number">3041</span>, <span class="number">3220</span>, <span class="number">3042</span>,</span><br><span class="line">       <span class="number">1720</span>,  <span class="number">148</span>,  <span class="number">884</span>])</span><br></pre></td></tr></table></figure>
<h3 id="Offline-Computing-Similarity"><a href="#Offline-Computing-Similarity" class="headerlink" title="Offline  Computing Similarity"></a>Offline  Computing Similarity</h3><p>Offline  版的  I2I 為離線計算好所有 item-ids 的 I2I 存進  DB，線上使用時直接取用</p>
<ol>
<li>計算所有 I2I，每個 triggerＩ 召回 30 個 I</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">each_match_num = <span class="number">30</span></span><br><span class="line">sim_score, match_movie_ids = movie_id_faiss_index.search(item_embedding.astype(<span class="string">&#x27;float32&#x27;</span>), each_match_num)</span><br></pre></td></tr></table></figure>
<ol>
<li>結構化，方便持久化儲存</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">key_2_I = []</span><br><span class="line"><span class="keyword">for</span> trigger_id, match_ids, match_scores <span class="keyword">in</span> <span class="built_in">zip</span>(all_item_id, match_movie_ids, sim_score):</span><br><span class="line">    k2i = &#123;&#125;</span><br><span class="line">    k2i[<span class="string">&#x27;trigger_key&#x27;</span>] = trigger_id</span><br><span class="line">    pairs = [&#123;<span class="string">&#x27;key&#x27;</span>:id_, <span class="string">&#x27;score&#x27;</span>: s &#125; <span class="keyword">for</span> id_, s <span class="keyword">in</span> <span class="built_in">zip</span>(match_ids, match_scores) <span class="keyword">if</span> i != trigger_id]</span><br><span class="line">    k2i[<span class="string">&#x27;pairs&#x27;</span>] = pairs</span><br><span class="line">    key_2_I.append(k2i)</span><br></pre></td></tr></table></figure>
<p>print  其中一個 trigger key 的結構看看</p>
<p>In:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(key_2_I[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="string">&#x27;trigger_key&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"><span class="string">&#x27;pairs&#x27;</span>: [&#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">2562</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">8.487403</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">3359</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">7.370528</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">853</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">7.3140664</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1910</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">7.243814</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">3778</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">7.064466</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">2819</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.9514284</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">475</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.9262433</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">551</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.886018</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1262</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.7728105</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1206</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.7252</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1268</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.680213</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">265</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.674436</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">722</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.644379</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">3281</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.6103525</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">2299</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.536831</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1983</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.5208454</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1188</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.499143</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1730</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.414803</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">240</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.3780065</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">2757</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.29891</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">2731</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.23072</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">3534</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.217514</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">746</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.1960998</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">326</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.194806</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">3296</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.143187</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1058</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.070843</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">2503</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">6.015757</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1187</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">5.9964414</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">1218</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">5.899559</span>&#125;,</span><br><span class="line">  &#123;<span class="string">&#x27;key&#x27;</span>: <span class="number">2782</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">5.8903465</span>&#125;]</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>存進 DB (i.e redis) 線上 I2I 使用</li>
</ol>
<h2 id="U2I"><a href="#U2I" class="headerlink" title="U2I"></a>U2I</h2><p>U2I 為利用 user embedding  $E^{user}_u$ 在 Faiss 內搜尋與之相似的 item  Embedding $E^{item}_i$</p>
<ol>
<li>首先取回 user-id $u$ 的 embedding</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user_vector_provider = VectorProvider(user_id_2_vector)</span><br><span class="line">query_user_id = <span class="number">500</span></span><br><span class="line">u_embedding = user_vector_provider[query_user_id]</span><br></pre></td></tr></table></figure>
<ol>
<li>將 u_embedding 丟進  Faiss  找相似的 item</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    excute u2i</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">topk = <span class="number">20</span></span><br><span class="line">sim_scores, match_movie_ids = movie_id_faiss_index.search(np.expand_dims(u_embedding.astype(<span class="string">&#x27;float32&#x27;</span>), <span class="number">0</span>), topk)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; now we have socre and match item ids&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>print item-ids 看看</p>
<p>In:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(match_movie_ids.flatten())</span><br></pre></td></tr></table></figure>
<p>Out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([ <span class="number">557</span>, <span class="number">3245</span>, <span class="number">2999</span>, <span class="number">2839</span>, <span class="number">1842</span>,  <span class="number">755</span>, <span class="number">3881</span>, <span class="number">1316</span>, <span class="number">2503</span>, <span class="number">2909</span>, <span class="number">3338</span>,</span><br><span class="line">       <span class="number">1664</span>,  <span class="number">701</span>,  <span class="number">406</span>, <span class="number">3828</span>,  <span class="number">682</span>, <span class="number">2833</span>, <span class="number">2811</span>, <span class="number">3517</span>,   <span class="number">53</span>])</span><br></pre></td></tr></table></figure>
<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>因為特徵不涉及 context 類型的，所以直接取回 user embedding $E^{user}_u$ 與多個 item embedding $E^{item}$ 內積計算 score 即可</p>
<ol>
<li>拿回待排序 item-ids</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    suppose we have the following match items from different methods</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">uid = <span class="number">1000</span></span><br><span class="line">match_from_A = np.random.choice(np.array(<span class="built_in">list</span>(item_vector_provider.ID2Vector.keys())), size=<span class="number">30</span>)</span><br><span class="line">match_from_B = np.random.choice(np.array(<span class="built_in">list</span>(item_vector_provider.ID2Vector.keys())),  size=<span class="number">30</span>)</span><br><span class="line">match_from_C = np.random.choice(np.array(<span class="built_in">list</span>(item_vector_provider.ID2Vector.keys())), size=<span class="number">30</span>)</span><br><span class="line">all_match = np.hstack([match_from_A, match_from_B, match_from_C])</span><br></pre></td></tr></table></figure>
<ol>
<li>取回 user embedding  與所有待排序 item-ids 的 embedding</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    online vector provider</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">item_vector_provider = VectorProvider(item_id_2_vector)</span><br><span class="line">user_vector_provider = VectorProvider(user_id_2_vector)</span><br><span class="line">u_vector = user_vector_provider[uid]</span><br><span class="line">i_vectors = np.array([item_vector_provider[movie_id] <span class="keyword">for</span> movie_id <span class="keyword">in</span> all_match])</span><br></pre></td></tr></table></figure>
<ol>
<li>inner product 計算 score</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scores = np.dot(i_vectors, u_vector)</span><br><span class="line">predicted_prob = sigmoid(scores)</span><br></pre></td></tr></table></figure>
<p>實際上只需要 scores 即可，sigmoid  operation 為多餘的，兩者排序結果一樣，除非需要 probability 做其他操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print((all_match[np.argsort(predicted_prob)[::-<span class="number">1</span>]] == all_match[np.argsort(scores)[::-<span class="number">1</span>]]).<span class="built_in">all</span>())</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">	<span class="literal">True</span></span><br></pre></td></tr></table></figure>
<ol>
<li>查看排序後的分數</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	sorted_index = np.argsort(scores)[::-<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">for</span> idx <span class="keyword">in</span> sorted_index:</span><br><span class="line">		print(<span class="string">&quot;id:&#123;&#125;, score: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(all_match[idx], scores[idx]))</span><br><span class="line">Out:</span><br><span class="line"><span class="built_in">id</span>:<span class="number">669</span>, score: <span class="number">3.925238274386397</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">942</span>, score: <span class="number">3.5017253691160786</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3030</span>, score: <span class="number">3.2159240692623685</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">962</span>, score: <span class="number">2.5550285960726438</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3578</span>, score: <span class="number">2.2348229702588718</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">1226</span>, score: <span class="number">2.1864282823175265</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3057</span>, score: <span class="number">2.0258533377047634</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">128</span>, score: <span class="number">1.9924200765247109</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3559</span>, score: <span class="number">1.86036771697748</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3147</span>, score: <span class="number">1.8530369719991735</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">1301</span>, score: <span class="number">1.721844059286982</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">2843</span>, score: <span class="number">1.6223211152117063</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">2686</span>, score: <span class="number">1.4088124542846256</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">106</span>, score: <span class="number">1.3856411763378178</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">2117</span>, score: <span class="number">1.2064026627971622</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">1625</span>, score: <span class="number">1.0726163382326743</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3746</span>, score: <span class="number">0.9731542416184148</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3504</span>, score: <span class="number">0.9079088224687981</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">373</span>, score: <span class="number">0.8813322676009746</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3101</span>, score: <span class="number">0.8557414634608994</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">2236</span>, score: <span class="number">0.8039020559275596</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3405</span>, score: <span class="number">0.7829994401182859</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">1210</span>, score: <span class="number">0.7523971549813273</span></span><br><span class="line"><span class="built_in">id</span>:<span class="number">3255</span>, score: <span class="number">0.7294679680393682</span></span><br></pre></td></tr></table></figure>
<h1 id="Last-but-not-Least"><a href="#Last-but-not-Least" class="headerlink" title="Last but not Least"></a>Last but not Least</h1><p>本篇便於理解使用 python + pytorch 實現思路，實際上在工業界 offline 應該是在 spark 上處理數據和訓練 FM  ; online 的推薦服務為 Java 或其他。</p>
<p>在 spark 上得自己實現 FM model，個人實現,因為用在公司生產上就不能貼了， 貌似 spark 3.0 有提供了，不過版本更新一向令人頭大。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><p>推荐系统召回四模型之：全能的FM模型 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/58160982">https://zhuanlan.zhihu.com/p/58160982</a></p>
</li>
<li><p>推荐系统召回四模型之二：沉重的FFM模型 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59528983">https://zhuanlan.zhihu.com/p/59528983</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/rixwew/pytorch-fm/">https://github.com/rixwew/pytorch-fm/</a></p>
</li>
<li><p>Factorization machine implemented in PyTorch <a target="_blank" rel="noopener" href="https://www.kaggle.com/gennadylaptev/factorization-machine-implemented-in-pytorch/data">https://www.kaggle.com/gennadylaptev/factorization-machine-implemented-in-pytorch</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/362190044/answer/945591801">https://www.zhihu.com/question/362190044/answer/945591801</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Recommendation">notebook seed9D/hands-on-machine-learning</a></p>
</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>推薦系統中的瑞士刀 Factorization Machine</p><p><a href="https://seed9d.github.io/pratical-FM-model/">https://seed9d.github.io/pratical-FM-model/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>seed9D</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-02-19</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-02-19</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/recommendation-system/">recommendation system, </a><a class="link-muted" rel="tag" href="/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/">推薦系統 </a></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/what-make-XGBoost-so-effective/"><span class="level-item">透視 XGBoost(0) 總結篇</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "24fac5d0718df2565faaf847dd2ee442",
            repo: "seed9D.github.io",
            owner: "seed9D",
            clientID: "eb1cbacea1411b9a4729",
            clientSecret: "13dcde9fed4b916ec81748c4c29e8047dc4861e7",
            admin: ["seed9D"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#多才多藝的-Factorization-Machine"><span class="level-left"><span class="level-item">多才多藝的 Factorization Machine</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#特徵-label-化"><span class="level-left"><span class="level-item">特徵 label 化</span></span></a></li><li><a class="level is-mobile" href="#FM-向量召回"><span class="level-left"><span class="level-item">FM 向量召回</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Math-Background"><span class="level-left"><span class="level-item">Math Background</span></span></a></li><li><a class="level is-mobile" href="#使用說明書"><span class="level-left"><span class="level-item">使用說明書</span></span></a></li><li><a class="level is-mobile" href="#Training-Tips"><span class="level-left"><span class="level-item">Training Tips</span></span></a></li></ul></li><li><a class="level is-mobile" href="#FM-排序"><span class="level-left"><span class="level-item">FM 排序</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#使用方式"><span class="level-left"><span class="level-item">使用方式</span></span></a></li><li><a class="level is-mobile" href="#Some-Tips"><span class="level-left"><span class="level-item">Some Tips</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Implement-FM-by-Pytorch"><span class="level-left"><span class="level-item">Implement FM by Pytorch</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Bucketize-and-Label-Features"><span class="level-left"><span class="level-item">Bucketize and Label Features</span></span></a></li><li><a class="level is-mobile" href="#FM-Model"><span class="level-left"><span class="level-item">FM Model</span></span></a></li><li><a class="level is-mobile" href="#訓練過程"><span class="level-left"><span class="level-item">訓練過程</span></span></a></li><li><a class="level is-mobile" href="#組合出-E-user-E-item"><span class="level-left"><span class="level-item">組合出  $E^{user}$ $E^{item}$</span></span></a></li><li><a class="level is-mobile" href="#塞進-Faiss"><span class="level-left"><span class="level-item">塞進 Faiss</span></span></a></li></ul></li><li><a class="level is-mobile" href="#應用"><span class="level-left"><span class="level-item">應用</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#I2I"><span class="level-left"><span class="level-item">I2I</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Online-Computing-Similarity"><span class="level-left"><span class="level-item">Online  Computing Similarity</span></span></a></li><li><a class="level is-mobile" href="#Offline-Computing-Similarity"><span class="level-left"><span class="level-item">Offline  Computing Similarity</span></span></a></li></ul></li><li><a class="level is-mobile" href="#U2I"><span class="level-left"><span class="level-item">U2I</span></span></a></li><li><a class="level is-mobile" href="#排序"><span class="level-left"><span class="level-item">排序</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Last-but-not-Least"><span class="level-left"><span class="level-item">Last but not Least</span></span></a></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-18T16:00:42.000Z">2021-02-19</time></p><p class="title"><a href="/pratical-FM-model/">推薦系統中的瑞士刀 Factorization Machine</a></p><p class="categories"><a href="/categories/recommendation-system/">recommendation system</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-17T15:20:35.000Z">2021-02-17</time></p><p class="title"><a href="/what-make-XGBoost-so-effective/">透視 XGBoost(0) 總結篇</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-17T02:00:16.000Z">2021-02-17</time></p><p class="title"><a href="/XGBoost-cool-optimization/">透視 XGBoost(4) 神奇 optimization 在哪裡？</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-16T16:17:27.000Z">2021-02-17</time></p><p class="title"><a href="/XGBoost-General-Objective-Function/">透視 XGBoost(3) 蘋果樹下的 objective function</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-16T15:48:04.000Z">2021-02-16</time></p><p class="title"><a href="/XGBoost-for-classification/">透視 XGBoost(2) 圖解 Classification</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a></p></div></article></div></div><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/images/GG.jpg" alt="seed9D"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">seed9D</p><p class="is-size-6 is-block">這一生志願平凡快樂</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>左岸</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">20</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">6</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/seed9D" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/seed9D"><i class="fab fa-github"></i></a></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/logo.svg" alt="seed9D&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 seed9D</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener external nofollow">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener external nofollow">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>