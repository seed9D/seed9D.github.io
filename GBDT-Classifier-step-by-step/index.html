<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>一步步透視 GBDT Classifier - seed9D&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="seed9D&#039;s blog"><meta name="msapplication-TileImage" content="/images/seed.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="seed9D&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) &amp;#x3D; F_0(x) + \nu\sum^M_{i&amp;#x3D;1}f_i(x)$   訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}"><meta property="og:type" content="article"><meta property="og:title" content="一步步透視 GBDT Classifier"><meta property="og:url" content="https://seed9d.github.io/GBDT-Classifier-step-by-step/"><meta property="og:site_name" content="seed9D&#039;s blog"><meta property="og:description" content="TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) &amp;#x3D; F_0(x) + \nu\sum^M_{i&amp;#x3D;1}f_i(x)$   訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://i.imgur.com/bfBpmPD.png"><meta property="og:image" content="https://i.imgur.com/xUdzKge.png"><meta property="og:image" content="https://i.imgur.com/vZnfhjM.png"><meta property="og:image" content="https://i.imgur.com/0g9VDEd.png"><meta property="og:image" content="https://i.imgur.com/L6ilbXq.png"><meta property="og:image" content="https://i.imgur.com/TvWxTvg.png"><meta property="og:image" content="https://i.imgur.com/wr9RAF2.png"><meta property="og:image" content="https://i.imgur.com/Xrwhgxy.png"><meta property="og:image" content="https://i.imgur.com/MywnkEq.png"><meta property="og:image" content="https://i.imgur.com/FRJtKz0.png"><meta property="og:image" content="https://i.imgur.com/obH8T1T.png"><meta property="og:image" content="https://i.imgur.com/j7I1oVk.png"><meta property="og:image" content="https://i.imgur.com/Sasd4Ei.png"><meta property="og:image" content="https://i.imgur.com/xt6rxMA.png"><meta property="og:image" content="https://i.imgur.com/mIWXSGC.png"><meta property="og:image" content="https://i.imgur.com/levLwV4.png"><meta property="og:image" content="https://i.imgur.com/XMBr1KE.png"><meta property="og:image" content="https://i.imgur.com/qM6crwo.png"><meta property="og:image" content="https://i.imgur.com/MOaIis1.png"><meta property="og:image" content="https://i.imgur.com/yKumKHj.png"><meta property="og:image" content="https://i.imgur.com/6Lxi8mZ.png"><meta property="og:image" content="https://i.imgur.com/MHQLmVE.png"><meta property="article:published_time" content="2021-01-23T16:09:45.000Z"><meta property="article:modified_time" content="2021-01-24T12:34:26.000Z"><meta property="article:author" content="seed9D"><meta property="article:tag" content="ML"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.imgur.com/bfBpmPD.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://seed9d.github.io/GBDT-Classifier-step-by-step/"},"headline":"seed9D's blog","image":["https://i.imgur.com/bfBpmPD.png","https://i.imgur.com/xUdzKge.png","https://i.imgur.com/vZnfhjM.png","https://i.imgur.com/0g9VDEd.png","https://i.imgur.com/L6ilbXq.png","https://i.imgur.com/TvWxTvg.png","https://i.imgur.com/wr9RAF2.png","https://i.imgur.com/Xrwhgxy.png","https://i.imgur.com/MywnkEq.png","https://i.imgur.com/FRJtKz0.png","https://i.imgur.com/obH8T1T.png","https://i.imgur.com/j7I1oVk.png","https://i.imgur.com/Sasd4Ei.png","https://i.imgur.com/xt6rxMA.png","https://i.imgur.com/mIWXSGC.png","https://i.imgur.com/levLwV4.png","https://i.imgur.com/XMBr1KE.png","https://i.imgur.com/qM6crwo.png","https://i.imgur.com/MOaIis1.png","https://i.imgur.com/yKumKHj.png","https://i.imgur.com/6Lxi8mZ.png","https://i.imgur.com/MHQLmVE.png"],"datePublished":"2021-01-23T16:09:45.000Z","dateModified":"2021-01-24T12:34:26.000Z","author":{"@type":"Person","name":"seed9D"},"description":"TL;DR 訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$ $F_M(x) &#x3D; F_0(x) + \\nu\\sum^M_{i&#x3D;1}f_i(x)$   訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\\textit{gradient decent}"}</script><link rel="canonical" href="https://seed9d.github.io/GBDT-Classifier-step-by-step/"><link rel="icon" href="/images/seed.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="seed9D's blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/logo.svg" alt="seed9D&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>一步步透視 GBDT Classifier</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-01-24</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-01-24</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">24 minutes read (About 3620 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a><strong>TL;DR</strong></h1><ul>
<li>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$<ul>
<li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x)$</li>
</ul>
</li>
<li>訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}$ 的方向<ul>
<li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li>
</ul>
</li>
<li>GBDT classifier 常用的 loss function 為 cross entropy</li>
<li>classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$  跟 $probability$  有關，得將 $F(x)$ 通過 $\textit{sigmoid function }$ 獲得  probability<ul>
<li>$p = \sigma(F(x))$</li>
</ul>
</li>
</ul>
<p>GBDT 簡介在 <a href="https://seed9d.github.io/GBDT-Rregression-Tree-Step-by-Step/#GBDT-%E7%B0%A1%E4%BB%8B">一步步透視 GBDT Regression Tree</a></p>
<p>直接進入正題吧</p>
<a id="more"></a>
<h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>GBDT classification tree algorithm 跟 regression  tree 並無不同</p>
<p><img src="https://i.imgur.com/bfBpmPD.png" style="zoom:50%;" /></p>
<h2 id="Input-Dat-and-Loss-Function"><a href="#Input-Dat-and-Loss-Function" class="headerlink" title="Input Dat and Loss Function"></a>Input Dat and Loss Function</h2><blockquote>
<p>Input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p>
</blockquote>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/xUdzKge.png" alt="Data"></p>
<ul>
<li>target $y_i$: who loves Troll2</li>
<li>features of $x_i$: “likes popcorn”, “Age”,  “favorite”</li>
</ul>
<p>Our goal is using $x_i$ to predict someone like Trolls 2 or not</p>
<p>loss function 為  cross entropy</p>
<script type="math/tex; mode=display">
\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​</script><p><strong>值得注意的是，GBDT - classifier $F(x)$  輸出的是  $log(odds)$ 而不是 $probability$</strong></p>
<p>要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\textit{sigmoide function}$</p>
<script type="math/tex; mode=display">
\textit{The probability of Loving Troll 2 } = \sigma(F(x)) = p</script><ul>
<li><p>$log(odds)$ 轉換成 $probability$ 公式</p>
<script type="math/tex; mode=display">
  p = \cfrac{\exp^{log(odds)}}{1 + exp^{log(odds)}}</script></li>
</ul>
<h2 id="Step-1-Initial-model-with-a-constant-value-F-0-X"><a href="#Step-1-Initial-model-with-a-constant-value-F-0-X" class="headerlink" title="Step 1 Initial model with a constant value $F_0(X)$"></a>Step 1 Initial model with a constant value $F_0(X)$</h2><p><img src="https://i.imgur.com/vZnfhjM.png" alt="初始 data samples" style="zoom:67%;" /></p>
<p>初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可</p>
<p><img src="https://i.imgur.com/0g9VDEd.png" alt=""></p>
<p>計算完，得到 $F_0(x) = 0.69$，每個  data point 的初始 prediction 都一樣就是 $F_0(x)$。</p>
<p>$F_0(x)$ 是 $\log(odds)$ 若要計算 probability of loving Troll 2 呢？</p>
<p><img src="https://i.imgur.com/L6ilbXq.png" alt=""></p>
<p>$\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。</p>
<p><img src="https://i.imgur.com/TvWxTvg.png" alt=""></p>
<ul>
<li>ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$</li>
<li>ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving  Troll 2 </li>
</ul>
<h2 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h2><p>For $m=1$ to $M$，重複做以下的事</p>
<ol>
<li>(A) calculate residuals of $F_{m-1}(x)$</li>
<li>(B) construct new regression tree $f_m(x)$</li>
<li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li>
<li>(D) update $F_m(x)$  <strong>with new tree $f_m(x)$</strong></li>
</ol>
<hr>
<h3 id="At-Epoch-m-1"><a href="#At-Epoch-m-1" class="headerlink" title="At Epoch m = 1"></a>At Epoch m = 1</h3><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>classification 問題中  residual 為 predicted probability  與 observed label $y$ 之間的差距</p>
<p>$residual = observed - \textit{predicted probability}$</p>
<p><img src="https://i.imgur.com/wr9RAF2.png" alt="residual" style="zoom:67%;" /></p>
<ul>
<li>true label 為 1</li>
<li>false label 為 0</li>
</ul>
<p><strong>注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$  輸出的是 $log(odds)$</strong></p>
<p>計算各 data sample 的 residual  後：</p>
<p><img src="https://i.imgur.com/Xrwhgxy.png" style="zoom:67%;" /></p>
<ul>
<li>ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$</li>
<li>ep_0_prob 表 $F_0(x)$ predicted probability，$\sigma(F_0(x))$</li>
<li>ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual</li>
</ul>
<h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$</p>
<p><img src="https://i.imgur.com/MywnkEq.png" style="zoom:67%;" /></p>
<p>建樹為一般 fit regression tree  的過程，criterion 為 mean square error，假設找到的樹結構為</p>
<p><img src="https://i.imgur.com/FRJtKz0.png" alt=""></p>
<p>可以看到綠色為 leaf node，所有的 data  sample $x$ 都被歸到特定 leaf node 下</p>
<p><img src="https://i.imgur.com/obH8T1T.png" alt="" style="zoom:67%;" /></p>
<ul>
<li>ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index</li>
</ul>
<h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下</p>
<script type="math/tex; mode=display">
\cfrac{\sum residual_i}{\sum [\textit{previous probability} \times \textit{(1 - previous probability)}]}</script><ul>
<li>分子是 each leaf node 下的 data sample $x$ 的 residual 和</li>
<li>分母的 previous probability 為 $m -1$  步 GBDT 輸出的 probability $p = \sigma(F(x))$ 。<br>在這個 epoch 是指 $F_0(x)$</li>
</ul>
<p>經過計算後，每個 leaf node 輸出</p>
<p><img src="https://i.imgur.com/j7I1oVk.png" alt=""></p>
<p><img src="https://i.imgur.com/Sasd4Ei.png" style="zoom:67%;" /></p>
<ul>
<li>ep_0_prob  表 $\sigma(F_0(x))$ 計算出的 probability of loving Troll2</li>
<li>ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob</li>
<li>ep_1_leaf_output 表 data sample $x$ 在  tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值</li>
</ul>
<h4 id="D-update-F-1-x-with-new-tree-f-1-x"><a href="#D-update-F-1-x-with-new-tree-f-1-x" class="headerlink" title="(D) update $F_1(x)$  with new tree $f_1(x)$"></a>(D) update $F_1(x)$  <strong>with new tree $f_1(x)$</strong></h4><p>現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 </p>
<script type="math/tex; mode=display">
F_1(x) = F_0(x) + \textit{learning rate} \times  f_1(x)</script><p>輸出的 probability 為 $\sigma(F_1(x))$</p>
<p>令 $\textit{learnign rate = 0.8}$，得到 epoch 2 每個  data sample 的 $\log(odds)$  prediction 與 probability prediction</p>
<p><img src="https://i.imgur.com/xt6rxMA.png" style="zoom:67%;" /></p>
<ul>
<li>ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$</li>
<li>ep_1_prob 為 $F_1(x)$ 輸出的 probability $\sigma(F_1(x))$</li>
</ul>
<hr>
<h3 id="At-Epoch-m-2"><a href="#At-Epoch-m-2" class="headerlink" title="At Epoch m = 2"></a>At Epoch m = 2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_1(x)$"></a>(A) Calculate Residuals of $F_1(x)$</h4><p>計算上一步 $\textit{residual of } F_1(X)$</p>
<script type="math/tex; mode=display">
residual = observed - \textit{predicted probability}</script><p><img src="https://i.imgur.com/mIWXSGC.png" style="zoom:67%;" /></p>
<ul>
<li>ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\sigma(F_1(x))$</li>
<li>ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$</li>
</ul>
<h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>用 data sample x 的 columns “like_popcor”, “age”, “favorite_color”  擬合 ep_2_residual   build a new tree $f_2(x)$</p>
<p><img src="https://i.imgur.com/levLwV4.png" style="zoom:67%;" /></p>
<p> 假設得到 $f_2(x)$ 的樹結構：</p>
<p><img src="https://i.imgur.com/XMBr1KE.png" alt=""></p>
<p> 每個 data sample 對應的 leaf index<img src="https://i.imgur.com/qM6crwo.png" style="zoom:67%;" /></p>
<ul>
<li>ep_2_leaf_index 表 data sample  對應到 $f_2(x)$  上的 leaf node index</li>
</ul>
<h4 id="D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$"></a>(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$</h4><p>計算 $f_2(x)$ 下每個 leaf node 的輸出:</p>
<p><img src="https://i.imgur.com/MOaIis1.png" alt=""></p>
<p>對應到 data sample 上:</p>
<p><img src="https://i.imgur.com/yKumKHj.png" style="zoom:67%;" /></p>
<ul>
<li>ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node  下會有一樣的值</li>
</ul>
<h3 id="Update-F-2-x-with-New-Tree-f-2-x"><a href="#Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="Update $F_2(x)$  with New Tree $f_2(x)$"></a>Update $F_2(x)$  with New Tree $f_2(x)$</h3><p>到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為</p>
<p>$F_2(x) = F_0(x) + \nu(f_1(x) + f_2(x))$</p>
<ul>
<li>$\nu$ 為 learning rate，假設為 0.8</li>
</ul>
<p>GBDT 輸出的 probability 為 $\sigma(F_2(x))$，計算 epoch 2 的 prediction of  probability of loving troll2:</p>
<p><img src="https://i.imgur.com/6Lxi8mZ.png" style="zoom:67%;" /></p>
<ul>
<li>love_toll2: our target</li>
<li>ep_0_pre 表 $F_0(x)$</li>
<li>ep_1_leaf_output 表 data sample x​  在第一顆樹 $f_1(x)$ 的輸出值</li>
<li>ep_2_leaf_output 表 data sample x 在第二顆樹  $f_2(x)$  的輸出值</li>
<li>ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$</li>
<li>ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\sigma(F_2(x))$</li>
</ul>
<h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote>
<p>Output GBDT fitted model $F_M(x)$</p>
</blockquote>
<p>把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p>
<script type="math/tex; mode=display">
F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/MHQLmVE.png" style="zoom:67%;" /></p>
<p>$F_M(x)$ 的每棵樹 $f_m(x)$  都是去 fit  $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$</p>
<script type="math/tex; mode=display">
residual = observed - \textit{predicted probability}</script><p>所以 $F_m(x)$ 又可以寫成</p>
<script type="math/tex; mode=display">
F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><p>這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住：</p>
<ul>
<li>$F_m(x)$ 輸出 $log(odds)$</li>
<li>$residual$  的計算與 probability 有關</li>
</ul>
<h1 id="GBDT-Classifier-背後的數學"><a href="#GBDT-Classifier-背後的數學" class="headerlink" title="GBDT Classifier 背後的數學"></a>GBDT Classifier 背後的數學</h1><h3 id="Q-為什麼用-cross-entropy-做為-loss-function"><a href="#Q-為什麼用-cross-entropy-做為-loss-function" class="headerlink" title="Q: 為什麼用 cross entropy 做為 loss function ?"></a>Q: 為什麼用 cross entropy 做為 loss function ?</h3><p>在分類問題上，我們預測的是 $\textit{The probability of loving Troll 2}$  $P(Y|x)$，$\textit{}$ 以 $maximize$ $\textit{log likelihood}$ 來解 $P(Y|x)$。</p>
<p>令 GBDT - classification tree 的  probability prediction 為 $P(Y| x) = \sigma(F(x))$，則 objective function 為 </p>
<script type="math/tex; mode=display">
\textit{log (likelihood of the obersved data given the prediction) }  \\= \sum_{i=1}^N [y_i log(p) + (1-y_i)log(1-p)]</script><ul>
<li>$p = P(Y=1|x)$，表 the probability of loving movie Troll 2</li>
<li>$y_i$ : observation  of data sample $x_i$ loving Troll 2 or not<ul>
<li>$y \in \{1, 0\}$</li>
</ul>
</li>
</ul>
<p>而 $\textit{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成</p>
<script type="math/tex; mode=display">
\textit{objective function} = - \sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)</script><p>所以 $\textit{loss function} = -[y log(p) + (1-y)log(1-p)]$</p>
<p>把 loss function 用 $odds$ 表示：</p>
<script type="math/tex; mode=display">
\begin{aligned} -[y log(p) + (1-y)log(1-p)] & = -ylog(p)-(1-y)log(1-p) \\ &= -ylog(p)-log(1-p) + ylog(1-p) \\ &= -y[log(p) - log(1-p)] - log(1-p)  \\ & = -ylog(odds) - log(1-p) \\ &= -ylog(odds) + log(1 + \exp^{log(odds)}) \end{aligned}</script><ul>
<li>第三個等號 到 第四個等號用到 $odds=\cfrac{p}{1-p}$</li>
<li>第四個等號 到 第五個等號用到 $p = \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}$ 這個結論<ul>
<li>$log(1-p) = log(1- \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}) = log(\cfrac{1}{1 + \exp^{log(odds)}}) = -log(1 + \exp^{log(odds)})$</li>
</ul>
</li>
</ul>
<p>把 loss  function 表示成 odds 的好處是， $-ylog(odds) + log(1 + \exp^{log(odds)})$ 對 $log(odds)$  微分形式很簡潔</p>
<script type="math/tex; mode=display">
\cfrac{d}{d \ log(odds)} -ylog(odds) + log(1 + \exp^{log(odds)}) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}} = -y + p</script><p>loss function 對 $log(odds)$ 的微分，既可以以 $log(odds)$ 表示，也可以以 probability $p$ 表示</p>
<ul>
<li>以 $log(odds)$ 表示：  $\cfrac{d}{d log(odds)}L(y_i, p) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}$</li>
<li>以 $p$ 表示：$\cfrac{d}{d log(odds)}L(y_i, p) = -y + p$</li>
</ul>
<p>用 $p$ 表示時，loss function 對 $log(odds)$ 的微分</p>
<script type="math/tex; mode=display">
-y + p = \textit{ -(observed  - predicted) = negative residual}</script><h3 id="Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false"><a href="#Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false" class="headerlink" title="Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?"></a>Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?</h3><blockquote>
<p>來自 Step 1 的問題</p>
</blockquote>
<p>根據選定的  loss function </p>
<script type="math/tex; mode=display">
\textit{loss function} = -[y log(p) + (1-y)log(1-p)]</script><ul>
<li>$P(Y=1|x) = p$ 為出現正類的 probability</li>
<li>$y \in \{1, 0\}$</li>
</ul>
<p>將 loss  function 以 $log(odds)$ 表示</p>
<script type="math/tex; mode=display">
-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \exp^{log(odds)})]</script><p>$F_0(x)$ 為能使 $\textit{cost function}$ 最小的 $log(odds): \gamma$</p>
<script type="math/tex; mode=display">
F_0(x) = argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma) = argmin_\gamma \sum^n_{i=1} -[y_ilog(odds) + log(1 + \exp^{log(odds)})]</script><ul>
<li>$n$ 為 number of data sample $x$</li>
</ul>
<p>令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$</p>
<p>cost  function 對 $log(odds)$  微分取極值：</p>
<script type="math/tex; mode=display">
\begin{aligned}& \cfrac{d}{d log(odds)}\sum^n_{i=1} -[y_ilog(odds) + log(1 + \exp^{log(odds)})]   \\ & = \cfrac{d}{d log(odds)}\sum^{n^{(1)}}_i -(log(odds) + log(1 + exp^{log(odds)})) - \sum^{n^{(0)}}_j (0 * log(odds) + log(1 + \exp^{log(odds)}))  \\& = \cfrac{d}{dlog(odds)} -n^{(1)} \times (log(odds) + log(1 + exp^{log(odds)})) - n^{(0)} \times log(1 + \exp^{log(odds)}) \\ & =0\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned} & n^{(1)} \times(-1 + \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}})   + n^{(0)} \times(\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}) \\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\ & = - n^{(1)} + n p \\ &= 0 \end{aligned}</script><p>移項得到  $p$</p>
<script type="math/tex; mode=display">
p = \cfrac{n^{(1)}}{n}</script><script type="math/tex; mode=display">
log(odds) = \cfrac{p}{1-p} = \cfrac{n^{(1)}}{n^{(0)}}</script><p>故得證，給定 $\textit{loss function }  = -[y log(p) + (1-y)log(1-p)]$， 能使 $argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma)$  的 $\gamma$ 為 </p>
<script type="math/tex; mode=display">
log(odds)= \cfrac{n^{(1)}}{n^{(0)}}</script><script type="math/tex; mode=display">
\therefore F_0(x) = \cfrac{n^{(1)}}{n^{(0)}}</script><h3 id="Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h3><blockquote>
<p>問題來自 Step 2 - (A)</p>
</blockquote>
<p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 </p>
<script type="math/tex; mode=display">
\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum -[y log(p) + (1-y)log(1-p)]</script><ul>
<li>$y$ 為 target label</li>
<li>$p = P(Y=1|x)$ 表正類的 probability</li>
</ul>
<p>注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy </p>
<p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p>
<p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p>
<script type="math/tex; mode=display">
F_m(x) = F_{m-1}(x) + \nu f_m(x) \hAar   F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p>
<script type="math/tex; mode=display">
\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \  L(y, F_{m-1}(x)) \\ &= -(-(y - p)) \\ &= -(-(observed - \textit{predict probability})) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual</strong></p>
<h3 id="Q-leaf-node-的輸出公式怎麼來的？"><a href="#Q-leaf-node-的輸出公式怎麼來的？" class="headerlink" title="Q: leaf node 的輸出公式怎麼來的？"></a>Q: leaf node 的輸出公式怎麼來的？</h3><blockquote>
<p>問題來自 Step 2-(C)</p>
</blockquote>
<p>在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\gamma_{jm}$，使的 cost function 最小</p>
<script type="math/tex; mode=display">
\begin{aligned}\gamma_{j,m} &= argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma) \\ &= argmin_\gamma \sum_{x_i \in R_{j, m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul>
<li>$j$ 表 leaf node index</li>
<li>$m$ 表第 $m$ 步</li>
<li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li>
<li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合</li>
</ul>
<p>將 loss function 以 $log(odds)$ 表示後的 objective function</p>
<script type="math/tex; mode=display">
\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul>
<li>$-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \exp^{log(odds)})]$<ul>
<li>$F_{m-1}(x)$ 輸出為 $log(odds)$</li>
</ul>
</li>
</ul>
<p>cost function  對 $\gamma$  微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近  loss function 處理</p>
<script type="math/tex; mode=display">
\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><p>讓  2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \gamma)$ 在 $F_{m-1}(x)$ 處展開</p>
<script type="math/tex; mode=display">L(y_i, F_{m-1}(x_i) + \gamma) \approx L(y_i, F_{m-1}(x_i) ) + \cfrac{d}{d (F_{m-1}(x))} L(y_i, F_{m-1}(x_i))\gamma   + \cfrac{1}{2} \cfrac{d^2}{d (F_{m-1}(x) )^2}L(y_i, F_{m-1}(x_i))\gamma^2</script><p>將 cost function 對 $\gamma$ 微分取極值，求  $\gamma_{j,m}$</p>
<script type="math/tex; mode=display">\sum_{x_i \in R_{jm}} \cfrac{d}{d\gamma}  L(y_i, F_{m-1}(x_i), \gamma) \approx \sum_{x_i \in R_{jm}} (\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\gamma) = 0</script><p>移項得到 $\gamma$</p>
<script type="math/tex; mode=display">
\gamma = \cfrac{\sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))}</script><p>分子是  derivative of Loss function ;   分母是  second derivative of loss function</p>
<ul>
<li><p>分子部分:</p>
<script type="math/tex; mode=display">\begin{aligned} & \sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\& = \sum \cfrac{d}{d(F_{m-1}(x_i))}  \ y_i \times [F_{m-1}(x_i)  ] - log(1 + \exp^{F_{m-1}(x_i)  }) \\ &=  \sum (y_i - \cfrac{\exp^{^{F_{m-1}(x_i) }}}{1 + \exp^{^{F_{m-1}(x_i) }}} ）\\& = \sum_{x_i \in R_{jm}} (y_i -p_i) \end{aligned}</script><ul>
<li>$F_{m-1}(x_i)$  是 $m-1$  步時 $classifier$  輸出的 $log(odds)$</li>
</ul>
<ul>
<li><strong>分子部分為 $\textit{summation of residual}$</strong></li>
</ul>
</li>
<li><p>分母部分</p>
</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}& \sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) 

\\ & = \sum_{x_i \in R_{jm} } \cfrac{d^2}{d \, \ (F_{m-1}(x_i))^2} \, -[y_i  \times F_{m-1}(x_i) - log(1 + \exp^{F_{m-1}(x_i)})] 

\\ &= \sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)}-[y_i  - 
\cfrac{\exp^{^{F_{m-1}(x_i) }}}{1 + \exp^{^{F_{m-1}(x_i) }}}] 

\\ & =\sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \exp^{F_{m-1}(x_i)})^{-1} \times \exp^{F_{m-1}(x_i)}]  



\\ & = \sum_{x_i \in R_{jm} }-[(1 + \exp^{F_{m-1}(x_i)})^{-2} \exp^{F_{m-1}(x_i)}\times \exp^{F_{m-1}(x_i)} - (1+ \exp^{F_{m-1}(x_i)})^{-1}  \times \exp^{F_{m-1}(x_i)}  ]

\\&=  \sum_{x_i \in R_{jm} } \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \quad + \quad \cfrac{\exp^{F_{m-1}(x_i)}}{1+ exp^{F_{m-1}(x_i)}} = \sum_{x_i \in R_{jm} }\cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2}  +  \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \times \cfrac{(1 + \exp^{F_{m-1}(x_i)})}{1 + \exp^{F_{m-1}(x_i)}}

\\&= \sum_{x_i \in R_{jm} } \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} + \cfrac{\exp^{F_{m-1}(x_i)} + \exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} = \sum_{x_i \in R_{jm} }\cfrac{\exp^{F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} 


\\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp^{F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})} \times \cfrac{1}{(1 + \exp^{F_{m-1}(x_i)})}

\\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp^{log(odds)_i}}{1 + \exp^{log(odds)_i}} \times \cfrac{1}{1 + \exp^{log(odds)_i}}

\\ &= \sum_{x_i \in R_{jm} } p_i \times (1-p_i)
\end{aligned}</script><p>綜合分子分母，能使 $F_m(x)$  cost function 最小化的  tree  $f_m(x)$   第 $j$ 個  leaf node 輸出為</p>
<script type="math/tex; mode=display">
\gamma_{jm}= \cfrac{\sum_{x_i \in R_{jm})} (y_i - p_i)}{\sum_{x_i \in R_{jm} }(p_i \times (1- p_i))} = \cfrac{\textit{summation of  residuals }}{\textit{summantion of (previous probability $\times$ (1 - previoous probability))}}</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><h2 id="Data-Sample"><a href="#Data-Sample" class="headerlink" title="Data Sample"></a>Data Sample</h2><blockquote>
<p>learning by doing it</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">12</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">87</span>, <span class="string">&#x27;gree&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">44</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">19</span>, <span class="string">&#x27;red&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">32</span>, <span class="string">&#x27;green&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">14</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;like_popcorn&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;love_troll2&#x27;</span>]</span><br><span class="line">target = <span class="string">&#x27;love_troll2&#x27;</span></span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=jxuNLH5dXCs&amp;">Gradient Boost Part 3 (of 4): Classification</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=StWY5QWMXCw&amp;t">Gradient Boost Part 4 (of 4): Classification Details</a></p>
</li>
</ul>
<ul>
<li>Gradient Boosting In Classification: Not a Black Box Anymore! <a target="_blank" rel="noopener" href="https://blog.paperspace.com/gradient-boosting-for-classification/">https://blog.paperspace.com/gradient-boosting-for-classification/</a><ul>
<li>statquest 整理</li>
</ul>
</li>
<li>StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s">https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s</a></li>
<li>The Logit and Sigmoid Functions <a target="_blank" rel="noopener" href="https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/">https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/</a></li>
<li>Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends <a target="_blank" rel="noopener" href="https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979">https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>一步步透視 GBDT Classifier</p><p><a href="https://seed9d.github.io/GBDT-Classifier-step-by-step/">https://seed9d.github.io/GBDT-Classifier-step-by-step/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>seed9D</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-01-24</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-01-24</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/ML/">ML </a></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/word2vec-from-theory-2-implement/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">word2Vec 從原理到實現</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/GBDT-Rregression-Tree-Step-by-Step/"><span class="level-item">一步步透視 GBDT Regression Tree</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "4fdbc2cafd4c8c1e8ce25bfb510c7645",
            repo: "seed9D.github.io",
            owner: "seed9D",
            clientID: "eb1cbacea1411b9a4729",
            clientSecret: "13dcde9fed4b916ec81748c4c29e8047dc4861e7",
            admin: ["seed9D"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#TL-DR"><span class="level-left"><span class="level-item">TL;DR</span></span></a></li><li><a class="level is-mobile" href="#GBDT-Algorithm-step-by-step"><span class="level-left"><span class="level-item">GBDT Algorithm - step by step</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Input-Dat-and-Loss-Function"><span class="level-left"><span class="level-item">Input Dat and Loss Function</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Data"><span class="level-left"><span class="level-item">Data</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Step-1-Initial-model-with-a-constant-value-F-0-X"><span class="level-left"><span class="level-item">Step 1 Initial model with a constant value $F_0(X)$</span></span></a></li><li><a class="level is-mobile" href="#Step2"><span class="level-left"><span class="level-item">Step2</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#At-Epoch-m-1"><span class="level-left"><span class="level-item">At Epoch m = 1</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-Calculate-Residuals-of-F-0-x"><span class="level-left"><span class="level-item">(A) Calculate Residuals of $F_{0}(x)$</span></span></a></li><li><a class="level is-mobile" href="#B-Construct-New-Regression-Tree-f-1-x"><span class="level-left"><span class="level-item">(B) Construct New Regression Tree $f_1(x)$</span></span></a></li><li><a class="level is-mobile" href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><span class="level-left"><span class="level-item">(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</span></span></a></li><li><a class="level is-mobile" href="#D-update-F-1-x-with-new-tree-f-1-x"><span class="level-left"><span class="level-item">(D) update $F_1(x)$  with new tree $f_1(x)$</span></span></a></li></ul></li><li><a class="level is-mobile" href="#At-Epoch-m-2"><span class="level-left"><span class="level-item">At Epoch m = 2</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-Calculate-Residuals-of-F-1-x"><span class="level-left"><span class="level-item">(A) Calculate Residuals of $F_1(x)$</span></span></a></li><li><a class="level is-mobile" href="#B-Construct-New-Regression-Tree-f-2-x"><span class="level-left"><span class="level-item">(B) Construct New Regression Tree $f_2(x)$</span></span></a></li><li><a class="level is-mobile" href="#D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><span class="level-left"><span class="level-item">(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Update-F-2-x-with-New-Tree-f-2-x"><span class="level-left"><span class="level-item">Update $F_2(x)$  with New Tree $f_2(x)$</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Step-3-Output-GBDT-fitted-model"><span class="level-left"><span class="level-item">Step 3 Output GBDT fitted model</span></span></a></li></ul></li><li><a class="level is-mobile" href="#GBDT-Classifier-背後的數學"><span class="level-left"><span class="level-item">GBDT Classifier 背後的數學</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#Q-為什麼用-cross-entropy-做為-loss-function"><span class="level-left"><span class="level-item">Q: 為什麼用 cross entropy 做為 loss function ?</span></span></a></li><li><a class="level is-mobile" href="#Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false"><span class="level-left"><span class="level-item">Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?</span></span></a></li><li><a class="level is-mobile" href="#Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><span class="level-left"><span class="level-item">Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</span></span></a></li><li><a class="level is-mobile" href="#Q-leaf-node-的輸出公式怎麼來的？"><span class="level-left"><span class="level-item">Q: leaf node 的輸出公式怎麼來的？</span></span></a></li></ul></ul></li><li><a class="level is-mobile" href="#寫在最後"><span class="level-left"><span class="level-item">寫在最後</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Data-Sample"><span class="level-left"><span class="level-item">Data Sample</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-24T15:05:45.000Z">2021-01-24</time></p><p class="title"><a href="/negative-sampling-in-word2vec/">Negative Sampling 背後的數學</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-24T11:51:54.000Z">2021-01-24</time></p><p class="title"><a href="/hierarchical-softmax-in-word2vec/">Hierarchical Softmax 背後的數學</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-24T11:33:05.000Z">2021-01-24</time></p><p class="title"><a href="/NLP-language-model/">NLP Language Model</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-24T11:11:42.000Z">2021-01-24</time></p><p class="title"><a href="/word2vec-from-theory-2-implement/">word2Vec 從原理到實現</a></p><p class="categories"><a href="/categories/NLP/">NLP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-23T16:09:45.000Z">2021-01-24</time></p><p class="title"><a href="/GBDT-Classifier-step-by-step/">一步步透視 GBDT Classifier</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a></p></div></article></div></div><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/images/GG.jpg" alt="seed9D"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">seed9D</p><p class="is-size-6 is-block">這一生志願平凡快樂</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>左岸</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">3</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/seed9D" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/seed9D"><i class="fab fa-github"></i></a></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/logo.svg" alt="seed9D&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 seed9D</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener external nofollow">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener external nofollow">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>