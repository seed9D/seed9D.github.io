<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Negative Sampling 背後的數學</title>
      <link href="negative-sampling-in-word2vec/"/>
      <url>negative-sampling-in-word2vec/</url>
      
        <content type="html"><![CDATA[<p>以下用 Skip-gram 為例</p><p><img src="https://i.imgur.com/YeDgnQ9.png" style="zoom: 50%;" /></p><a id="more"></a><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h2 id="Noise-Contrastive-Estimation-NCE"><a href="#Noise-Contrastive-Estimation-NCE" class="headerlink" title="Noise Contrastive Estimation (NCE)"></a>Noise Contrastive Estimation (NCE)</h2><blockquote><p>NCE attempts to approximately maximize the log probability of the softmax output</p></blockquote><ul><li>The Noise Contractive Estimation metric intends to differentiate the target word from noise sample using a logistic regression classifier</li></ul><p><img src="https://i.imgur.com/m2gAqLM.png" style="zoom: 50%;" /></p><h3 id="從-cross-entropy-說起"><a href="#從-cross-entropy-說起" class="headerlink" title="從 cross entropy 說起"></a>從 cross entropy 說起</h3><p> True label $y_i$ is 1 only when $w_i$ is the output word:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \sum_{i=1}^V y_i \log p(w_i | w_I) = - \log p(w_O \vert w_I)</script><p>又</p><p>$p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ </p><ul><li>$\text{v}^{‘}_{w_i}$ is  vector of word $w_i$ in $W^{‘}$</li><li>$w_O$ is the output word in $V$</li><li>$w_I$ is the input word in $V$</li></ul><p>代入後</p><script type="math/tex; mode=display">\mathcal{L}_{\theta} = - \log \frac{\exp({\text{v}'_{w_O}}^{\top}{\text{v}_{w_I}})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top}{\text{v}_{w_I} })}= - {\text{v}'_{w_O}}^{\top}{\text{v}_{w_I} } + \log \sum_{i=1}^V \exp({\text{v}'_{w_i} }^{\top}{\text{v}_{w_I}})</script><p>Compute gradient of loss function w.s.t mode’s parameter $\theta$，令 $z_{IO} = {\text{v}’_{w_O}}^{\top}{\text{v}_{w_I}}$ ; $z_{Ii} = {\text{v}’_{w_i}}^{\top}{\text{v}_{w_I}}$</p><script type="math/tex; mode=display">\begin{aligned}\nabla_\theta \mathcal{L}_{\theta}&= \nabla_\theta\big( - z_{IO} + \log \sum_{i=1}^V e^{z_{Ii}} \big) \\ &= - \nabla_\theta z_{IO} + \nabla_\theta \big( \log \sum_{i=1}^V e^{z_{Ii}} \big) \\&= - \nabla_\theta z_{IO} + \frac{1}{\sum_{i=1}^V e^{z_{Ii}}} \sum_{i=1}^V e^{z_{Ii}} \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \sum_{i=1}^V \frac{e^{z_{Ii}}}{\sum_{i=1}^V e^{z_{Ii}}} \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta z_{Ii}\end{aligned}</script><p>可以看出，gradient $\nabla_{\theta}\mathcal{L}_{\theta}$是由兩部分組成 :</p><ol><li>a positive reinforcement for the target word $w_O$,  $\nabla_{\theta}z_{O}$</li><li>a negative reinforcement for all other words $w_i$, which weighted by their probability, $\sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta z_{Ii}$</li></ol><p><strong>Second term actually is just the expectation  of the gradient $\nabla_{\theta}z_{Ii}$ for all words $w_i$ in $V$。</strong></p><p>And probability distribution $Q(\tilde{w})$ could see as the distribution of noise samples</p><h3 id="NCE-sample-原理"><a href="#NCE-sample-原理" class="headerlink" title="NCE sample 原理"></a>NCE sample 原理</h3><p>According to gradient of loss function $\nabla_{\theta}\mathcal{L}_{\theta}$</p><script type="math/tex; mode=display">\nabla_{\theta}\mathcal{L}_{\theta} =- \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta z_{Ii}</script><p>Given an input word $w_I$, the correct output word is known as $w$，我們同時可以從 noise sample distribution $Q$  sample 出 $M$ 個 samples $\tilde{w}_1<br>, \tilde{w}_2, \dots, \tilde{w}_M \sim Q$ 來近似 cross entropy gradient 的後半部分</p><p>現在我們手上有個 correct output word $w$ 和 $M$ 個從 $Q$ sample 出來的 word $\tilde{w}$ ， 假設我們有一個  binary classifier 負責告訴我們，手上的 word $w$，哪個是 correct  $p(d=1 | w, w_I)$，哪個是 negative $p(d=0 | \tilde{w}, w_I)$</p><p>於是 loss function  改寫成：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) + \sum_{i=1, \tilde{w}_i \sim Q}^M \log p(d=0|\tilde{w}_i, w_I) ]</script><p>According to the law of large numbers $E_{p(x)} [ f(x)] \approx \frac{1}{n} \sum^{n}_{i=1}f(x_i)$，we could simplify:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) +  M \mathbb{E}_{\tilde{w}_i \sim Q} \log p(d=0|\tilde{w}_i, w_I)]</script><p>$p(d |w, w_I)$ 可以從 joint probability $p(d, w|w_I)$ 求</p><ol><li><p>$p(d, w | w_I) =<br>\begin{cases}<br>  \frac{1}{M+1} p(w \vert w_I) &amp; \text{if } d=1 \\<br>  \frac{M}{M+1} q(\tilde{w}) &amp; \text{if } d=0<br>  \end{cases}$</p><ul><li>$d$  is binary value</li><li>$M$ is number of samples，we have a correct output word $w$ and sample M word from distribution $Q$</li></ul></li><li><p>因爲 $p(d| w, w_I) = \frac{p(d, w, w_I)}{p(w, w_I)} = \frac{p(d, w | w_I) p(w_I)}{p(w|w_I)p(w_I)} = \frac{p(d, w| w_I)}{\sum_dp(d,w| w_I)}$</p><p>可以得出</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{Mq(\tilde{w})}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script></li></ol><ul><li>$q(\tilde{w})$ 表從 distribution $Q$ sample 出 word $\tilde{w}$ 的 probability</li></ul><p>最終 loss function of NCE</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_\theta & = - [ \log p(d=1 \vert w, w_I) +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log p(d=0|\tilde{w}_i, w_I)] \\& = - [ \log \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})} +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{p(w \vert w_I) + Mq(\tilde{w}_i)}]\end{aligned}</script><p>$p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 代入 $\mathcal{L}_{\theta}$</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\theta} &= -[log\frac{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})}}{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})+ Mq(\tilde{w})}} + \sum_{\substack{i=1 \\ \tilde{w}_i\sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})}+ Mq(\tilde{w}_i)}]\end{aligned}</script><p>可以看到 normalizer $Z(w) = {\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 依然要 sum up the entire vocabulary，實務上通常會假設 $Z(w)\approx1$，所以 $\mathcal{L}_\theta$ 簡化成:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \frac{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}})}{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}}) + Mq(\tilde{w})} +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}}) + Mq(\tilde{w}_i)}]</script><h3 id="關於-Noise-distribution-Q"><a href="#關於-Noise-distribution-Q" class="headerlink" title="關於 Noise distribution $Q$"></a>關於 Noise distribution $Q$</h3><p>關於 noise distribution $Q$，在設計的時候通常會考慮</p><ul><li>it should intuitively be very similar to the real data distribution.</li><li>it should be easy to sample from.</li></ul><h2 id="Negative-Sampling-NEG"><a href="#Negative-Sampling-NEG" class="headerlink" title="Negative Sampling (NEG)"></a>Negative Sampling (NEG)</h2><p>Negative sampling can be seen as an approximation to NCE</p><ul><li>Noise Contrastive Estimation attempts to approximately maximize the log probability of the softmax output</li><li>The objective of NEG is to <strong>learn high-quality word representations</strong> rather than achieving low perplexity on a test set, as is the goal in language modeling</li></ul><h3 id="從-NCE-說起"><a href="#從-NCE-說起" class="headerlink" title="從 NCE 說起"></a>從 NCE 說起</h3><p>NCE  $p(d|w, w_I)$ equation，$p(d=1 |w, w_I)$ 代表 word $w$ 是 output word 的機率; $p(d=0|w, w_I)$ 表 sample 出 noise word 的機率</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{Mq(\tilde{w})}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><p>NCE 假設  $p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 中的分母 $Z(w) = {\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})} = 1$，所以簡化成</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}&= \frac{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i})}{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) + Mq(\tilde{w})}\end{aligned}</script><h3 id="NEG-繼續化簡"><a href="#NEG-繼續化簡" class="headerlink" title="NEG 繼續化簡"></a>NEG 繼續化簡</h3><p><strong>NEG 繼續假設 $Nq(\tilde{w}) = 1$</strong> 式子變成</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &=\frac{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i})}{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) + 1}&=\frac{1}{1  +\exp(-{\text{v}^{'}_w}^{\top}\text{v}_{w_i})} = \sigma({\text{v}^{'}_w}^{\top}\text{v}_{w_i})\end{aligned}</script><script type="math/tex; mode=display">p(d=0|w, w_I) = 1 - \sigma({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) = \sigma(-{\text{v}^{'}_w}^{\top}\text{v}_{w_i})</script><p>最終得到 loss function </p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \sigma({\text{v}'_{w}}^\top \text{v}_{w_I}) +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \sigma(-{\text{v}'_{\tilde{w}_i}}^\top \text{v}_{w_I})]</script><p>前項是 positive sample $p(d=1 \vert w, w_I)$，後項是 M 個 negative samples $p(d=0|w, w_I) $</p><p>在 skipgram with negative sampling 上</p><ul><li>$\text{v}_{w_I}$ 表 input 的 center word  $w_I$ 的 vector，來自 $W$</li><li>$\text{v}’_{w}$ 表 output side  的一個 context word  $w$ 的 vector， 來自 $W’$</li></ul><p>實作上  skipgram 一個單位的 data sample 只會包含 一個 center word 與 一個 context word  的 pair 對 $(w_I, w_{C,j})$</p><p>參閱 [todo]</p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><ul><li>NEG 實際上目的跟 Hierarchical Softmax 不一樣，並不直接學 likelihood of correct word，而是直接學 words 的 embedding，也就是更好的 word distribution representation</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Learning Word Embedding <a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions</a><ul><li>此篇從 skip gram 講解 negative sampling</li></ul></li><li>On word embeddings - Part 2: Approximating the Softmax <a href="https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches">https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches</a><ul><li>此篇從 CBOW 講解 negative sampling</li></ul></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722">http://arxiv.org/abs/1402.3722</a></li><li>Word2Vec Tutorial Part 2 - Negative Sampling [<a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hierarchical Softmax 背後的數學</title>
      <link href="hierarchical-softmax-in-word2vec/"/>
      <url>hierarchical-softmax-in-word2vec/</url>
      
        <content type="html"><![CDATA[<h1 id="以-CBOW-為例"><a href="#以-CBOW-為例" class="headerlink" title="以 CBOW 為例"></a>以 CBOW 為例</h1><p><img src="https://i.imgur.com/Pbdqtx9.png" style="zoom:50%;" /></p><a id="more"></a><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>$\text{v}’_j$ 表 $W’$  中  j-th columns  vector</li><li>在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是  leaf node  ，跟 column vector $\text{v}’_j$ 與 word $w_i$  不是直接對應的關係</li></ul></li></ul><p>Hierarchical Softmax 優化了原始 softmax 分母 normalization 項需要對整個 vocabulary set 內的所有 word 內積，原始 softmax 如下</p><script type="math/tex; mode=display">p(w | c) = \dfrac{\text{exp}({h^\top \text{v}'_w})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><ul><li>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$，is average of input context words’ vector representation in $W$</li></ul><p>Hierarchical Softmax build a full binary tree to avoid computation over all vocabulary，示意圖：</p><p><img src="https://i.imgur.com/WnaPO5L.png" style="zoom:50%;" /></p><ol><li>每個 leaf node 代表一個 word $w_i$</li><li>Matrix $W^{‘}$  就是所有 non-leaf node $n$ 代表的 vector $\text{v}^{‘}_n$ 集合，與 word $w_i$ 無一對一對應關係</li></ol><p>Binary tree 中每個 node 分岔的 probability 是個 binary classification problem</p><p>$p(n, \text{left}) = \sigma({\text{v}’_n}^{\top} h)$</p><p>$p(n, \text{righ}) = 1 - p(\text{left},n) = \sigma(-{\text{v}’_n}^{\top} h)$</p><ul><li>$\text{v}^{‘}_{n}$  代表 node $n$ 的 vector</li></ul><p>則每個 word $w_O = w_i$ 的 generate probability $p(w_i)$ 為其從 root node 分岔到 lead node 的 probability</p><p>$p(w_i = w_O) = \prod^{L(w_O)-1}_{j=1} \sigma(\mathbb{I}_{\text{turn}}(n(w_O, j), n(w_O, j + 1) \cdot {v^{‘}_{n(w_O, j)}}^{\top}h)$</p><ul><li><p>$w_O$ 表 output word 的意思</p></li><li><p>$L(w_O)$ is the depth of the path leading to the output word $w_O$</p></li><li><p>$\mathbb{I}_{turn}$  is a specially indicator function</p><p>1 if $n(w_O, k+1)$ is the <strong>left</strong> child of $n(w_O, k)$</p><p>-1 if $n(w_O, k+1)$ is the <strong>right</strong> child of $n(w_O, k)$</p></li><li><p>$n(w, j)$ means the $j$ th unit on the path from root to the word $w$</p></li><li><p>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$ average of all context word vector</p></li></ul><h2 id="簡單的例子"><a href="#簡單的例子" class="headerlink" title="簡單的例子"></a>簡單的例子</h2><p>Ex 1: Following the path from the root to leaf node of $w_2$, we can compute the probability of $w_2$ $p(w_2)$:</p><p><img src="https://i.imgur.com/swoUyHO.png" style="zoom:50%;" /></p><p>Ex 2:</p><p><img src="https://i.imgur.com/oomh3Bv.png" alt="" style="zoom: 33%;" /></p><ul><li>$\sum^{V}_{i=1} p(w_i = w_O) = 1$</li></ul><p>probability $p(\text{cat}| context)$,  是由 $ node1  \stackrel{\text{left}}{\to} node \stackrel{\text{right}}{\to}  node 5 \stackrel{\text{right}}{\to}  cat  $  這條路徑組成</p><p>其中 context words  經過  hidden layer 後的輸出為 $h(\text{context words})$</p><h2 id="為什麼-Hierarchical-Softmax-可以減少-Time-Complexity"><a href="#為什麼-Hierarchical-Softmax-可以減少-Time-Complexity" class="headerlink" title="為什麼 Hierarchical Softmax 可以減少 Time Complexity?"></a>為什麼 Hierarchical Softmax 可以減少 Time Complexity?</h2><p>透過  Hierarchical Softmax ， 原本計算 $p(w|c)$  需要求所有 word  $w_i$ 的 vector $\text{v}_{w_i}$對 $h$ 的內積，現在只需要計算路徑上的節點數 $L(w_i) -1$，又 HS 是 full binary tree ，其最大深度為 $\log_2|V|$</p><p><strong>So we only need to evaluate at most $log_2|V|$</strong></p><h1 id="Hierarchical-Softmax-如何-update-參數"><a href="#Hierarchical-Softmax-如何-update-參數" class="headerlink" title="Hierarchical Softmax 如何 update 參數"></a>Hierarchical Softmax 如何 update 參數</h1><h2 id="Error-Funtion-of-Hierarchical-Softmax"><a href="#Error-Funtion-of-Hierarchical-Softmax" class="headerlink" title="Error Funtion of Hierarchical Softmax"></a>Error Funtion of Hierarchical Softmax</h2><p>Error function $E$ is negative log likelihood</p><p><img src="https://i.imgur.com/vYCY3GR.png" style="zoom:50%;" /></p><p><img src="https://i.imgur.com/lmxXtuJ.png" style="zoom:50%;" /></p><ul><li>$L(w_i) -1$ 表 從 root node  到 leaf node of $w_i$ 的 node number</li><li>$[ \cdot ]$表分岔判斷</li><li>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$ average of all context word vector</li></ul><p>And we use <strong>gradient decent</strong> to update $\text{v}^{‘}_j$  and $h$ in $W’$ and $W’$</p><h2 id="Calculate-the-Derivate-E-with-Regard-to-text-v-‘-top-jh"><a href="#Calculate-the-Derivate-E-with-Regard-to-text-v-‘-top-jh" class="headerlink" title="Calculate the Derivate $E$ with Regard to  $\text{v}^{‘\top}_jh$"></a>Calculate the Derivate $E$ with Regard to  $\text{v}^{‘\top}_jh$</h2><p>先求 total loss 對 $\text{v}^{‘\top}_jh$ 的 gradient</p><p><img src="https://i.imgur.com/JejjNsl.png" style="zoom:50%;" /></p><ul><li>$\sigma^{‘}(x) = \sigma(x)[1 - \sigma(x)]$</li><li>$[\log\sigma(x)]^{‘} = 1 - \sigma(x)$   ⇒    $[log(1 - \sigma(x)]^{‘} = -\sigma(x)$</li></ul><h2 id="Calculate-the-Derivate-E-with-Regard-to-text-v-‘-j"><a href="#Calculate-the-Derivate-E-with-Regard-to-text-v-‘-j" class="headerlink" title="Calculate the Derivate $E$ with Regard to  $\text{v}^{‘}_j$"></a>Calculate the Derivate $E$ with Regard to  $\text{v}^{‘}_j$</h2><p>根據 chain rule 可以求出 total loss 對 huffman tree node vector $\text{v}^{‘}_j$ 的 gradient</p><p><img src="https://i.imgur.com/RbiLvNW.png" alt="" style="zoom:50%;" /></p><h3 id="Update-Equation"><a href="#Update-Equation" class="headerlink" title="Update Equation"></a>Update Equation</h3><p><img src="https://i.imgur.com/hLzWOpf.png" style="zoom:50%;" /></p><h2 id="Calculate-the-Derivate-E-with-Regard-to-h"><a href="#Calculate-the-Derivate-E-with-Regard-to-h" class="headerlink" title="Calculate the Derivate $E$ with Regard to $h$"></a>Calculate the Derivate $E$ with Regard to $h$</h2><p> 最後求 total loss 對 hidden layer outpot $h$ 的 gradient</p><p><img src="https://i.imgur.com/BLGopRf.png" style="zoom:50%;" /></p><ul><li>$EH$: an N-dim vector, is the sum of the output vector of all words in the vocabulary, weighted by their prediction error</li></ul><h3 id="Update-Equation-1"><a href="#Update-Equation-1" class="headerlink" title="Update Equation"></a>Update Equation</h3><p>Because hidden vector $h$ is composed with all the context word $w_{I,c}$</p><p><img src="https://i.imgur.com/W4aEVVm.png" style="zoom:50%;" /></p><ul><li>$\text{v}_{w_{I,c}}$ is the input vector of c-th word in input context</li></ul><h1 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h1><p>CBOW + HS 實現 [todo]</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Language Models, Word2Vec, and Efficient Softmax Approximations <a href="https://rohanvarma.me/Word2Vec/">https://rohanvarma.me/Word2Vec/</a></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722">http://arxiv.org/abs/1402.3722</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP Language Model</title>
      <link href="NLP-language-model/"/>
      <url>NLP-language-model/</url>
      
        <content type="html"><![CDATA[<h1 id="General-Form"><a href="#General-Form" class="headerlink" title="General Form"></a>General Form</h1><script type="math/tex; mode=display">p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_1)</script><p>展開來後 $P(w_1, w_2, w_3,…,w_T) = P(w_1)P(x_2|w_1)P(w_3|w_2, w_1)…P(w_T|w_1,…w_{T-1})$</p><a id="more"></a><ul><li>EX</li></ul><p><img src="https://i.imgur.com/4JCLrKg.png" alt="" style="zoom:50%;" /></p><h1 id="Ngram-Model"><a href="#Ngram-Model" class="headerlink" title="Ngram Model"></a>Ngram Model</h1><p>根據 Markov assumption， word $i$ 只跟其包含 $i$ 的連續 $n$ 個 word 有關, 即前面 $n -1$ 個 word</p><script type="math/tex; mode=display">p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1})</script><p>其中 conditional probability 統計 corpus 內的 word frequency，可以看到 ngram 只跟前面 $n-1$ 個 word 有關</p><script type="math/tex; mode=display">p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1}) = \dfrac{count(w_{i-n+1}, \cdots , w_{i-1},w_o)}{count({w_{i-n+1}, \cdots , w_{i-1}})}</script><p>如果 $k=2$ 則稱為 bigram model :</p><script type="math/tex; mode=display">p(w_i|w_1, w_2, ... w_{i-1}) \approx p(w_i|w_{i-1})</script><p>最簡單的實作直接統計 corpus 內的 word frequency 得到 conditional probability:</p><p><img src="https://i.imgur.com/C3DgaYi.png" alt="2 gram model" style="zoom:50%;" /></p><p>但通常泛化性不足，所以用 neural network 與 softmax 來泛化 n gram conditional probability  $p(w_i \: | \: w_{i-1} , \cdots , w_{i-i+1})$</p><h1 id="Neural-Network-Implementation"><a href="#Neural-Network-Implementation" class="headerlink" title="Neural Network Implementation"></a>Neural Network Implementation</h1><p>In neural network, we achieve the same objective using the softmax layer</p><p><img src="https://i.imgur.com/evkVSOl.png" style="zoom: 67%;" /></p><p>$p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top v’_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top v’_{w_i}})}$</p><ul><li>$h$ is the output vector of the penultimate network layer</li><li>$v^{‘}_{w}$ is the output embedding of word $w$</li><li>the inner product $h^\top v’_{w_t}$ computes the unnormalized log probability of word $w_t$</li><li>the denominator normalizes log probability by sum of the log-probabilities of all word in $V$</li></ul><h1 id="Implement-Ngram-model-with-Pytorch"><a href="#Implement-Ngram-model-with-Pytorch" class="headerlink" title="Implement Ngram model with Pytorch"></a>Implement Ngram model with Pytorch</h1><h3 id="Creating-Corpus-and-Training-Pairs"><a href="#Creating-Corpus-and-Training-Pairs" class="headerlink" title="Creating Corpus and Training Pairs"></a>Creating Corpus and Training Pairs</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">test_sentence = <span class="string">&quot;&quot;&quot;When forty winters shall besiege thy brow,</span></span><br><span class="line"><span class="string">And dig deep trenches in thy beauty&#x27;s field,</span></span><br><span class="line"><span class="string">Thy youth&#x27;s proud livery so gazed on now,</span></span><br><span class="line"><span class="string">Will be a totter&#x27;d weed of small worth held:</span></span><br><span class="line"><span class="string">Then being asked, where all thy beauty lies,</span></span><br><span class="line"><span class="string">Where all the treasure of thy lusty days;</span></span><br><span class="line"><span class="string">To say, within thine own deep sunken eyes,</span></span><br><span class="line"><span class="string">Were an all-eating shame, and thriftless praise.</span></span><br><span class="line"><span class="string">How much more praise deserv&#x27;d thy beauty&#x27;s use,</span></span><br><span class="line"><span class="string">If thou couldst answer &#x27;This fair child of mine</span></span><br><span class="line"><span class="string">Shall sum my count, and make my old excuse,&#x27;</span></span><br><span class="line"><span class="string">Proving his beauty by succession thine!</span></span><br><span class="line"><span class="string">This were to be new made when thou art old,</span></span><br><span class="line"><span class="string">And see thy blood warm when thou feel&#x27;st it cold.&quot;&quot;&quot;</span>.split()</span><br><span class="line"></span><br><span class="line">trigrams = [([test_sentence[i], test_sentence[i + <span class="number">1</span>]], test_sentence[i + <span class="number">2</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_sentence) - <span class="number">2</span>)]</span><br><span class="line">vocab = <span class="built_in">set</span>(test_sentence)</span><br><span class="line">word_to_idx = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br></pre></td></tr></table></figure><h3 id="Define-N-Gram-Model"><a href="#Define-N-Gram-Model" class="headerlink" title="Define N Gram Model"></a>Define N Gram Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NGramLanguageModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NGramLanguageModel, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line"></span><br><span class="line">        out = self.linear2(out)</span><br><span class="line"></span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br></pre></td></tr></table></figure><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">10</span></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">net = NGramLanguageModel(<span class="built_in">len</span>(vocab), EMBEDDING_DIM, CONTEXT_SIZE)</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> trigrams:</span><br><span class="line">        context_idxs = torch.tensor([word_to_idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> context], dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        </span><br><span class="line">        log_probs = net(context_idxs)</span><br><span class="line">        loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.data</span><br><span class="line">    print(<span class="string">&quot;epcoh &#123;&#125; loss &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, total_loss))</span><br><span class="line">    losses.append(total_loss)</span><br></pre></td></tr></table></figure><h3 id="Fetch-Embedding"><a href="#Fetch-Embedding" class="headerlink" title="Fetch Embedding"></a>Fetch Embedding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emb = net.embeddings(torch.tensor([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vocab))])).detach().numpy()</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>on word embeddings <a href="https://ruder.io/word-embeddings-1/">https://ruder.io/word-embeddings-1/</a></li><li><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html">https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2Vec 從原理到實現</title>
      <link href="word2vec-from-theory-2-implement/"/>
      <url>word2vec-from-theory-2-implement/</url>
      
        <content type="html"><![CDATA[<p>這篇是在 notion 整理的筆記大綱，只提供綱要性的說明</p><h1 id="預備知識"><a href="#預備知識" class="headerlink" title="預備知識"></a>預備知識</h1><ul><li><p>language model： NLP 語言模型</p><p>參閱 <a href="/NLP-language-model/" title="NLP Language Model">NLP Language Model</a></p></li><li><p>huffman tree</p></li></ul><h1 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h1><h2 id="兩種網路結構"><a href="#兩種網路結構" class="headerlink" title="兩種網路結構"></a>兩種網路結構</h2><p><img src="https://i.imgur.com/395NfQN.png" alt="CBOW and skipgram" style="zoom:67%;" /></p><a id="more"></a><h3 id="Continuous-bag-of-words-CBOW-amp-Softmax"><a href="#Continuous-bag-of-words-CBOW-amp-Softmax" class="headerlink" title="Continuous bag of words (CBOW) &amp; Softmax"></a>Continuous bag of words (CBOW) &amp; Softmax</h3><blockquote><p>CBOW feeds $n$ words around the target word $w_t$ at each step</p></blockquote><script type="math/tex; mode=display">P(center|context;\theta)</script><p><img src="https://i.imgur.com/JTnC7Ko.png" alt="CBOW" style="zoom: 50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h4 id="CBOW-的-Objective-Function"><a href="#CBOW-的-Objective-Function" class="headerlink" title="CBOW 的 Objective Function"></a>CBOW 的 Objective Function</h4><p>$J_\theta = \frac{1}{V}\sum\limits_{t=1}^V\ \text{log} \space p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})$</p><p>其中</p><script type="math/tex; mode=display">p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})  = \cfrac{\exp(h^\top \text{v}^{'}_{w_{t}})}{\sum_{w_i \in V}\exp(h^\top \text{v}'_{w_i})}</script><ul><li>$n$ 表 window size</li><li>$w_t$ 表 CBOW target center word </li><li>$w_i$ 表 word $i$ in vocabulary $V$</li><li>$\text{v}_{w_i}$ 表 matrix $W$ 中，代表 $w_i$ 的那個  row vector</li><li>$\text{v}’_{w_i}$ 表 matrix $W’$ 中，代表 $w_i$ 的那個  column vector</li><li>$h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\cfrac{1}{C}(\text{v}_{w_1} + \text{v}_{w_2}+ …+ \text{v}_{w_C})^T$</li></ul><p>Because there are multiple contextual words, we construct the input vector by averaging each words’s distribution representation</p><h3 id="Skipgram-amp-Softmax"><a href="#Skipgram-amp-Softmax" class="headerlink" title="Skipgram &amp; Softmax"></a>Skipgram &amp; Softmax</h3><blockquote><p>skipgram uses the centre word to predict the surrounding words instead of using the surrounding words to predict the centre word</p></blockquote><script type="math/tex; mode=display">P(context|center; \theta)</script><p><img src="https://i.imgur.com/aYoLDWN.png" alt="Skipgram" style="zoom: 50%;" /></p><h4 id="Skipgram-的-Objective-Function"><a href="#Skipgram-的-Objective-Function" class="headerlink" title="Skipgram 的 Objective Function"></a>Skipgram 的 Objective Function</h4><script type="math/tex; mode=display">J_\theta = \frac{1}{V}\sum\limits_{t=1}^V\ \sum\limits_{-n \leq j \leq n , \neq 0} \text{log} \space p(w_{t+j} \: | \: w_t)</script><p>其中</p><script type="math/tex; mode=display">p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({h^\top \text{v}'_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><ul><li>$n$ 為 window size</li><li>$w_{t+j}$ 表 skipgram target 第 j 個 context word</li><li>$w_t$ 為 skipgram input 的 center word</li><li>skip-gram 有兩個獨立的 embedding matrix $W$ and $W^{‘}$<ul><li>$W$ : $V \times  N$ , $V$ is vocabulary size; N is vector dimension</li><li>output matrix $W^{‘}$: $N \times V$, encoding the meaning of context</li></ul></li><li>$\text{v}^{‘}_{w_i}$ is  column vector of word $w_i$ in $Ｗ^{‘}$</li><li>$h$ is the hidden layer’s output</li></ul><p>事實上，skip-gram 沒有 hidden layer, 因爲 input 只有一個 word,  $h$  就是 word embedding  $\text{v}_{w_t}$of the word $w_t$ in $W$。</p><p>所以  </p><p> $p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({\text{v}^\top_{w_t} \text{v}’_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({\text{v}^\top_{w_t} \text{v}’_{w_i}})}$</p><h2 id="兩種-loss-function-優化"><a href="#兩種-loss-function-優化" class="headerlink" title="兩種 loss function 優化"></a>兩種 loss function 優化</h2><p>原始 softmax 作為 objective function， 分母必須對所有 word $w_i$ in vocabulary 與 vector $ h$ 內積，造成運算瓶頸</p><script type="math/tex; mode=display">p(w_O | w_I) = \dfrac{\text{exp}({h^\top \text{v}'_{w_{O}}})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><p>所以 word2Vec 論文中採用兩種 objective function 的優化方案，Hierarchical Softmax  與 Negatvie Sampling</p><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>原理推導請參閱  <a href="/hierarchical-softmax-in-word2vec/" title="Hierarchical Softmax 背後的數學">Hierarchical Softmax 背後的數學</a></p><blockquote><p>Hierarchical softmax build a full binary tree to avoid computation over all vocabulary</p></blockquote><p><img src="https://i.imgur.com/Kqj88Gk.png" alt=""></p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>原理推導請參閱  <a href="/negative-sampling-in-word2vec/" title="Negative Sampling 背後的數學">Negative Sampling 背後的數學</a></p><blockquote><p>negative sampling did further simplification because it focuses on learning high-quality word embedding rather than modeling the likelihood of correct word in natural language.</p></blockquote><ul><li>In implement，negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them</li></ul><h1 id="實現-WordVec"><a href="#實現-WordVec" class="headerlink" title="實現 WordVec"></a>實現 WordVec</h1><ul><li>skip gram + softmax  [todo]</li><li>CBOW + softmax   [todo]</li><li>CBOW + hierarchical softmax  [todo]</li><li>CBOW + negatove sampling</li><li>skip gram + hierarchical softmax</li><li>skip gram + negative sampling [todo]</li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Skip gram 與 CBOW 實際上都 train 了兩個  embedding matrix $W$ and $W’$ </p><ul><li>$W:$ 在 C implement  稱作 $syn0$。</li><li>$W’$:<ul><li>若採用 hierarchical softmax 稱為  $syn1$</li><li>若採用 negative sampling 叫 $syn1neg$ </li></ul></li></ul><p>根據性質，$syn0$ 與 $syn1neg$ 是一體兩面的，本身都可以代表 word embedding，皆可使用。</p><p>而代表 $W’$ 的 $syn1$ 因爲連結的是 huffman tree 的 non leaf node，所以其本身對 word $w_i$ 沒直接意義。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p>On word embeddings - Part 1 <a href="https://ruder.io/word-embeddings-1/">https://ruder.io/word-embeddings-1/</a></p></li><li><p>On word embeddings - Part 2: Approximating the Softmax <a href="https://ruder.io/word-embeddings-softmax/">https://ruder.io/word-embeddings-softmax/</a></p></li><li><p>Learning Word Embedding <a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy</a></p></li><li><p>other</p><ul><li>Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from <a href="http://arxiv.org/abs/1411.2738">http://arxiv.org/abs/1411.2738</a></li><li>Language Models, Word2Vec, and Efficient Softmax Approximations <a href="https://rohanvarma.me/Word2Vec/">https://rohanvarma.me/Word2Vec/</a></li></ul><ul><li><p><a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a></p></li><li><p><a href="https://www.cnblogs.com/pinard/p/7160330.html">word2vec原理(一) CBOW与Skip-Gram模型基础</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a></p></li><li><p><a href="http://shomy.top/2017/07/28/word2vec-all/">Word2vec数学原理全家桶</a></p></li></ul><ul><li><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li><li>Word2Vec-知其然知其所以然 <a href="https://www.zybuluo.com/Dounm/note/591752#word2vec-%E7%9F%A5%E5%85%B6%E7%84%B6%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6">https://www.zybuluo.com/Dounm/note/591752#word2vec-知其然知其所以然</a></li></ul></li><li><p>C source code</p><ul><li>Word2Vec源码解析 <a href="https://www.cnblogs.com/neopenx/p/4571996.html">https://www.cnblogs.com/neopenx/p/4571996.html</a></li></ul></li><li><p>應用</p><ul><li>小白看Word2Vec的正确打开姿势|全部理解和应用 <a href="https://zhuanlan.zhihu.com/p/120148300">https://zhuanlan.zhihu.com/p/120148300</a></li><li>推荐系统从零单排系列(六)—Word2Vec优化策略层次Softmax与负采样 [<a href="https://zhuanlan.zhihu.com/p/66417229">https://zhuanlan.zhihu.com/p/66417229</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一步步透視 GBDT Classifier</title>
      <link href="GBDT-Classifier-step-by-step/"/>
      <url>GBDT-Classifier-step-by-step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a><strong>TL;DR</strong></h1><ul><li>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$<ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x)$</li></ul></li><li>訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}$ 的方向<ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li><li>GBDT classifier 常用的 loss function 為 cross entropy</li><li>classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$  跟 $probability$  有關，得將 $F(x)$ 通過 $\textit{sigmoid function }$ 獲得  probability<ul><li>$p = \sigma(F(x))$</li></ul></li></ul><p>GBDT 簡介在 <a href="https://seed9d.github.io/GBDT-Rregression-Tree-Step-by-Step/#GBDT-%E7%B0%A1%E4%BB%8B">一步步透視 GBDT Regression Tree</a></p><p>直接進入正題吧</p><a id="more"></a><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>GBDT classification tree algorithm 跟 regression  tree 並無不同</p><p><img src="https://i.imgur.com/bfBpmPD.png" style="zoom:50%;" /></p><h2 id="Input-Dat-and-Loss-Function"><a href="#Input-Dat-and-Loss-Function" class="headerlink" title="Input Dat and Loss Function"></a>Input Dat and Loss Function</h2><blockquote><p>Input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/xUdzKge.png" alt="Data"></p><ul><li>target $y_i$: who loves Troll2</li><li>features of $x_i$: “likes popcorn”, “Age”,  “favorite”</li></ul><p>Our goal is using $x_i$ to predict someone like Trolls 2 or not</p><p>loss function 為  cross entropy</p><script type="math/tex; mode=display">\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​</script><p><strong>值得注意的是，GBDT - classifier $F(x)$  輸出的是  $log(odds)$ 而不是 $probability$</strong></p><p>要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\textit{sigmoide function}$</p><script type="math/tex; mode=display">\textit{The probability of Loving Troll 2 } = \sigma(F(x)) = p</script><ul><li><p>$log(odds)$ 轉換成 $probability$ 公式</p><script type="math/tex; mode=display">  p = \cfrac{\exp^{log(odds)}}{1 + exp^{log(odds)}}</script></li></ul><h2 id="Step-1-Initial-model-with-a-constant-value-F-0-X"><a href="#Step-1-Initial-model-with-a-constant-value-F-0-X" class="headerlink" title="Step 1 Initial model with a constant value $F_0(X)$"></a>Step 1 Initial model with a constant value $F_0(X)$</h2><p><img src="https://i.imgur.com/vZnfhjM.png" alt="初始 data samples" style="zoom:67%;" /></p><p>初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可</p><p><img src="https://i.imgur.com/0g9VDEd.png" alt=""></p><p>計算完，得到 $F_0(x) = 0.69$，每個  data point 的初始 prediction 都一樣就是 $F_0(x)$。</p><p>$F_0(x)$ 是 $\log(odds)$ 若要計算 probability of loving Troll 2 呢？</p><p><img src="https://i.imgur.com/L6ilbXq.png" alt=""></p><p>$\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。</p><p><img src="https://i.imgur.com/TvWxTvg.png" alt=""></p><ul><li>ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$</li><li>ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving  Troll 2 </li></ul><h2 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  <strong>with new tree $f_m(x)$</strong></li></ol><hr><h3 id="At-Epoch-m-1"><a href="#At-Epoch-m-1" class="headerlink" title="At Epoch m = 1"></a>At Epoch m = 1</h3><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>classification 問題中  residual 為 predicted probability  與 observed label $y$ 之間的差距</p><p>$residual = observed - \textit{predicted probability}$</p><p><img src="https://i.imgur.com/wr9RAF2.png" alt="residual" style="zoom:67%;" /></p><ul><li>true label 為 1</li><li>false label 為 0</li></ul><p><strong>注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$  輸出的是 $log(odds)$</strong></p><p>計算各 data sample 的 residual  後：</p><p><img src="https://i.imgur.com/Xrwhgxy.png" style="zoom:67%;" /></p><ul><li>ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$</li><li>ep_0_prob 表 $F_0(x)$ predicted probability，$\sigma(F_0(x))$</li><li>ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/MywnkEq.png" style="zoom:67%;" /></p><p>建樹為一般 fit regression tree  的過程，criterion 為 mean square error，假設找到的樹結構為</p><p><img src="https://i.imgur.com/FRJtKz0.png" alt=""></p><p>可以看到綠色為 leaf node，所有的 data  sample $x$ 都被歸到特定 leaf node 下</p><p><img src="https://i.imgur.com/obH8T1T.png" alt="" style="zoom:67%;" /></p><ul><li>ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下</p><script type="math/tex; mode=display">\cfrac{\sum residual_i}{\sum [\textit{previous probability} \times \textit{(1 - previous probability)}]}</script><ul><li>分子是 each leaf node 下的 data sample $x$ 的 residual 和</li><li>分母的 previous probability 為 $m -1$  步 GBDT 輸出的 probability $p = \sigma(F(x))$ 。<br>在這個 epoch 是指 $F_0(x)$</li></ul><p>經過計算後，每個 leaf node 輸出</p><p><img src="https://i.imgur.com/j7I1oVk.png" alt=""></p><p><img src="https://i.imgur.com/Sasd4Ei.png" style="zoom:67%;" /></p><ul><li>ep_0_prob  表 $\sigma(F_0(x))$ 計算出的 probability of loving Troll2</li><li>ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob</li><li>ep_1_leaf_output 表 data sample $x$ 在  tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值</li></ul><h4 id="D-update-F-1-x-with-new-tree-f-1-x"><a href="#D-update-F-1-x-with-new-tree-f-1-x" class="headerlink" title="(D) update $F_1(x)$  with new tree $f_1(x)$"></a>(D) update $F_1(x)$  <strong>with new tree $f_1(x)$</strong></h4><p>現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 </p><script type="math/tex; mode=display">F_1(x) = F_0(x) + \textit{learning rate} \times  f_1(x)</script><p>輸出的 probability 為 $\sigma(F_1(x))$</p><p>令 $\textit{learnign rate = 0.8}$，得到 epoch 2 每個  data sample 的 $\log(odds)$  prediction 與 probability prediction</p><p><img src="https://i.imgur.com/xt6rxMA.png" style="zoom:67%;" /></p><ul><li>ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$</li><li>ep_1_prob 為 $F_1(x)$ 輸出的 probability $\sigma(F_1(x))$</li></ul><hr><h3 id="At-Epoch-m-2"><a href="#At-Epoch-m-2" class="headerlink" title="At Epoch m = 2"></a>At Epoch m = 2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_1(x)$"></a>(A) Calculate Residuals of $F_1(x)$</h4><p>計算上一步 $\textit{residual of } F_1(X)$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p><img src="https://i.imgur.com/mIWXSGC.png" style="zoom:67%;" /></p><ul><li>ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\sigma(F_1(x))$</li><li>ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>用 data sample x 的 columns “like_popcor”, “age”, “favorite_color”  擬合 ep_2_residual   build a new tree $f_2(x)$</p><p><img src="https://i.imgur.com/levLwV4.png" style="zoom:67%;" /></p><p> 假設得到 $f_2(x)$ 的樹結構：</p><p><img src="https://i.imgur.com/XMBr1KE.png" alt=""></p><p> 每個 data sample 對應的 leaf index<img src="https://i.imgur.com/qM6crwo.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_index 表 data sample  對應到 $f_2(x)$  上的 leaf node index</li></ul><h4 id="D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$"></a>(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$</h4><p>計算 $f_2(x)$ 下每個 leaf node 的輸出:</p><p><img src="https://i.imgur.com/MOaIis1.png" alt=""></p><p>對應到 data sample 上:</p><p><img src="https://i.imgur.com/yKumKHj.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node  下會有一樣的值</li></ul><h3 id="Update-F-2-x-with-New-Tree-f-2-x"><a href="#Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="Update $F_2(x)$  with New Tree $f_2(x)$"></a>Update $F_2(x)$  with New Tree $f_2(x)$</h3><p>到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為</p><p>$F_2(x) = F_0(x) + \nu(f_1(x) + f_2(x))$</p><ul><li>$\nu$ 為 learning rate，假設為 0.8</li></ul><p>GBDT 輸出的 probability 為 $\sigma(F_2(x))$，計算 epoch 2 的 prediction of  probability of loving troll2:</p><p><img src="https://i.imgur.com/6Lxi8mZ.png" style="zoom:67%;" /></p><ul><li>love_toll2: our target</li><li>ep_0_pre 表 $F_0(x)$</li><li>ep_1_leaf_output 表 data sample x​  在第一顆樹 $f_1(x)$ 的輸出值</li><li>ep_2_leaf_output 表 data sample x 在第二顆樹  $f_2(x)$  的輸出值</li><li>ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$</li><li>ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\sigma(F_2(x))$</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p>Output GBDT fitted model $F_M(x)$</p></blockquote><p>把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/MHQLmVE.png" style="zoom:67%;" /></p><p>$F_M(x)$ 的每棵樹 $f_m(x)$  都是去 fit  $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p>所以 $F_m(x)$ 又可以寫成</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><p>這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住：</p><ul><li>$F_m(x)$ 輸出 $log(odds)$</li><li>$residual$  的計算與 probability 有關</li></ul><h1 id="GBDT-Classifier-背後的數學"><a href="#GBDT-Classifier-背後的數學" class="headerlink" title="GBDT Classifier 背後的數學"></a>GBDT Classifier 背後的數學</h1><h3 id="Q-為什麼用-cross-entropy-做為-loss-function"><a href="#Q-為什麼用-cross-entropy-做為-loss-function" class="headerlink" title="Q: 為什麼用 cross entropy 做為 loss function ?"></a>Q: 為什麼用 cross entropy 做為 loss function ?</h3><p>在分類問題上，我們預測的是 $\textit{The probability of loving Troll 2}$  $P(Y|x)$，$\textit{}$ 以 $maximize$ $\textit{log likelihood}$ 來解 $P(Y|x)$。</p><p>令 GBDT - classification tree 的  probability prediction 為 $P(Y| x) = \sigma(F(x))$，則 objective function 為 </p><script type="math/tex; mode=display">\textit{log (likelihood of the obersved data given the prediction) }  \\= \sum_{i=1}^N [y_i log(p) + (1-y_i)log(1-p)]</script><ul><li>$p = P(Y=1|x)$，表 the probability of loving movie Troll 2</li><li>$y_i$ : observation  of data sample $x_i$ loving Troll 2 or not<ul><li>$y \in \{1, 0\}$</li></ul></li></ul><p>而 $\textit{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成</p><script type="math/tex; mode=display">\textit{objective function} = - \sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)</script><p>所以 $\textit{loss function} = -[y log(p) + (1-y)log(1-p)]$</p><p>把 loss function 用 $odds$ 表示：</p><script type="math/tex; mode=display">\begin{aligned} -[y log(p) + (1-y)log(1-p)] & = -ylog(p)-(1-y)log(1-p) \\ &= -ylog(p)-log(1-p) + ylog(1-p) \\ &= -y[log(p) - log(1-p)] - log(1-p)  \\ & = -ylog(odds) - log(1-p) \\ &= -ylog(odds) + log(1 + \exp^{log(odds)}) \end{aligned}</script><ul><li>第三個等號 到 第四個等號用到 $odds=\cfrac{p}{1-p}$</li><li>第四個等號 到 第五個等號用到 $p = \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}$ 這個結論<ul><li>$log(1-p) = log(1- \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}) = log(\cfrac{1}{1 + \exp^{log(odds)}}) = -log(1 + \exp^{log(odds)})$</li></ul></li></ul><p>把 loss  function 表示成 odds 的好處是， $-ylog(odds) + log(1 + \exp^{log(odds)})$ 對 $log(odds)$  微分形式很簡潔</p><script type="math/tex; mode=display">\cfrac{d}{d \ log(odds)} -ylog(odds) + log(1 + \exp^{log(odds)}) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}} = -y + p</script><p>loss function 對 $log(odds)$ 的微分，既可以以 $log(odds)$ 表示，也可以以 probability $p$ 表示</p><ul><li>以 $log(odds)$ 表示：  $\cfrac{d}{d log(odds)}L(y_i, p) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}$</li><li>以 $p$ 表示：$\cfrac{d}{d log(odds)}L(y_i, p) = -y + p$</li></ul><p>用 $p$ 表示時，loss function 對 $log(odds)$ 的微分</p><script type="math/tex; mode=display">-y + p = \textit{ -(observed  - predicted) = negative residual}</script><h3 id="Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false"><a href="#Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false" class="headerlink" title="Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?"></a>Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?</h3><blockquote><p>來自 Step 1 的問題</p></blockquote><p>根據選定的  loss function </p><script type="math/tex; mode=display">\textit{loss function} = -[y log(p) + (1-y)log(1-p)]</script><ul><li>$P(Y=1|x) = p$ 為出現正類的 probability</li><li>$y \in \{1, 0\}$</li></ul><p>將 loss  function 以 $log(odds)$ 表示</p><script type="math/tex; mode=display">-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \exp^{log(odds)})]</script><p>$F_0(x)$ 為能使 $\textit{cost function}$ 最小的 $log(odds): \gamma$</p><script type="math/tex; mode=display">F_0(x) = argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma) = argmin_\gamma \sum^n_{i=1} -[y_ilog(odds) + log(1 + \exp^{log(odds)})]</script><ul><li>$n$ 為 number of data sample $x$</li></ul><p>令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$</p><p>cost  function 對 $log(odds)$  微分取極值：</p><script type="math/tex; mode=display">\begin{aligned}& \cfrac{d}{d log(odds)}\sum^n_{i=1} -[y_ilog(odds) + log(1 + \exp^{log(odds)})]   \\ & = \cfrac{d}{d log(odds)}\sum^{n^{(1)}}_i -(log(odds) + log(1 + exp^{log(odds)})) - \sum^{n^{(0)}}_j (0 * log(odds) + log(1 + \exp^{log(odds)}))  \\& = \cfrac{d}{dlog(odds)} -n^{(1)} \times (log(odds) + log(1 + exp^{log(odds)})) - n^{(0)} \times log(1 + \exp^{log(odds)}) \\ & =0\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} & n^{(1)} \times(-1 + \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}})   + n^{(0)} \times(\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}) \\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\ & = - n^{(1)} + n p \\ &= 0 \end{aligned}</script><p>移項得到  $p$</p><script type="math/tex; mode=display">p = \cfrac{n^{(1)}}{n}</script><script type="math/tex; mode=display">log(odds) = \cfrac{p}{1-p} = \cfrac{n^{(1)}}{n^{(0)}}</script><p>故得證，給定 $\textit{loss function }  = -[y log(p) + (1-y)log(1-p)]$， 能使 $argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma)$  的 $\gamma$ 為 </p><script type="math/tex; mode=display">log(odds)= \cfrac{n^{(1)}}{n^{(0)}}</script><script type="math/tex; mode=display">\therefore F_0(x) = \cfrac{n^{(1)}}{n^{(0)}}</script><h3 id="Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h3><blockquote><p>問題來自 Step 2 - (A)</p></blockquote><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum -[y log(p) + (1-y)log(1-p)]</script><ul><li>$y$ 為 target label</li><li>$p = P(Y=1|x)$ 表正類的 probability</li></ul><p>注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy </p><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x) \hAar   F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \  L(y, F_{m-1}(x)) \\ &= -(-(y - p)) \\ &= -(-(observed - \textit{predict probability})) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual</strong></p><h3 id="Q-leaf-node-的輸出公式怎麼來的？"><a href="#Q-leaf-node-的輸出公式怎麼來的？" class="headerlink" title="Q: leaf node 的輸出公式怎麼來的？"></a>Q: leaf node 的輸出公式怎麼來的？</h3><blockquote><p>問題來自 Step 2-(C)</p></blockquote><p>在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\gamma_{jm}$，使的 cost function 最小</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &= argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma) \\ &= argmin_\gamma \sum_{x_i \in R_{j, m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$j$ 表 leaf node index</li><li>$m$ 表第 $m$ 步</li><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合</li></ul><p>將 loss function 以 $log(odds)$ 表示後的 objective function</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \exp^{log(odds)})]$<ul><li>$F_{m-1}(x)$ 輸出為 $log(odds)$</li></ul></li></ul><p>cost function  對 $\gamma$  微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近  loss function 處理</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><p>讓  2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \gamma)$ 在 $F_{m-1}(x)$ 處展開</p><script type="math/tex; mode=display">L(y_i, F_{m-1}(x_i) + \gamma) \approx L(y_i, F_{m-1}(x_i) ) + \cfrac{d}{d (F_{m-1}(x))} L(y_i, F_{m-1}(x_i))\gamma   + \cfrac{1}{2} \cfrac{d^2}{d (F_{m-1}(x) )^2}L(y_i, F_{m-1}(x_i))\gamma^2</script><p>將 cost function 對 $\gamma$ 微分取極值，求  $\gamma_{j,m}$</p><script type="math/tex; mode=display">\sum_{x_i \in R_{jm}} \cfrac{d}{d\gamma}  L(y_i, F_{m-1}(x_i), \gamma) \approx \sum_{x_i \in R_{jm}} (\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\gamma) = 0</script><p>移項得到 $\gamma$</p><script type="math/tex; mode=display">\gamma = \cfrac{\sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))}</script><p>分子是  derivative of Loss function ;   分母是  second derivative of loss function</p><ul><li><p>分子部分:</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\& = \sum \cfrac{d}{d(F_{m-1}(x_i))}  \ y_i \times [F_{m-1}(x_i)  ] - log(1 + \exp^{F_{m-1}(x_i)  }) \\ &=  \sum (y_i - \cfrac{\exp^{^{F_{m-1}(x_i) }}}{1 + \exp^{^{F_{m-1}(x_i) }}} ）\\& = \sum_{x_i \in R_{jm}} (y_i -p_i) \end{aligned}</script><ul><li>$F_{m-1}(x_i)$  是 $m-1$  步時 $classifier$  輸出的 $log(odds)$</li></ul><ul><li><strong>分子部分為 $\textit{summation of residual}$</strong></li></ul></li><li><p>分母部分</p></li></ul><script type="math/tex; mode=display">\begin{aligned}& \sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\ & = \sum_{x_i \in R_{jm} } \cfrac{d^2}{d \, \ (F_{m-1}(x_i))^2} \, -[y_i  \times F_{m-1}(x_i) - log(1 + \exp^{F_{m-1}(x_i)})] \\ &= \sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)}-[y_i  - \cfrac{\exp^{^{F_{m-1}(x_i) }}}{1 + \exp^{^{F_{m-1}(x_i) }}}] \\ & =\sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \exp^{F_{m-1}(x_i)})^{-1} \times \exp^{F_{m-1}(x_i)}]  \\ & = \sum_{x_i \in R_{jm} }-[(1 + \exp^{F_{m-1}(x_i)})^{-2} \exp^{F_{m-1}(x_i)}\times \exp^{F_{m-1}(x_i)} - (1+ \exp^{F_{m-1}(x_i)})^{-1}  \times \exp^{F_{m-1}(x_i)}  ]\\&=  \sum_{x_i \in R_{jm} } \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \quad + \quad \cfrac{\exp^{F_{m-1}(x_i)}}{1+ exp^{F_{m-1}(x_i)}} = \sum_{x_i \in R_{jm} }\cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2}  +  \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \times \cfrac{(1 + \exp^{F_{m-1}(x_i)})}{1 + \exp^{F_{m-1}(x_i)}}\\&= \sum_{x_i \in R_{jm} } \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} + \cfrac{\exp^{F_{m-1}(x_i)} + \exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} = \sum_{x_i \in R_{jm} }\cfrac{\exp^{F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp^{F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})} \times \cfrac{1}{(1 + \exp^{F_{m-1}(x_i)})}\\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp^{log(odds)_i}}{1 + \exp^{log(odds)_i}} \times \cfrac{1}{1 + \exp^{log(odds)_i}}\\ &= \sum_{x_i \in R_{jm} } p_i \times (1-p_i)\end{aligned}</script><p>綜合分子分母，能使 $F_m(x)$  cost function 最小化的  tree  $f_m(x)$   第 $j$ 個  leaf node 輸出為</p><script type="math/tex; mode=display">\gamma_{jm}= \cfrac{\sum_{x_i \in R_{jm})} (y_i - p_i)}{\sum_{x_i \in R_{jm} }(p_i \times (1- p_i))} = \cfrac{\textit{summation of  residuals }}{\textit{summantion of (previous probability $\times$ (1 - previoous probability))}}</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><h2 id="Data-Sample"><a href="#Data-Sample" class="headerlink" title="Data Sample"></a>Data Sample</h2><blockquote><p>learning by doing it</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">12</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">87</span>, <span class="string">&#x27;gree&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">44</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">19</span>, <span class="string">&#x27;red&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">32</span>, <span class="string">&#x27;green&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">14</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;like_popcorn&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;love_troll2&#x27;</span>]</span><br><span class="line">target = <span class="string">&#x27;love_troll2&#x27;</span></span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p><a href="https://www.youtube.com/watch?v=jxuNLH5dXCs&amp;">Gradient Boost Part 3 (of 4): Classification</a></p></li><li><p><a href="https://www.youtube.com/watch?v=StWY5QWMXCw&amp;t">Gradient Boost Part 4 (of 4): Classification Details</a></p></li></ul><ul><li>Gradient Boosting In Classification: Not a Black Box Anymore! <a href="https://blog.paperspace.com/gradient-boosting-for-classification/">https://blog.paperspace.com/gradient-boosting-for-classification/</a><ul><li>statquest 整理</li></ul></li><li>StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! <a href="https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s">https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s</a></li><li>The Logit and Sigmoid Functions <a href="https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/">https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/</a></li><li>Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends <a href="https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979">https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一步步透視 GBDT Regression Tree</title>
      <link href="GBDT-Rregression-Tree-Step-by-Step/"/>
      <url>GBDT-Rregression-Tree-Step-by-Step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><ul><li><p>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$</p><ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x) $</li></ul></li><li><p>訓練中的 GBDT，每棵新樹  $f_m(x)$ 都去擬合  target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient  decent}$  的方向 </p><ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li></ul><p><img src="https://i.loli.net/2021/01/23/ZWgsyMOw5LoQxFz.png" alt="Golf Boosting"></p><a id="more"></a><p>結論說完了，想看數學原理請移駕 <a href="#math">背後的數學</a>，想看例子從無到有生出 GBDT 的請到 <a href="#alg">Algorithm - step by step</a> ，想離開得請按上一頁。</p><h1 id="GBDT-簡介"><a href="#GBDT-簡介" class="headerlink" title="GBDT 簡介"></a>GBDT 簡介</h1><p>GBDT-regression tree 簡單來說，訓練時依序建立 trees $\{ f_1(x), f_2(x), …. , f_M(x)\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。</p><p>所以 GBDT - regression  tree 的訓練是 sequentially ，無法以並行訓練加速。</p><p>我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下:</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu\sum^m_{i=1}f_i(x) = F_{m-1}(x)  + \nu f_m(x)</script><ul><li>$\nu$ 為 learning rate</li></ul><p>GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。</p><p>差多遠即 residual 的概念：</p><script type="math/tex; mode=display">\textit{residual = observed - predicted}</script><p>因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual</p><p><a name="alg"><a> </p><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>Algorithm 參考了 statQuest 對 GBDT 的講解，連結放在  reference，必看！</p><p>GBDT-regression tree 擬合 algorithm：</p><h2 id="Input-Data-and-Loss-Function"><a href="#Input-Data-and-Loss-Function" class="headerlink" title="Input Data and Loss Function"></a><img src="https://i.loli.net/2021/01/23/6BuRsPyvaM7GULc.png" alt="Algorithm of GBDT">Input Data and Loss Function</h2><blockquote><p>input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/yYJ9IpX.png" alt=""></p><p>接下來都用這組簡單的數據，其中</p><ul><li>Target $y_i$ : weight which is what we want to predict</li><li>$x_i$: features 的組成有 身高，喜歡的顏色，性別</li></ul><p>目標是用 $x_i$ 的 height, favorite color, gender  來預測  $y_i$ 的 weight</p><p>loss function 為 square error </p><script type="math/tex; mode=display">L(y_i, F(x)) = \cfrac{1}{2}(\textit{observed - predicted}) ^2 = \cfrac{1}{2}(y_i^2 - F(x))^2</script><ul><li><p>square error commonly use in Regression with Gradient Boost</p></li><li><p>$\textit{observed - predicted}$  is called  $residual$</p></li><li><p>$y_i$ are observed value</p></li><li><p>$F(x)$: the function which give us the predicted value</p><ul><li><p>也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><script type="math/tex; mode=display">F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)</script></li></ul></li></ul><p><a name="step1"></a></p><h2 id="Step-1-Initialize-Model-with-a-Constant-Value"><a href="#Step-1-Initialize-Model-with-a-Constant-Value" class="headerlink" title="Step 1 Initialize Model with a Constant Value"></a>Step 1 Initialize Model with a Constant Value</h2><p>初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可</p><p><img src="https://i.imgur.com/uamGKg1.png" alt=""></p><p>取  weight 平均得到 $F_0(x) = 71.2 = \textit{average weight}$</p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  with new tree $f_m(x)$</li></ol><hr><h3 id="At-epoch-m-1"><a href="#At-epoch-m-1" class="headerlink" title="At epoch m=1"></a>At epoch m=1</h3><p><a name="step2A"></a></p><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals </p><p>​    <script type="math/tex">residuals = \textit{observed weight - predicted weight}</script></p><p>而 $F_0(x) = \textit{average weight = 71.17}$</p><p>計算 residual 後: </p><p><img src="https://i.imgur.com/FmhobDH.png" alt=""></p><ul><li>epoch_0_prediction 表 $F_0(x)$ 輸出</li><li>epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的  residual </p><p>用 columns $\textit{height, favorite, color, gender}$  預測 $residuals$ 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/Vg5cKjw.png" alt=""></p><p>建樹的過程為一般的 regression tree building 過程，target 就是 residuals。</p><p>假設我們找到分支結構是</p><p><img src="https://i.imgur.com/qmuUdME.png" alt=""></p><p>綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$  特定的 leaf node 下。</p><p><img src="https://i.imgur.com/C0P4ukP.png" alt=""></p><ul><li>epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index</li></ul><p><a name="step2C"></a></p><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值</p><p>直覺地對每個 leaf node 內的 data sample $x$  weight 值取平均，得到輸出值</p><p><img src="https://i.imgur.com/wruHARq.png" alt=""></p><p><img src="https://i.imgur.com/ausJcrf.png" alt="Untitled 9"></p><ul><li>epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值</li></ul><p><a name="step2D"></a></p><h4 id="D-Update-F-1-x-with-New-Tree-f-1-x"><a href="#D-Update-F-1-x-with-New-Tree-f-1-x" class="headerlink" title="(D) Update $F_1(x)$  with New Tree $f_1(x)$"></a>(D) Update $F_1(x)$  with New Tree $f_1(x)$</h4><p>此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$</p><p>現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \textit{learning rate } \times f_1(x)$</p><p>假設 $\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下</p><p><img src="https://i.imgur.com/1fNc1dF.png" alt=""></p><ul><li>epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值</li></ul><hr><h3 id="At-epoch-m-2"><a href="#At-epoch-m-2" class="headerlink" title="At epoch m=2"></a>At epoch m=2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_{1}(x)$"></a>(A) Calculate Residuals of $F_{1}(x)$</h4><p>m = 2 新的 residual between observed weight and $F_1(x)$如下</p><p><img src="https://i.imgur.com/NMS5YJ0.png" alt=""></p><ul><li>epoch_1_prediction 為 $F_1(x)$  的輸出</li><li>epoch_2_residual 為 observed weight  與 predicted weight $F_1(x)$ 的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>建一顆新樹擬合 epoch 2 (A)  得出的 residual</p><p><img src="https://i.imgur.com/bcUyFaW.png" alt=""></p><ul><li>epoch_2_residual 為 $f_2(x)$ 要擬合的 target</li></ul><p>假設 $f_2(x)$ 擬合後樹結構長這樣</p><p><img src="https://i.imgur.com/pJJZRC6.png" alt=""></p><p><img src="https://i.imgur.com/eNDlBn4.png" alt=""></p><ul><li>epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$</h4><p>決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個  leaf node 下的 data sample $x$ 取平均</p><p><img src="https://i.imgur.com/SM21atv.png" alt=""></p><ul><li>epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是  residual epoch_2_residual</li></ul><h4 id="D-Update-F-2-x-with-New-Tree-f-2-x"><a href="#D-Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="(D) Update $F_2(x)$  with New Tree $f_2(x)$"></a>(D) Update $F_2(x)$  with New Tree $f_2(x)$</h4><p>到目前為止我們建立了兩顆 $tree$  $f_1(x), f_2(x)$，假設  $\textit{learning rate = 0.1}$，則 $F_2(x)$ 為</p><script type="math/tex; mode=display">F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))</script><p>每個  data sample  在 $F_2(x)$ 的 predict 值如下圖：</p><p><img src="https://i.imgur.com/wbbpfNc.png" alt=""> </p><ul><li>weight: out target value</li><li>epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值</li><li>epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual</li><li>epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual</li><li>epoch_2_prediction:  $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p> Output GBDT fitted model $F_M(x)$</p></blockquote><p>Step 3 輸出模型 $F_M(x)$，把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/ZPUv6Cr.png" alt=""></p><p>$F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$  與上一步 predicted value $\hat{y}$ $F_{m-1}(x)$ 的  $residual$</p><p><a name="math"></a></p><h1 id="GBDT-Regression-背後的數學"><a href="#GBDT-Regression-背後的數學" class="headerlink" title="GBDT Regression 背後的數學"></a>GBDT Regression 背後的數學</h1><h2 id="Q-Loss-function-為什麼用-mean-square-error"><a href="#Q-Loss-function-為什麼用-mean-square-error" class="headerlink" title="Q:  Loss function 為什麼用 mean square error ?"></a>Q:  Loss function 為什麼用 mean square error ?</h2><p>​    </p><p>選擇 $\cfrac{1}{2}(\textit{observed - predicted}) ^2$  當作 loss function  的好處是對 $F(X)$ 微分的形式簡潔</p><script type="math/tex; mode=display">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script><p>其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\textit{negative  residual}$</p><p>​    $-(y_i - F(X)) = \textit{-(observed - predicted) = negative residual }  $</p><p>而我們知道 $F(X)$ 在  loss function $L(y_i, F(X))$ 的 gradient 就是 $\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$</p><h2 id="Q：為什麼初始-F-0-X-直接對-targets-取平均？"><a href="#Q：為什麼初始-F-0-X-直接對-targets-取平均？" class="headerlink" title="Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？"></a>Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？</h2><p><a href="#step1">問題來自 step 1</a></p><p>我們要找的是能使 cost function $\sum^n_{i=1}L(y_i, \gamma)$ 最小的那個輸出值 $\gamma$ 做為  $F_0(X)$。</p><p>$F_0(x) = argmin_r \sum^n_{i=1} L(y_i,\gamma)$</p><ul><li><p>$F_0(x)$ 初始化的  function，其值是常數</p></li><li><p>$\gamma$  refers to the predicted values</p></li><li><p>$n$ 是 data sample 數</p></li></ul><p><em>Proof:</em></p><p>已知</p><p>​    <script type="math/tex">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script></p><p>所以 cost function 對 $\gamma $ 微分後，是所有 sample data 跟 $\gamma$ 的 negative residual 和 為 0</p><p>​    <script type="math/tex">\sum^n_{i=1}L(y_i, \gamma) = -(y_1 - \gamma) - (y_2 - \gamma)-  ... -(y_n - \gamma) = 0</script></p><p>移項得到 $\gamma$</p><p>​    <script type="math/tex">\gamma = \cfrac{y_1 + y_2 + .... + y_n}{n}</script></p><p>正是所有 target value 的 mean，故得證</p><p>​    <script type="math/tex">F_0(x) = \gamma = \textit{the average of targets value}</script> </p><h2 id="Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h2><p><a href="#step2A">問題來自 step 2A</a></p><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum \cfrac{1}{2}(y_i^2 - F(x))^2</script><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)\hAar F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} \ L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \ L(y, F_{m-1}(x)) \\ &= -(-(y - F_{m-1}(x))) \\ &= -(-(observed - predicted )) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent  的方向和值， 也就是 residual</strong> </p><p>​    $f_m(x)$  = $\textit{gradient decent}$   = $\textit{negative gradient}$  = $residual$</p><p><strong>GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\textit{gradient  decent}$  的方向得到新的 $F(x)$</strong></p><p><strong>這也正是為什麼叫做 gradient boost</strong> 。</p><p>by the way，step 2-(A) compute residuals:</p><script type="math/tex; mode=display">r_{im} = -[\cfrac{\partial L(y_i, F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \ \textit{for i = 1,....,n}</script><ul><li>$i$: sample number</li><li>$m$: the tree we are trying to build</li></ul><h2 id="Q：個別-leaf-node-的輸出為什麼是取平均-？"><a href="#Q：個別-leaf-node-的輸出為什麼是取平均-？" class="headerlink" title="Q：個別 leaf node 的輸出為什麼是取平均 ？"></a>Q：個別 leaf node 的輸出為什麼是取平均 ？</h2><p><a href="#step2C">問題來自 step 2C </a></p><p>在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\gamma$ 使 cost function  最小</p><p>$\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma)$</p><ul><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合</li><li>$F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$  上的輸出值為 $\gamma_{j,m}$</li></ul><script type="math/tex; mode=display">\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2</script><p>直接對  $\gamma$ 微分</p><script type="math/tex; mode=display">\begin{aligned}\cfrac{d}{d \gamma} \ \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2 =  \ \sum_{x_i \in R_{j,m}}-(y_i - F_{m-1}(x_i) - \gamma) = 0\end{aligned}</script><p>移項</p><script type="math/tex; mode=display">\gamma = \cfrac{1}{n_{jm}} \sum_{x_i \in R_{j,m}}y_i - F_{m-1}(x_i)</script><ul><li>$n_{jm}$ is the number of data sample in leaf node $j$ at step $m$</li></ul><p>白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均</p><p>事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $  都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。</p><h2 id="Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？"><a href="#Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？" class="headerlink" title="Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？"></a>Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？</h2><p><a href="#step2D">問題來自 step 2D </a></p><script type="math/tex; mode=display">\begin{aligned}F_m(x) & = F_{m-1}(x) + \nu f_m(x) \\ &= F_{m-1}(x) + \nu \sum^{J_m}_{j=1}\gamma_{jm}I(x \in R_{jm})\end{aligned}</script><ul><li>$F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)$</li><li>$\nu$ learning rate</li><li>$J_m$ m 步 的 leaf node 總數</li><li>$\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出</li></ul><p>$F_m(x)$ 展開來就是</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu(f_1(x) + f_2(x)+ ...+f_m(t))</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><blockquote><p>seeing is believing</p></blockquote><p>太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙</p><p>人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶</p><blockquote><p>learning by doing it</p></blockquote><p>所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">88</span>),</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Greem&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">76</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">56</span>),</span><br><span class="line">    (<span class="number">1.8</span>, <span class="string">&#x27;Red&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">73</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Green&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">77</span>),</span><br><span class="line">    (<span class="number">1.4</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">57</span>)   </span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>]</span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><blockquote><p>always get your hands dirty</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h2 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h2><ul><li><p><a href="https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=75s">Gradient Boost Part 1 (of 4): Regression Main Ideas</a></p></li><li><p><a href="https://www.youtube.com/watch?v=2xudPOBz-vs">Gradient Boost Part 2 (of 4): Regression Details</a></p></li></ul><p>ccd comment: 上面兩個必看</p><ul><li>A Step by Step Gradient Boosting Decision Tree Example <a href="https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/">https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/</a><ul><li>真 step by step</li></ul></li><li>Gradient Boosting Decision Tree Algorithm Explained <a href="https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4">https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4</a><ul><li>including sklearn 實作</li></ul></li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li>Gradient Boosting Decision Tree <a href="http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95">http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法</a></li><li>提升树算法理论详解 <a href="https://hexinlin.top/2020/03/01/GBDT/">https://hexinlin.top/2020/03/01/GBDT/</a></li><li>梯度提升树(GBDT)原理小结 <a href="https://www.cnblogs.com/pinard/p/6140514.html">https://www.cnblogs.com/pinard/p/6140514.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="ckkb4qj5300030ce221fmb5r8/"/>
      <url>ckkb4qj5300030ce221fmb5r8/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
