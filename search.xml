<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>內容推薦 (2) Title Embedding with Keyword</title>
      <link href="title-embedding-with-keywords/"/>
      <url>title-embedding-with-keywords/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前篇 <a href="/recognize-keywords-by-entorpy/" title="內容推薦 (1) 關鍵詞識別">內容推薦 (1) 關鍵詞識別</a> 中，我們利用 entropy 從商品池的 title  中辨識出 product word  &amp; label word  </p><p>此篇，我們將利用已經辨識出的 product word &amp; label word 回頭對商品池中的商品 title 做 embedding</p><p>當然你也可以直接將所有 title 送進 Word2Vec 硬 train 一發，然後對 title 內的所有的 word vectors 取平均得到 title vector。</p><a id="more"></a><p><a href="https://seed9d.github.io/categories/recommendation-system/">推薦系統相關文章</a></p><h1 id="Weight-Keyword-Embedding"><a href="#Weight-Keyword-Embedding" class="headerlink" title="Weight Keyword Embedding"></a>Weight Keyword Embedding</h1><p>假設我們有一個 title ，我們希望能根據 word 在 title 中的重要程度將他 embedding 化，要怎麼做？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;summer fisherman hat female outdoor sun hat sun hat japanese student basin hat watch travel fishing sun hat male&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="從-CBOW-說起"><a href="#從-CBOW-說起" class="headerlink" title="從 CBOW 說起"></a>從 CBOW 說起</h2><p>CBOW  的思想是用兩側 context words 去預測中間的 center word</p><script type="math/tex; mode=display">P(center|context;\theta)</script><p>換句話說，給定 context words 集合 $w_{I,C}$， word $w_j$ 是 center word $w_O$ 的 probability 越大 是否代表 $w_j$ 在 context $C$ 中越關鍵？</p><script type="math/tex; mode=display">P(w_O = w_j |w_{I,C};\theta)</script><p>如果上面的推測成立的話，CBOW 在 Hierarchical Softmax 下的  objective function: negative log likelihood  </p><script type="math/tex; mode=display">\begin{aligned} &-\log p(w_O| w_I) = -\log \dfrac{\text{exp}({h^\top \text{v}'_O})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})} \\& = - \sum^{L(w_O)-1}_{l=1}  \log\sigma([ \cdot] h^\top \text{v}^{'}_l)\end{aligned} \tag{1}</script><ul><li>CBOW with Hierarchical Softmax 有兩個 matrix $W$  and $W’$<ul><li>$W$ 的 row vector 對應到 word $w_i$ 的 vector</li><li>$W’$ 對應的是 huffman tree non-leaf node 的 vector</li><li>參見  <a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Pytorch 實作 CBOW with Hierarchical Softmax">Pytorch 實作 CBOW with Hierarchical Softmax</a></li></ul></li><li>$\text{v}’_j$  表 output side matrix $W’$  中  j-th columns  vector，跟任何 word 沒一對一對應關係</li><li>$L(w_i) -1$ 表 huffman tree 中從 root node  到 leaf node of $w_i$ 的 node number</li><li>$[ \cdot ]$表 huffman tree 的分岔判斷<ul><li>$[ \cdot ] = 1$ 表 turn left</li><li>$[ \cdot ] = -1$ 表 turn right</li></ul></li><li>$h = \frac {1}{C} \sum^{C}_{j=1}\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$</li></ul><p>Score function $\log p(w_O| w_I)$ <strong>(沒負號)，</strong> 本質上是對 output word $w_O$ 的打分。</p><p>先改寫一下 score function，等等會用到</p><script type="math/tex; mode=display">\begin{aligned}\log p(w_O| w_I) &= \sum^{L(w_O)-1}_{l=1}\log\sigma([ \cdot] h^\top  \text{v}^{'}_l)  \\&= \sum^{L(w_O)-1}_{l=1} \log(\cfrac{1}{1+ \exp^{- [ \cdot] h^{\top} v_l^{'}}}) \\&= \sum^{L(w_O)-1}_{l=1} [\log(1) -\log(1+ \exp^{- [ \cdot] h^{\top} v_l^{'}}) ]\\& = \sum^{L(w_O)-1}_{l=1}-\log(1 + \exp^{- [ \cdot] h^{\top} v_l^{'}}) \end{aligned} \tag{2}</script><p>有了 式(2) score function ，給定一 title words 集合 $w_{T}$ 只要對 title 裡的每個 word  $w_j \in w_{T}$  ，令 $\log p(w_O=w_j|w_I = w_{T, \lnot j})$，進行打分即可得到每個  word $w_j$ 在 title 裡的重要程度 </p><script type="math/tex; mode=display">\text{weight}_j = \log p(w_O=w_j|w_I = w_{T, \lnot j}) \tag{3}</script><p>而我們要的 title embedding 即 weighted sum of words in title</p><script type="math/tex; mode=display">\text{v}_{\text{title}} = \sum_{w_j \in w_T}\text{weight}_j \times \text{v}_{w_{j}} \tag{4}</script><ul><li>$w_T$: 某 title 的 word 集合</li><li>$\text{v}_{w_{j}}$: word  $w_j$ 對應 matrix $W$ 中 row vector</li></ul><h2 id="Gensim-實作"><a href="#Gensim-實作" class="headerlink" title="Gensim 實作"></a>Gensim 實作</h2><h3 id="Train-CBOW-HS"><a href="#Train-CBOW-HS" class="headerlink" title="Train CBOW + HS"></a>Train CBOW + HS</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line">w2v_model = Word2Vec(</span><br><span class="line">        min_count=<span class="number">3</span>,</span><br><span class="line">        window=<span class="number">5</span>,</span><br><span class="line">        size=<span class="number">100</span>,</span><br><span class="line">        alpha=<span class="number">0.005</span>,</span><br><span class="line">        min_alpha=<span class="number">0.0007</span>,</span><br><span class="line">        hs=<span class="number">1</span>,</span><br><span class="line">        sg=<span class="number">0</span>,</span><br><span class="line">        workers=<span class="number">4</span>,</span><br><span class="line">        batch_words=<span class="number">100</span>,</span><br><span class="line">        cbow_mean = <span class="number">1</span></span><br><span class="line">    )</span><br><span class="line">w2v_model.build_vocab(corpus) <span class="comment"># build huffman tree</span></span><br><span class="line">w2v_model.train(</span><br><span class="line">        corpus,</span><br><span class="line">        total_examples=w2v_model.corpus_count,</span><br><span class="line">        epochs=<span class="number">50</span>,</span><br><span class="line">        report_delay=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>corpus 裡的每個 title，應該已經先行合併 bigram and trigram 的 product &amp; label 詞 ，最好可以去除無用詞，如下 sentence，某些 words 已合併 :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;hyuna_style ins cute wild sunscreen female hand sleeves arm_guard ice_silk sleeves driving anti_ultraviolet ice gloves tide&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="Scoring-Words-in-Title"><a href="#Scoring-Words-in-Title" class="headerlink" title="Scoring Words in Title"></a>Scoring Words in Title</h3><p>訓練完 model  後，對每個 title 內的 words 重要性進行評分:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_log_probs</span>(<span class="params">model, target_w, context_embd: np.ndarray</span>)-&gt; np.ndarray:</span></span><br><span class="line">    turns = (-<span class="number">1.0</span>) ** target_w.code</span><br><span class="line">    path_embd = model.trainables.syn1[target_w.point]</span><br><span class="line">    log_probs = -np.logaddexp(<span class="number">0</span>, -turns * np.dot(context_embd, path_embd.T))</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(log_probs)</span><br></pre></td></tr></table></figure><ul><li>為 式 (2)  的實現，實際上就是 gensim Word2Vec 內的  score_cbow_pair</li><li>word 在 huffman tree  的 path code 是 0/1 code，使用時須轉換成 -1 or 1</li><li>model.trainables.syn1 即 $W’$ ，存放  huffman tree non-leaf node 的 vector</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cal_keyword_score</span>(<span class="params">model, sentence:List[<span class="built_in">str</span>]</span>) -&gt; Dict[str, float]:</span></span><br><span class="line">    word_vocabs = [model.wv.vocab[w] <span class="keyword">for</span> w <span class="keyword">in</span> sentence <span class="keyword">if</span> w <span class="keyword">in</span> model.wv.vocab]</span><br><span class="line">    </span><br><span class="line">    word_importance = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> pos_center, center_w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_vocabs):</span><br><span class="line">        context_w_indices = [w.index <span class="keyword">for</span> pos_w, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_vocabs) <span class="keyword">if</span> pos_center != pos_w]</span><br><span class="line">        context_embed = np.mean(model.wv.vectors[context_w_indices], axis=<span class="number">0</span>)</span><br><span class="line">        log_probs = cal_log_probs(model, center_w, context_embed)</span><br><span class="line">        </span><br><span class="line">        center_w_term = w2v_model.wv.index2word[center_w.index]</span><br><span class="line">        word_importance[center_w_term] = word_importance.get(center_w_term, <span class="number">0</span>) + log_probs</span><br><span class="line">    <span class="keyword">return</span> word_importance</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_keyword_score</span>(<span class="params">model, sentence: List[<span class="built_in">str</span>]</span>) -&gt; np.ndarray:</span></span><br><span class="line">    word_importance = _cal_keyword_score(model, sentence)</span><br><span class="line">    ds = pd.Series(word_importance).sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    scalar = MinMaxScaler(feature_range=(<span class="number">0.1</span>, <span class="number">1</span>))</span><br><span class="line">    array = ds.to_numpy()</span><br><span class="line">    array = scalar.fit_transform(array.reshape(array.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">    ds = pd.Series(array.reshape(-<span class="number">1</span>, ), index=ds.index)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><ul><li>model.wv.vectors 存放 $W$，即訓練完後每個  word 的 vector</li><li>MinMaxScaler: 縮放到 0.1  到  1 是為了方便觀察</li></ul><p>使用方式如下</p><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sent = corpus_with_bigram_trigram[<span class="number">7676</span>]</span><br><span class="line">ds = cal_keyword_score(w2v_model, sent)</span><br><span class="line">print(sent), print(ds)</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;haining&#x27;</span>, <span class="string">&#x27;leather&#x27;</span>, <span class="string">&#x27;male&#x27;</span>, <span class="string">&#x27;stand_collar&#x27;</span>, <span class="string">&#x27;middle_aged&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>, <span class="string">&#x27;fur&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;winter&#x27;</span>, <span class="string">&#x27;cashmere&#x27;</span>, <span class="string">&#x27;thick&#x27;</span>, <span class="string">&#x27;money&#x27;</span>, <span class="string">&#x27;father_loaded&#x27;</span>]</span><br><span class="line">coat             <span class="number">1.000000</span></span><br><span class="line">leather          <span class="number">0.874738</span></span><br><span class="line">fur              <span class="number">0.861750</span></span><br><span class="line">middle_aged      <span class="number">0.812752</span></span><br><span class="line">winter           <span class="number">0.773609</span></span><br><span class="line">male             <span class="number">0.734654</span></span><br><span class="line">stand_collar     <span class="number">0.699505</span></span><br><span class="line">thick            <span class="number">0.676800</span></span><br><span class="line">cashmere         <span class="number">0.642869</span></span><br><span class="line">one              <span class="number">0.546631</span></span><br><span class="line">haining          <span class="number">0.457806</span></span><br><span class="line">father_loaded    <span class="number">0.393533</span></span><br><span class="line">money            <span class="number">0.100000</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><h3 id="Weighted-Sum-of-Word-Vectors"><a href="#Weighted-Sum-of-Word-Vectors" class="headerlink" title="Weighted Sum of Word Vectors"></a>Weighted Sum of Word Vectors</h3><p>從 w2v_model 中取出某 title 內所有 words 的 vector 做 weighed sum</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted_sum_w2v</span>(<span class="params">w2v_model, ds: pd.Series</span>) -&gt; np.ndarray:</span></span><br><span class="line">    ds_  = ds.copy() / <span class="built_in">sum</span>(ds)</span><br><span class="line">    w2v = w2v_model.wv[ds_.index]</span><br><span class="line">    weights = np.expand_dims(ds_.to_numpy(), <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>((w2v * weights), axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>得出的 title vector is un-normalized ，要使用前得先  L2-norm</li></ul><p>參閱 bible 做的示例 notebook <a href="https://github.com/seed9D/hands-on-machine-learning/blob/main/Embedding/embeddng_with_keyword_CBOW_HS.ipynb">seed9D/hands-on-machine-learning</a></p><h1 id="Title-Embedding-應用"><a href="#Title-Embedding-應用" class="headerlink" title="Title Embedding 應用"></a>Title Embedding 應用</h1><p>title embedding 最直覺的應用是 content I2I，用戶點擊了 商品 A，我們就可以透過 商品 A 的 title vector 召回 TopK 個最相似 title 的商品推薦給他。</p><p>而 title embedding with keyword weighting 中，我們將 product word 與 label word 在 title 中的重要程度進行 weighted sum，能更準確的表達 title 的意思，不再只是簡單的對 word vector 取平均，連一些無用詞的 vector 也混進去。</p><p>不過 title embedding 在推薦的效果不如利用用戶交互數據訓練出來的 embedding，但因爲每個商品一定會有 title， 很適合作為商品冷啟動召回策略之一使用。在我負責的推薦應用裡，也是利用 title embedding 關聯新商品到有交互數據的舊商品上後讓新商品取得曝光機會。</p><p>title embedding 結合 label 詞 &amp; product 詞的另一個業務應用就是卡片式的主題推薦，類似淘寶上的一個頁面就講一個購物主題，選定一個主題 (ex: 旅遊)與某樣你曾經互動過商品進行推薦</p><p><img src="https://i.imgur.com/EXzpEKq.jpg" alt="" style="zoom:33%;" /></p><p>這個算法側的實作不難，留到下次說吧</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>【不可思议的Word2Vec】 3.提取关键词 <a href="https://spaces.ac.cn/archives/4316">https://spaces.ac.cn/archives/4316</a><ul><li>以 skipgram  角度計算</li></ul></li><li>my post<ul><li><a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Pytorch 實作 CBOW with Hierarchical Softmax">Pytorch 實作 CBOW with Hierarchical Softmax</a></li><li><a href="/hierarchical-softmax-in-word2vec/" title="Hierarchical Softmax 背後的數學">Hierarchical Softmax 背後的數學</a></li></ul></li><li>gensim<ul><li><a href="https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html">https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html</a></li><li>Gensim Word2Vec Tutorial [<a href="https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial">https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> recommendation system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation system </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>內容推薦 (1) 關鍵詞識別</title>
      <link href="recognize-keywords-by-entorpy/"/>
      <url>recognize-keywords-by-entorpy/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="從內容召回說起"><a href="#從內容召回說起" class="headerlink" title="從內容召回說起"></a>從內容召回說起</h2><p>電商推薦系統多路召回中，通常會有一路召回商品叫內容召回 $\text{content I2I}$</p><p>content I2I 與其他根據用戶行為的召回不同， 最簡單的 content I2I 可以根據 A 商品 title 內容找出其他與 A 相似的 title 的商品，這給了他很強的推薦解釋性</p><p>對於商品冷啟動或者系統冷啟動來說，content I2I 可以在沒有用戶交互訊息時，就可以進行推薦，也不失為一種冷啟動方案。</p><p>在萬物皆可 Embedding 的今天，content I2I 只要把所有商品的 title 送進 word2Vec  硬 train 一發也就完事了</p><p>當然要是這麼簡單，也就不會有這篇了</p><p><img src="https://i.imgur.com/hz7T95F.png" alt=""></p><a id="more"></a><p><a href="https://seed9d.github.io/categories/recommendation-system/">推薦系統相關文章</a></p><h2 id="一言難盡的商品-title"><a href="#一言難盡的商品-title" class="headerlink" title="一言難盡的商品 title"></a>一言難盡的商品 title</h2><p>我司的電商商品 title 為中翻英淘寶商品 title 而來，基本上毫無文法可言</p><p><img src="https://i.imgur.com/WvJYlRV.png" alt=""></p><p>如果用 word2Vec  硬做一發，再以 doc2vector 的思路融合成 sentence vector ，肯定會加入某些糟糕詞彙的 vector。</p><p>諸如此類的怪異詞彙：</p><ul><li>EX: “real time” (應該是實時發貨？) , liu haichang(劉海夾??), two yuan(兩元？), yiwu(義烏？)</li></ul><p>為了讓 vector 能更好的表達句子 title，加上組內對於商品關鍵字有需求，於是就有了以下商品 title  挖掘出中 產品詞 與 標籤詞的識別任務 </p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="關鍵詞識別"><a href="#關鍵詞識別" class="headerlink" title="關鍵詞識別"></a>關鍵詞識別</h2><p>先解釋一下，什麼是產品詞，什麼是標籤詞</p><p>以下是我自己的定義：</p><p><img src="https://i.imgur.com/9ZV7aF5.png" alt="商品 title"></p><p>所有商品都會有自己的 tilte，但肯定會有一個 “產品詞” 去描述這商品到底是賣什麼，他可以是單詞，稱為 unigram，也可以複合詞，bigram or trigram……</p><ul><li>unigram:shirt , blouse</li><li>bigram: apron dress, bermuda shorts</li><li>trigram: buckle strap shoes, denim mini dress</li></ul><p>“ 標籤詞 “ (label words)，定義比較空泛， 狹義一點指那些可以用來形容商品或能凸顯商品特色的詞</p><ul><li>unigram: denim, hipster</li><li>bigram: chinese tunic, cotton padded</li><li>trigram: deep v collar</li></ul><p>廣義一點，也可以包含產品詞，最終還得看業務需求，標籤詞他還能在細分出 ＂屬性詞＂(propery)</p><ul><li>領口：高領， 低領，V 領，深 V ..</li><li>材質：棉，麻 …</li></ul><p>不管怎樣，如果沒有人工去蒐集出詞彙，那就得靠機器自己挖掘詞彙字典。</p><p>問題來了，在我們的商品池中的商品 title，基本上沒什文法可言，詞彙也是中翻英出來的。</p><p>如何找出有意義的詞彙組成的 ＂產品詞＂或＂標籤詞＂就是問題的核心了</p><p>EX:<br>＂long＂,＂sleeved＂ 分別看沒什意義，但合起來變成 bigram words： ＂long sleeved” 就有意義</p><p>那要怎麼衡量 words 跟 words 之間的組合程度呢？</p><p>就是 <strong>熵 entropy了</strong></p><h2 id="Entropy-識別關鍵字"><a href="#Entropy-識別關鍵字" class="headerlink" title="Entropy 識別關鍵字"></a>Entropy 識別關鍵字</h2><h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><p>在 information theory 中 ，entropy 被用來衡量系統內的不確定性程度</p><script type="math/tex; mode=display">H(X) = -\sum_x p(x)\log p(x)</script><p>＂不確定性＂(uncertainty) 跟 information 豐富程度是一體兩面的</p><p>entropy 越高，代表不確定性越高，代表能提供的 information 也更多</p><p>舉例來說，如果明天會下雨的 probability 為 0.5，天氣系統的 entropy (以 2 為底)就是</p><script type="math/tex; mode=display">-\cfrac{1}{2} \log_2\cfrac{1}{2} - -\cfrac{1}{2} \log_2\cfrac{1}{2} = 1</script><p>如果明天會下雨的 probability 為 1，那天氣系統的 entropy 就是</p><script type="math/tex; mode=display">-1log_21 = 0</script><p>代表天氣系統完全沒有不確定性，明天肯定會下雨，對我們無法提供任何 information</p><p>我們可以利用 entropy 來衡量 </p><ul><li>內聚力：word  跟 word 之間的連結緊密程度</li><li>豐富度：words 本身的自由運用程度</li></ul><h3 id="互消息-MI-Mutual-Information"><a href="#互消息-MI-Mutual-Information" class="headerlink" title="互消息 (MI - Mutual Information)"></a>互消息 (MI - Mutual Information)</h3><p>先上公式</p><script type="math/tex; mode=display">I(X,Y) =\sum_{y \in Y}\sum_{x \in X} p(x,y) \log(\cfrac{p(x,y)}{p(x) p(y)})</script><p>再上圖</p><p><img src="https://i.imgur.com/q58vjPv.png" alt=""></p><p>看圖就可以直觀明白，$I(X,Y)$ 可以用來衡量兩個事件彼此的關聯性，直觀上互消息可以用來衡量兩個 word 之間的依賴程度。</p><p>$\text{PMI}$ (point-wise mutual information) 也可以用來來衡量兩個 word  的相關性，他被視為簡化版的 $\text{MI}$</p><script type="math/tex; mode=display">PMI(x,y) = \log \cfrac{p(x,y)}{p(x)p(y)}</script><p>word A  跟 word B 的 PMI(A， B) 或 MI(A，B）value  越高，代表 A, B  越相互依賴，組成一個 term 的可能性越越大</p><p>但從公式上不難看出，MI 是 weighted  過後的 PMI。在實務上，PMI 傾向給 “those word only occur together” 組成的 bigram 較高的分數 ; 而 MI 傾向給 high frequency bigram 更高的分數</p><p>EX:<br>在商品 title 中有個詞 ＂small fresh＂</p><ul><li>joint probability $\text{p(“small”, “fresh”)} = 0.002$</li><li>$\text{p(“small”)} = 0.0058$ $\text{p(“fresh”)}= 0.0035$</li><li>$\text{PMI(“small”, “fresh”)} = 4.59$</li><li>$\text{MI(“small”, “fresh”)} =0.009$</li></ul><p>有另一個詞 ＂fresh loos＂</p><ul><li>join probability $\text{p(“fresh loose”)} = 0.0001$</li><li>$\text{p(“fresh”) =0.0035}$  $\text{p(“loose”)} = 0.0024$</li><li>$\text{PMI(“fresh”, “loose”)} = 2.47$</li><li>$\text{MI(“fresh”, “loose”)} =0.00024$</li></ul><p>顯然，對於 fresh 這個 word 而言，＂small fresh＂比 ＂fresh loose＂成詞程度較高。</p><h3 id="左右熵"><a href="#左右熵" class="headerlink" title="左右熵"></a>左右熵</h3><p>左右熵代表了 word 本身可以自由運用的程度</p><p>我們知道，一個 word A 可以跟左邊的 word L，也可以跟右邊的 word R 組合，而左右熵就是來衡量 word A 組成 phase 的豐富程度</p><script type="math/tex; mode=display">H_L(W) = -\sum_{l \in L}p(\text{l::w | w}) \ \log_2 p(\text{l::w| w}) \\H_R(W) = -\sum_{r \in R}p(\text{r::w | w}) \ \log_2 p(\text{r::w| w})</script><p>EX：</p><p>假設 ＂skirt ＂ 這個產品詞在池子中的鄰字組合計數如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_information</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> - x * math.log(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_entropy</span>(<span class="params">freqDict</span>):</span></span><br><span class="line">    total_count = <span class="built_in">sum</span>(<span class="built_in">list</span>(freqDict.values()))</span><br><span class="line">    informations = [cal_information(fre / total_count) <span class="keyword">for</span> fre <span class="keyword">in</span> freqDict.values()]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(informations)</span><br><span class="line"></span><br><span class="line">skirt_left = &#123;</span><br><span class="line">    <span class="string">&quot;long skirt&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;midi skirt&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">&quot;pegged skirt&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;pleated skirt&quot;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&quot;prairie skirt&quot;</span>: <span class="number">20</span></span><br><span class="line">    <span class="string">&quot;printed skirt&quot;</span>: <span class="number">400</span>,</span><br><span class="line">    <span class="string">&quot;sarong skirt&quot;</span>: <span class="number">80</span>,</span><br><span class="line">    <span class="string">&quot;trumpet skirt&quot;</span>: <span class="number">600</span>       </span><br><span class="line">&#125;</span><br><span class="line">skirt_right = &#123;</span><br><span class="line">    <span class="string">&quot;skirt suit&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;skirt dress&quot;</span>: <span class="number">1000</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>則 skirt 的 left entropy 為</p><script type="math/tex; mode=display">E_L(\text{"skirt"}) = 1.82</script><p>right entropy為</p><script type="math/tex; mode=display">E_R(\text{"skirt"}) =0.63</script><p>顯然對 “skirt” 而言，左側語境比右側豐富</p><h3 id="Normalize-Entropy-amp-PMI-amp-MI"><a href="#Normalize-Entropy-amp-PMI-amp-MI" class="headerlink" title="Normalize Entropy &amp; PMI &amp; MI"></a>Normalize Entropy &amp; PMI &amp; MI</h3><p>PMI, MI 與  entropy 的值域是個相對 unbound 的值，造成在使用時比較難拿捏 threshold，得來回比對數值決定成詞標準，解決方法是 normalize 值到固定範圍內:</p><ul><li>Normalizing PMI  into (1, -1)</li></ul><script type="math/tex; mode=display">\text{PMI}_n(x, y) = (\ln\cfrac{p(x,y)}{p(x)p(y)}) / -\ln p(x,y)</script><ul><li>Normalizing MI into (0, 1)</li></ul><script type="math/tex; mode=display">\text{MI}*n(X,Y) = \cfrac{\sum*{x,y} p(x,y) \ln \frac{p(x,y)}{p(x) p(y)}}{-\sum_{x,y}p(x,y)\ln p(x,y)}</script><ul><li>Normalizing entropy into (0, 1)</li></ul><script type="math/tex; mode=display">H_n(X) = -\sum_x \frac{p(x) \log p(x)}{\log n}</script><p>論文研究顯示 Normalized MI &amp; PMI  為對角線趨勢，但依然會有一定失真，所以在使用上得自行拿捏<img src="https://i.imgur.com/dTFYXMh.png" alt="Normalized (Pointwise) Mutual Information in Collocation Extraction" style="zoom:50%;" /></p><p>一般來說</p><ul><li>MI 偏向 high frequency，NMI 會稍微將高頻詞 push down ，低頻詞 pull up</li><li>PMI 偏向 low frequency，NPMI 稍微降低低頻詞的 rank</li></ul><h2 id="Score-成詞分數"><a href="#Score-成詞分數" class="headerlink" title="Score 成詞分數"></a>Score 成詞分數</h2><p>有了度量語境豐富度跟詞彙內聚力的工具後，得進一步定出一個 score 代表＂成詞程度＂，score  越高，代表這個詞成為有意義詞的可能性相對較高。</p><p>先定義一個 candidate phrase 的抽象表達，方便我們計算其成詞 score。</p><p>我們的 Candidate phrase  可以是以下這些組合：</p><ul><li>unigram candidate (special case)<ul><li>[unigram] ,    ex: [skirt]</li></ul></li><li>bigram candidate<ul><li>[unigram] :: [unigram], ex: [long :: skirt]</li></ul></li><li>trigram candidate<ul><li>[unigram] :: [bigram], ex: [casual] :: [long skirt]</li><li>[bigram] :: [unigram] ex: [flower printed]  :: [shirt]</li></ul></li></ul><p>拆分成  [Left] :: [Right] 的形式方便我們泛化處理 candidate phrase</p><p>接下來利用定義好的 candidate phrase 來計算成詞 score</p><p><img src="https://i.imgur.com/YJQTaFk.png" alt="product_label_word"></p><p>這裡給出一個最簡單的 score 計算：</p><script type="math/tex; mode=display">\text{score} = \text{(PMI or MI)} - \min(Right_{\text{left_entropy}}, Left_{\text{right_entropy}}) + \min(\text{right_entropy}, \text{left_entropy})</script><ul><li>$\min(Right_{\text{left_entropy}}, Left_{\text{right_entropy}})$  分別表示，Left side 與 Right  side 各自的語境豐富程度，通常取  min 後的的值越大，代表 Left side 或 Right side 有一側傾向與其他詞結合，candidate 越不可能成詞</li><li>$\min(\text{right_entropy}, \text{left_entropy})$ 表示 candidate 左右兩側語境豐富成度，越大代表 candidate 越可能成詞</li></ul><h2 id="Label-Score-amp-Product-Score"><a href="#Label-Score-amp-Product-Score" class="headerlink" title="Label Score &amp;  Product Score"></a>Label Score &amp;  Product Score</h2><p>有了以上的 background 是時候來說說產品詞跟標籤詞的特性了，大致上</p><ul><li>產品詞在 candidate 會出現在 right side，其左側自由度較高:  left_entropy &gt;  right_entropy</li></ul><p><img src="https://i.imgur.com/HsbGKYa.png" alt="lace blouse" style="zoom: 50%;" /></p><ul><li>標籤詞在 candidate 會出現在 left side，其右側自由度較高: right_entropy &gt; left_entropy</li></ul><p><img src="https://i.imgur.com/QIKT45W.png" alt="short sleeve" style="zoom: 67%;" /></p><p>顯然只有成詞分數 score 不足以將產品詞和標籤詞分離，所以每個 candidate phrase，會針對 label 跟 product 特性計算 label score 跟 product score。</p><p>先上圖：<img src="https://i.imgur.com/vXEteao.png" alt="product_label_recong"></p><ul><li>Right Phrase，表  corpus 內出現在 candidate  right side 的 phrase 集合<ul><li>EX : short sleeve  right side unigram 集合</li></ul></li></ul><p><img src="https://i.imgur.com/FaMp6a8.png" style="zoom: 67%;" /></p><ul><li>Light Phrase，表  corpus 內出現在 candidate  left side 的 phrase 集合</li></ul><p>計算 Right Phrases 集合內每一個 phase 對 candidate phrase 的 sum of  information，代表從所有 right phrases 的角度來看 candidate phrase 的豐富度，值越高代表  candidate 的 right phrases 組成越豐富，其成為 label 的機會越高。</p><p><img src="https://i.imgur.com/gNO1xCI.png" alt="cal_information"></p><p>我們希望單個 $\text{phrase}_i$ 對 candidate phrase $C$ 的 conditional probability $p(\text{phrase}_i::C|\text{phrase}_i)$ 不要太高也不要太低，此時算出的  $\text{information} = - p(\text{phrase}_i::C|\text{phrase}_i) \log p(\text{phrase}_i::C|\text{phrase}_i)$ 恰好是最大</p><script type="math/tex; mode=display">\begin{aligned} I_{L,C} &= \sum_{phase_i \in C_{L}}  - p(\text{phrase}_i::C|\text{phrase}_i) \log p(\text{phrase}\_i::C|\text{phrase}_i)\\ I_{R,C} &= \sum_{phrase_j \in C*{R}}  - p(C::\text{phrase}_j|\text{phrase}_j) \log p(C::\text{phrase}_j|\text{phrase}_j) \end{aligned}</script><ul><li>$I_{L,C}$ 表 left phrases 對 candidate 的 sum of information</li><li>$C_L$ 表 candidate  的 left phrase 集合</li></ul><p>有了 left / right phrases information，一個簡單的  label score and product score 計算如下</p><script type="math/tex; mode=display">\begin{aligned} \text{label score} &= (\text{right_entropy - left_entropy}) + (I_{R,C} - I_{L,C})  \\ \text{product score} & = (\text{left_entropy - right_entropy}) + (I_{L,C} - I_{R,C})\end{aligned}</script><p>P.S. 上面分數計算只是提供一個計算思路，實際使用還是得資料分析</p><h1 id="Engineering"><a href="#Engineering" class="headerlink" title="Engineering"></a>Engineering</h1><h2 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h2><p>candidate phrase  的需要計算的值有</p><ul><li>candidate 的 left entropy and right entropy</li><li>candidate 的 PMI/MI 中的  joint probability / frequency</li><li>left component 的 entropy ; right component 的 entropy</li><li>right phrase information and left phrase information</li><li>… etc</li></ul><p>為了方便計算 candidate 需要兩種 Tries ，一個存 corpus 內所有 sentence 的 prefix tries，另一個存 corpus 內所有 reversed title 的  reversed tries (叫 suffix  tries 怕有歧義)</p><p>以 <code>title = &quot;Masks Scarf Cashmere Sweater Cap&quot;</code> 為例</p><p>首先將 title 所有可能 ngram 取出:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cashmere&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cap&#x27;</span>]</span><br></pre></td></tr></table></figure><p>build prefix tries:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>build reversed tries:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>當我們的 <code>candidate phrase = [&quot;Cashmere&quot; :: &quot;Sweater&quot;]</code> 時，</p><p>透過 prefix tries 即可找出 <code>[&quot;Cashmere&quot; :: &quot;Sweater&quot;]</code> 右側所有的 phrase node</p><p>透過 reversed tries 即可找出 <code>[&quot;Cashmere&quot; :: &quot;Sweater&quot;]</code> 左側所有的 phrase node</p><h2 id="啟發式辨識流程"><a href="#啟發式辨識流程" class="headerlink" title="啟發式辨識流程"></a>啟發式辨識流程</h2><p><img src="https://i.imgur.com/vstiDmj.png" alt=""></p><p>用 entropy 辨識產品詞與標籤詞本質上是 unsupervised learning 的做法，如果以 threshold 卡 score, label score, product score 判斷結果，肯定事倍工半。辨識過程中加入 clustering/grouping 輔助判斷，多迭代幾遍後就能搜集到高度置信的結果。</p><h3 id="Grouping"><a href="#Grouping" class="headerlink" title="Grouping"></a>Grouping</h3><p>把 candidate phrase 當成 data sample $x$ 的話，其包含的特徵有</p><ul><li>自身統計類：frequency ，probability … etc</li><li>自身 entropy related PMI/MI，NPMI/NMI，left entropy / right entropy</li><li>left / right component related: PMI/MI ，entropy to left/entropy to right</li><li>left phrase/right phrase related: sum of information ，deviation，diversity，total frequency，average frequency，total phrase …etc</li><li>score  類：成詞 score ，label score，product score</li></ul><p>挑出 data samples 裡有鑑別度的特徵丟進  cluster 算法中初步分成四群：</p><ul><li>group A: 獨立成詞<ul><li>一些用法固定的詞彙</li><li>ex : “big code”(這應該是想表示大碼？)，”united state”</li></ul></li><li>group B: label 詞<ul><li>符合 label 詞的特性，右側自由度高</li><li>ex: “short sleeved”</li></ul></li><li>group C: 右側 product 詞<ul><li>符合 product 詞的特性，左側自由度高</li><li>ex: “lace blouse”</li></ul></li><li>group D: 無用詞<ul><li>沒什意義的詞，本身成詞 score 不高</li><li>ex: “lace long”</li></ul></li></ul><p>然後在每個  group  中，分別挑出多個  high confidence and typical data samples ，跟其餘的 data sample 做 KNN/Kmean，來回個幾次做 semi-supervised  。</p><h3 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h3><p>隨著置信的 data sample 越多，可以考慮訓練 decision tree，辨識新的 candidate phrases。</p><p>也可以利用  word2Vec 強大的相近詞搜索相似的 產品詞/標籤詞 挖掘辨識新的產品詞/標籤詞</p><p>這兩個做法建立在手頭上的詞彙已能很好區分出產品詞和標籤詞的情況下，例如用 word2Vec 找相近產品詞有奇效：</p><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w2v_model.wv.most_similar(<span class="string">&quot;flight_jacket&quot;</span>, topn=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;bomber_jacket&#x27;</span>, <span class="number">0.8251528739929199</span>),</span><br><span class="line"> (<span class="string">&#x27;flight_suit&#x27;</span>, <span class="number">0.8104218244552612</span>),</span><br><span class="line"> (<span class="string">&#x27;coach_jacket&#x27;</span>, <span class="number">0.7058290243148804</span>),</span><br><span class="line"> (<span class="string">&#x27;workwear_jacket&#x27;</span>, <span class="number">0.7037896513938904</span>),</span><br><span class="line"> (<span class="string">&#x27;jacket&#x27;</span>, <span class="number">0.7035773992538452</span>),</span><br><span class="line"> (<span class="string">&#x27;ma_1&#x27;</span>, <span class="number">0.6985215544700623</span>),</span><br><span class="line"> (<span class="string">&#x27;baseball_uniform&#x27;</span>, <span class="number">0.6333736181259155</span>),</span><br><span class="line"> (<span class="string">&#x27;ma1_pilot&#x27;</span>, <span class="number">0.6201080679893494</span>),</span><br><span class="line"> (<span class="string">&#x27;jackets&#x27;</span>, <span class="number">0.608674168586731</span>),</span><br><span class="line"> (<span class="string">&#x27;denim_jacket&#x27;</span>, <span class="number">0.6048851013183594</span>)]</span><br></pre></td></tr></table></figure><h2 id="Some-Tips"><a href="#Some-Tips" class="headerlink" title="Some Tips"></a>Some Tips</h2><ol><li>candidate phrase proposal：可以透過 TF-IDF, frequency, student-t, PMI 先行召回一批 candidate 再開始辨識</li><li>一次處理一種 ngram</li><li>辨識過程中加入字典輔助<ul><li>product words 黑白字典</li><li>label words 黑白字典</li></ul></li><li>善用 bigram / trigram 可以由其他 phrase 組合出，可以省去很多計算量<ul><li>產品詞組成<ul><li>unigram 產品詞<ul><li>[unigram], ex： skirt</li></ul></li><li>bigram 產品詞<ul><li>[unigram label] :: [unigram product] , ex: long skirt</li><li>[unigram] :: [unigram],  ex: phone shell (手機殼 …)</li></ul></li><li>trigram 產品詞<ul><li>[bigram label] :: [unigram product], ex: long sleeved blouse</li><li>[unigram label] :: [bigram product], ex: little black dress</li></ul></li></ul></li><li>標籤詞組成<ul><li>unigram 標籤詞<ul><li>[unigram],  ex:  slim</li></ul></li><li>bigram 標籤詞<ul><li>[unigram] :: [unigram], ex: v neck, high cut</li></ul></li><li>trigram 標籤詞<ul><li>[unigram label] :: [bigram label], ex: half high collar</li><li>[bigram label] :: [unigram], ex: deep v collar</li></ul></li></ul></li></ul></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>data mining basing on entropy<ul><li>Language Models – handling unseen sequences &amp; Information Theory<br><a href="https://www3.cs.stonybrook.edu/~ychoi/cse628/lecture/03-ngram.pdf">https://www3.cs.stonybrook.edu/~ychoi/cse628/lecture/03-ngram.pdf</a></li><li>反作弊基于左右信息熵和互信息的新词挖掘 <a href="https://zhuanlan.zhihu.com/p/25499358">https://zhuanlan.zhihu.com/p/25499358</a></li><li>基于互信息和左右信息熵的短语提取识别 <a href="http://www.hankcs.com/nlp/extraction-and-identification-of-mutual-information-about-the-phrase-based-on-information-entropy.html">http://www.hankcs.com/nlp/extraction-and-identification-of-mutual-information-about-the-phrase-based-on-information-entropy.html</a></li></ul></li><li>Normalization entropy<ul><li>Efficiency (normalized entropy) <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory">https://en.wikipedia.org/wiki/Entropy_(information_theory)#Efficiency_(normalized_entropy)</a>#Efficiency_(normalized_entropy))</li><li>Bouma, G. (2009). Normalized ( Pointwise ) Mutual Information in Collocation Extraction. Proceedings of German Society for Computational Linguistics (GSCL 2009), 31–40.</li><li>[<a href="https://math.stackexchange.com/questions/395121/how-entropy-scales-with-sample-size">https://math.stackexchange.com/questions/395121/how-entropy-scales-with-sample-size</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> recommendation system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation system </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thompson Sampling 推薦系統中簡單實用的 Exploring Strategy</title>
      <link href="Implement-Thompson-Sampling-in-Recommendation-System/"/>
      <url>Implement-Thompson-Sampling-in-Recommendation-System/</url>
      
        <content type="html"><![CDATA[<h1 id="Exploring-and-Exploiting"><a href="#Exploring-and-Exploiting" class="headerlink" title="Exploring and Exploiting"></a>Exploring and Exploiting</h1><p><strong>Exploring and Exploiting (EE)</strong> 是推薦系統中歷久不衰的議題，如何幫助用戶發現更多感興趣的 entity 以及基於已有對用戶的認知推薦他感興趣的 entity，在推薦系統的實務上都得考慮。</p><p>具象化這個問題：在推薦系統中有$\text{}$ $\text{category A, category B, category C, category D, category E}$ 等五大類的 entity 集合，今天有個新用戶 $U$來了，我們要如何</p><ol><li>知道他對哪個種類的 entity 比較感興趣？</li><li>人的興趣可以分成長期興趣跟短期興趣，在電商場景中，用戶短期興趣指他有立即需求的商品，我們如何快速抓到他的意圖，調整推薦系統的響應？</li><li>推薦哪些類目能帶給他意料之外的驚喜 ? 那些他沒預期，但我們推薦給他，能讓他感到滿意的 category。</li></ol><p>Multi-armed bandit problem, K-armed bandit problem (MAP) 中的 Thompson Sampling，簡單又實用</p><a id="more"></a><p><a href="https://seed9d.github.io/categories/recommendation-system/">推薦系統相關文章</a></p><h1 id="Thompson-Sampling"><a href="#Thompson-Sampling" class="headerlink" title="Thompson Sampling"></a>Thompson Sampling</h1><blockquote><p>Thompson Sampling 利用了 beta distribution 是 bernoulli distribution 的 conjugacy prior， 來更新 entity 被選中的 posterior probability distribution</p></blockquote><h2 id="從-Beta-distribution-說起"><a href="#從-Beta-distribution-說起" class="headerlink" title="從 Beta distribution 說起"></a>從 Beta distribution 說起</h2><script type="math/tex; mode=display">Beta(p|\alpha, \beta) \triangleq  \cfrac{1}{B(\alpha, \beta)} \ p^{\alpha -1} \ (1-p)^{\beta - 1}</script><ul><li>beta function $B(\alpha, \beta)$ is a normalization term ，其作用是使 $\int^1_0 \cfrac{1}{B(\alpha, \beta)} \ p^{\alpha -1} \ (1-p)^{\beta - 1} dp = 1$<ul><li>$B(\alpha, \beta) = \int^1_0 p^{\alpha -1} (1-p)^{\beta -1} dp$</li></ul></li></ul><p>Beta distribution $beta(\alpha, \beta)$ 的期望值很簡潔</p><script type="math/tex; mode=display">E[p] = \cfrac{\alpha}{\alpha + \beta}</script><p>我們知道期望值本身是結果的加權平均，如果把 $\alpha$ 視為成功次數， $\beta$ 視為失敗次數，那不就是平均成功率了嗎？</p><p>更神奇的是，平均成功率還可以隨著試驗的失敗跟成功次數變動，依然還是 beta distribution</p><script type="math/tex; mode=display">\cfrac{\alpha + n^{(1)}}{\alpha + \beta + n^{(1)} + n^{(0)}}</script><ul><li>$n^{(1)}$：表新增成功次數</li><li>$n^{(0)}$: 表新增失敗次數</li></ul><p>也因為這個直覺的特性，Beta distribution 非常適合用在估計 打擊率, 點擊率, 命中率 …等等 binary problem</p><h3 id="以推薦系統-click-through-rate-CTR-當例子"><a href="#以推薦系統-click-through-rate-CTR-當例子" class="headerlink" title="以推薦系統 click through rate (CTR) 當例子"></a>以推薦系統 click through rate (CTR) 當例子</h3><p>在推薦系統中，category $A$ 在用戶的點擊率 (ctr) 統計中，所有用戶對 category $A$ :</p><ul><li>$\text{average ctr} = 0.33$</li><li>$\text{ctr variacne} = 0.00147$</li></ul><p>以 $\text{mean=0.33 variacne=0.00147}$  算出 $\alpha \approx50$ $\beta \approx100$， $\alpha =50$ $\beta=100$ 在本推薦系統中的意義是，$\text{category A}$  平均每 150 次 impression ($\alpha + \beta$) 能產生 50 次 click ($\alpha$)，100 次 看了不點 ($\beta$)。</p><p>畫出 PDF</p><p><img src="https://i.imgur.com/k4XcKD1.png" style="zoom: 33%;" /></p><ul><li><p>圖中 PDF curve 的意義是，有個人叫做 “平均用戶”，”平均用戶” 對 $\text{category A}$ 最有可能的點擊率是 $0.33$，但不一定是 0.33, 可能比 0.33 高，可能比 0.33 低，但產生 0.33 這個點擊率的 likelihood $L(\theta| X=0.33)$ 最高</p><ul><li>下圖是對  $beta(50, 100)$ sample 500 次，可以看出 $X=0.33$ 附近被 sample 到的次數的確較高</li></ul><p><img src="https://i.imgur.com/WpSNpVB.png" style="zoom: 33%;" /></p></li></ul><p>今天來了個新用戶 $U$，我們不知道他對 $\text{category A}$ 的喜好程度怎麼樣，但我們可以利用前面的 “平均用戶” 做為先驗： 150 impression 產生 50 次 click  ($\alpha=50 \ , \beta=100$ )，再透過他後續跟 $\text{category A}$ 的互動修正出 for  $\text{user U}$ 的 $\alpha_U \ \beta_U$。</p><p>假設我們給 $U$ 展示 $\text{category A}$  100 次後， 他 click了 60 次，看了不點 40 次，那他的 beta distribution 變成</p><p>$beta(50 + 60, 100 + 40 ) = beta(110, 140)$</p><p><img src="https://i.imgur.com/NKVZ91e.png" style="zoom: 33%;" /></p><p>可以發現橘線變得更尖，且往右移，此時 $mean =0.44$，表示 $user \ U$ 比＂平均用戶＂更加偏好 $\text{category A}$。</p><p>總結以上，一開始我們對於新用戶 $U$ 一無所知，不知道他對 $\text{category A}$ 的偏好，但我們透過已有的先驗，結合他跟推薦系統的互動，慢慢修正對他的認知：</p><script type="math/tex; mode=display">\cfrac{\alpha + n^{(1)}}{\alpha + \beta + n^{(1)} + n^{(0)}} = \cfrac{50 + 60}{50 + 100 + 60 + 40} = 0.44</script><ul><li>$n^{(1)}$：對 $\text{category A}$ 新的點擊行為</li><li>$n^{(0)}$: 對 $\text{category A}$ 新的＂看了未點＂的行為</li></ul><p>於是，ctr 從原本 “最有可能” 0.33 修正到 “最有可能” 0.44 。</p><ul><li>“最有可能”: 因爲一切都是 distribution 阿</li></ul><p>這個神奇又簡潔的現象背後的數學原理，正是 beta distribution 的 conjugacy 特性。</p><h2 id="Conjugate-prior-amp-Bayesian-inference"><a href="#Conjugate-prior-amp-Bayesian-inference" class="headerlink" title="Conjugate prior &amp; Bayesian inference"></a>Conjugate prior &amp; Bayesian inference</h2><blockquote><p>prior $p(\theta)$ is conjugate to the  likelihood function $p(X|\theta)$ when the posterior $p(\theta|X)$ has the same function form as the prior</p></blockquote><script type="math/tex; mode=display">p(\theta|X) = \cfrac{p(X|\theta)  p(\theta)}{p(X)}  \Leftrightarrow  \text{posterior} = \cfrac{\text{likelihood} \cdot \text{prior}}{\text{evidence}}</script><ul><li><p>$p(X)$ is the normalization term</p><p>$p(X) = \int_{\theta\in \Theta}p(X|\theta)p(\theta)d\theta$</p></li></ul><p>即是</p><ul><li>prior $p(\theta)$  為 beta distribution $Beta(\theta|\alpha, \beta) = \cfrac{1}{B(\alpha, \beta)} \ \theta^{\alpha -1} \ (1-\theta)^{\beta - 1}$</li><li>likelihood function $p(X|\theta)$ 為 bernoulli distribution   $Bern(c|\theta) = \theta^c(1-\theta)^{1-c}$</li></ul><p>beta distribution 與 bernoulli distribution 都有類似的 form: $\theta^m(1-\theta)^n$ ，同時 posterior distribution $p(\theta|X)$ 也是 beta distribution</p><p>posterior  $p(\theta|X)$ 也是 beta distribution 證明如下</p><h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>假設  </p><ul><li><p>推薦系統中，對 $category \ A$ 曝光 $N$ 次，用戶 $U$ 點擊次數 $n^{(1)}$，未點擊次數 $n^{(0)}$，本質上是個 $N \ bernoulli \ trail$ ， 所以其 likelihood function：</p><p>$p(C|p) =\prod_{i=1}^n p(C=c_i|p)= p^{n^{(1)}}(1-p)^{n^{(0)}}$ (忽略係數)</p><ul><li>$C$ 是 outcome, $c=1$ for positive ; $c=0$ for negative</li></ul></li><li><p>$prior$ $p(p)$ 為 beta distribution :</p><script type="math/tex; mode=display">p(p|\alpha, \beta) =  Beta(p|\alpha, \beta) = \cfrac{1}{B(\alpha, \beta)} \ p^{\alpha -1} \ (1-p)^{\beta - 1}</script></li></ul><p>則 $\text{posterior}$  $p(p|C,\alpha, \beta) = \cfrac{ p(C|p) \ p(p|\alpha, \beta)}{\int^1_0  p(C|p) \ p(p|\alpha, \beta) \ dp}$ </p><p>分母項 $\int^1_0  p(C|p) \ p(p|\alpha, \beta) \ dp$  作用為 normalize the distribution，通常用 $Z$ 代表：</p><script type="math/tex; mode=display">\begin{aligned}p(p|C,\alpha, \beta)  &= \cfrac{ p(C|p)  p(p|\alpha, \beta)}{\int^1_0  p(C|p)  p(p|\alpha, \beta) dp} \\&= \cfrac{p^{n^{(1)}} (1-p)^{n^{(0)}} \cfrac{1}{B(\alpha,\beta)} p^{\alpha -1} (1-p)^{\beta -1}}{Z} \\ &= \cfrac{p^{[n^{(1)} + \alpha] -1} (1 - p) ^{[n^{(0)} + \beta]-1}}{B(\alpha, \beta) Z} \end{aligned}</script><ul><li>$Z =\cfrac{1}{B(\alpha,\beta)} \int^1_0 p^{n^{(1)}} (1-p)^{n^{(0)}}  p^{\alpha -1} (1-p)^{\beta -1} dp$</li></ul><p>分母要 normalize 整個 probability distribution 使  $\int p(p|C,\alpha, \beta) dp= 1$</p><p>而新的 normalization 項為 </p><script type="math/tex; mode=display">B(\alpha,\beta)Z =  \int^1_0 p^{[n^{(1)} + \alpha] -1} (1 - p) ^{[n^{(0)} + \beta]-1}dp</script><p>這不正是另一個 Beta function:  $B(n^{(1)} + \alpha , n^{(0) } +\beta)$ ？？</p><p>所以  $p(p|C,\alpha, \beta)$  最終化簡成</p><script type="math/tex; mode=display">\begin{aligned} p(p|C,\alpha, \beta)  &= \cfrac{p^{[n^{(1)} + \alpha] -1} (1 - p) ^{[n^{(0)} + \beta]-1}}{B(n^{(1)} + \alpha , n^{(0) } +\beta)} \\ &= Beta (p|n^{(1)} +\alpha, n^{(0)} + \beta)\end{aligned}</script><p>故得證 $\text{posterior}$ $p(p|C,\alpha, \beta)$  也是 $\text{Beta distribution}$</p><h1 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h1><p>一個簡單的實作方式是</p><ol><li>先在線下計算好每個 category 的 ctr mean 跟 variance。</li><li>在實時推薦時，拿回某用戶近期對每個 category 交互數據 impression 與 click ，計算出新的  $\alpha  \ \beta$。</li><li>有了每個類目的 $\alpha \ \beta$ 後，對每個類目的 $beta(\alpha, \beta)$ sampling，接著取出 sample 後 top K 的類目即可。</li><li>C2I 召回 …..</li></ol><p>當然，你也可以不基於 category 維度計算 beta distribution，而是基於每一個 entity。不過如果 entity 數量上百萬，這顯然不切實際。</p><h2 id="線下"><a href="#線下" class="headerlink" title="線下"></a>線下</h2><h3 id="統計每個-category-CTR-的-variance-and-mean"><a href="#統計每個-category-CTR-的-variance-and-mean" class="headerlink" title="統計每個 category CTR 的 variance and mean"></a>統計每個 category CTR 的 variance and mean</h3><ul><li>Spark snippets</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calAvgAndVar</span></span>(input: <span class="type">Dataset</span>[<span class="type">Row</span>], categoryCol: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">Row</span>] =</span><br><span class="line">    input.select(categoryCol, ctrCol)</span><br><span class="line">      .groupBy(categoryCol).agg(</span><br><span class="line">      fn.avg(fn.col(ctrCol)).as(<span class="string">&quot;ctr_avg&quot;</span>),</span><br><span class="line">      fn.variance(ctrCol).alias(<span class="string">&quot;ctr_var&quot;</span>))</span><br><span class="line">      .na.drop</span><br><span class="line">      .withColumnRenamed(categoryCol, <span class="string">&quot;categoryId&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="線上"><a href="#線上" class="headerlink" title="線上"></a>線上</h2><ol><li><p>計算每個 category 的初始 $\alpha_0 \ \beta_0$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ImmutablePair&lt;Double, Double&gt; <span class="title">calAlphaAndBeta</span><span class="params">(<span class="keyword">double</span> ctrMean, <span class="keyword">double</span> ctrVar)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">double</span> alpha = (((<span class="number">1</span> - ctrMean) / (ctrMean)) - <span class="number">1</span> / ctrMean) * Math.pow(ctrMean, <span class="number">2</span>);</span><br><span class="line">            <span class="keyword">double</span> beta = alpha * ((<span class="number">1</span> / ctrMean) - <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> ImmutablePair.of(alpha, beta);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}\alpha &=\left(\frac{1-\mu}{\sigma^2}-\frac{1}{\mu}\right)\mu^2 \\\beta &= \alpha\left(\frac{1}{\mu}-1\right)\end{aligned}</script></li><li><p>取回用戶的近期 category 交互行為 impression and click，並計算新的 $\alpha_t,\ \beta_t$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* left: alpha, right: beta */</span></span><br><span class="line">ImmutablePair&lt;Double, Double&gt; prior = calAlphaAndBeta(<span class="keyword">double</span> ctrMean, <span class="keyword">double</span> ctrVar);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* left: impression, right: click */</span></span><br><span class="line">ImmutablePair&lt;Integer, Integer&gt; posteriorPair = posteriorData.getOrDefault(cateId, ImmutablePair.of(<span class="number">0</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> clickCount = posteriorPair.getRight();</span><br><span class="line"><span class="keyword">int</span> impressionCount = posteriorPair.getLeft();</span><br><span class="line"><span class="keyword">int</span> impressionWithoutClick = (impressionCount - clickCount) &gt; <span class="number">0</span> ? (impressionCount - clickCount) : impressionCount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> newAlpha = prior.getLeft() + clickCount;</span><br><span class="line"><span class="keyword">double</span> newBeta =  prior.getRight() + impressionWithoutClick;</span><br></pre></td></tr></table></figure></li><li><p>對每個 category 的 beta distribution $beta(\alpha, \beta)$ sampling</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.math3.distribution.BetaDistribution;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">calBetaProbability</span><span class="params">(<span class="keyword">double</span> alpha, <span class="keyword">double</span> beta)</span> </span>&#123;</span><br><span class="line">  BetaDistribution betaDistribution = <span class="keyword">new</span> BetaDistribution(alpha, beta);</span><br><span class="line">    <span class="keyword">double</span> rand = Math.random();</span><br><span class="line">    <span class="keyword">return</span> betaDistribution.inverseCumulativeProbability(rand);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>sampling 利用 beta distribution 的 inverse cumulative distribution function (inverse CDF) sampling 出 random variable<ul><li>參考 <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">https://en.wikipedia.org/wiki/Inverse_transform_sampling</a></li></ul></li></ul></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Multi-Armed Bandit With Thompson Sampling <a href="https://predictivehacks.com/multi-armed-bandit-with-thompson-sampling/">https://predictivehacks.com/multi-armed-bandit-with-thompson-sampling/</a></li><li>Conjugacy in Bayesian Inference <a href="http://gregorygundersen.com/blog/2019/03/16/conjugacy/">http://gregorygundersen.com/blog/2019/03/16/conjugacy/</a></li><li>Understanding the beta distribution (using baseball statistics) <a href="http://varianceexplained.org/statistics/beta_distribution_and_baseball/">http://varianceexplained.org/statistics/beta_distribution_and_baseball/</a><ul><li>中文翻譯 : 如何通俗理解 beta 分布？ - 小杰的回答 - 知乎 <a href="https://www.zhihu.com/question/30269898/answer/123261564">https://www.zhihu.com/question/30269898/answer/123261564</a></li></ul></li><li><a href="https://en.wikipedia.org/wiki/Beta_distribution">https://en.wikipedia.org/wiki/Beta_distribution</a></li><li>Heinrich, G. (2005). Parameter estimation for text analysis<ul><li>雖然是講 LDA，但前面從 ML MAP 一路推導到 Bayesian inference ，很詳細</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> recommendation system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation system </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 實作 CBOW with Hierarchical Softmax</title>
      <link href="Pytorch-Implement-CBOW-with-Hierarchical-Softmax/"/>
      <url>Pytorch-Implement-CBOW-with-Hierarchical-Softmax/</url>
      
        <content type="html"><![CDATA[<h1 id="CBOW-with-Hierarchical-Softmax"><a href="#CBOW-with-Hierarchical-Softmax" class="headerlink" title="CBOW with Hierarchical Softmax"></a>CBOW with Hierarchical Softmax</h1><p>CBOW  的思想是用兩側 context words 去預測中間的 center word</p><script type="math/tex; mode=display">P(center|context;\theta)</script><a id="more"></a><p><img src="https://i.imgur.com/WukfJ8F.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>$\text{v}’_j$ 表 $W’$  中  j-th columns  vector</li><li>在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是 leaf node  ，跟 column vector $\text{v}’_j$ 與 word $w_i$  不是直接對應的關係</li></ul></li></ul><h2 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h2><p><img src="https://i.imgur.com/c40z44h.png" style="zoom:50%;" /></p><p>Huffman Tree</p><p>令 $w_{I,j}$  表 input 的 第 $j$ 個 context word; $w_O$ 表 target 的 center word</p><p> 則 Hierarchical Softmax 下的 objective function</p><script type="math/tex; mode=display">\begin{aligned} &-\log p(w_O| w_I) = -\log \dfrac{\text{exp}({h^\top \text{v}'_O})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})} \\& = - \sum^{L(w)-1}_{l=1}  \log\sigma([\cdot] h^\top \text{v}^{'}_l)\end{aligned}</script><ul><li>$L(w_i) -1$ 表 huffman tree 中從 root node  到 leaf node of $w_i$ 的 node number</li><li>$[\cdot]$表 huffman tree 的分岔判斷<ul><li>$[\cdot] = 1$ 表 turn left</li><li>$[\cdot ] = -1$ 表 turn right</li></ul></li><li>$h = \frac {1}{C} \sum^{C}_{j=1}\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$</li></ul><p>詳細推導請見 <a href="/hierarchical-softmax-in-word2vec/" title="Hierarchical Softmax 背後的數學">Hierarchical Softmax 背後的數學</a></p><p>透過 Hierarchical Softmax，因爲 huffman tree 為 full binary tree， time complexity 降成 $\log_2|V|$</p><h1 id="Pytorch-CBOW-with-Hierarchical-Softmax"><a href="#Pytorch-CBOW-with-Hierarchical-Softmax" class="headerlink" title="Pytorch CBOW with Hierarchical Softmax"></a>Pytorch CBOW with Hierarchical Softmax</h1><h2 id="Building-Huffman-Tree"><a href="#Building-Huffman-Tree" class="headerlink" title="Building Huffman Tree"></a>Building Huffman Tree</h2><p>Huffman Tree  建樹過程</p><figure class="highlight"><figcaption><span>HuffmanTree >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuffmanTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, fre_dict</span>):</span></span><br><span class="line">        self.root = <span class="literal">None</span></span><br><span class="line">        freq_dict = <span class="built_in">sorted</span>(fre_dict.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(freq_dict)</span><br><span class="line">        self.node_dict = &#123;&#125;</span><br><span class="line">        self._build_tree(freq_dict)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_tree</span>(<span class="params">self, freq_dict</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            freq_dict is in decent order</span></span><br><span class="line"><span class="string">            node_list: two part: [leaf node :: internal node]</span></span><br><span class="line"><span class="string">                leaf node is sorting by frequency in decent order; </span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">        node_list = [HuffmanNode(is_leaf=<span class="literal">True</span>, value=w, fre=fre) <span class="keyword">for</span> w, fre <span class="keyword">in</span> freq_dict]  <span class="comment"># create leaf node</span></span><br><span class="line">        node_list += [HuffmanNode(is_leaf=<span class="literal">False</span>, fre=<span class="number">1e10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size)]  <span class="comment"># create non-leaf node</span></span><br><span class="line"></span><br><span class="line">        parentNode = [<span class="number">0</span>] * (self.vocab_size * <span class="number">2</span>)  <span class="comment"># only 2 * vocab_size - 2 be used</span></span><br><span class="line">        binary = [<span class="number">0</span>] * (self.vocab_size * <span class="number">2</span>)  <span class="comment"># recording turning left or turning right</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">          pos1 points to currently processing leaf node at left side of node_list</span></span><br><span class="line"><span class="string">          pos2 points to currently processing non-leaf node at right side of node_list</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        pos1 = self.vocab_size - <span class="number">1</span></span><br><span class="line">        pos2 = self.vocab_size</span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            each iteration picks two node from node_list</span></span><br><span class="line"><span class="string">            the first pick assigns to min1i</span></span><br><span class="line"><span class="string">            the second pick assigns to min2i </span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            min2i&#x27;s frequency is always larger than min1i</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        min1i = <span class="number">0</span></span><br><span class="line">        min2i = <span class="number">0</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            the main process of building huffman tree</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size - <span class="number">1</span>):</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                first pick assigns to min1i</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> pos1 &gt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> node_list[pos1].fre &lt; node_list[pos2].fre:</span><br><span class="line">                    min1i = pos1</span><br><span class="line">                    pos1 -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    min1i = pos2</span><br><span class="line">                    pos2 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                min1i = pos2</span><br><span class="line">                pos2 += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">               second pick assigns to min2i </span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> pos1 &gt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> node_list[pos1].fre &lt; node_list[pos2].fre:</span><br><span class="line">                    min2i = pos1</span><br><span class="line">                    pos1 -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    min2i = pos2</span><br><span class="line">                    pos2 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                min2i = pos2</span><br><span class="line">                pos2 += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27; fill information of non leaf node &#x27;&#x27;&#x27;</span></span><br><span class="line">            node_list[self.vocab_size + a].fre = node_list[min1i].fre + node_list[min2i].fre</span><br><span class="line">            node_list[self.vocab_size + a].left = node_list[min1i]</span><br><span class="line">            node_list[self.vocab_size + a].right = node_list[min2i]</span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                the parent node always is non leaf node</span></span><br><span class="line"><span class="string">                assigen lead child (min2i) and right child (min1i) to parent node</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            parentNode[min1i] = self.vocab_size + a  <span class="comment"># max index = 2 * vocab_size - 2</span></span><br><span class="line">            parentNode[min2i] = self.vocab_size + a</span><br><span class="line">            binary[min2i] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;generate huffman code of each leaf node &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size):</span><br><span class="line">            b = a</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            code = []</span><br><span class="line">            point = []</span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                backtrace path from current node until root node. (bottom up)</span></span><br><span class="line"><span class="string">                &#x27;root node index&#x27; in node_list is  2 * vocab_size - 2 </span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">while</span> b != self.vocab_size * <span class="number">2</span> - <span class="number">2</span>:</span><br><span class="line">                code.append(binary[b])  </span><br><span class="line">                b = parentNode[b]</span><br><span class="line">                <span class="comment"># point recording the path index from leaf node to root, the length of point is less 1 than the length of code</span></span><br><span class="line">                point.append(b)</span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                huffman code should be top down, so we reverse it.</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            node_list[a].code_len = <span class="built_in">len</span>(code)</span><br><span class="line">            node_list[a].code = <span class="built_in">list</span>(<span class="built_in">reversed</span>(code))</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                1. Recording the path from root to leaf node (top down). </span></span><br><span class="line"><span class="string">                </span></span><br><span class="line"><span class="string">                2.The actual index value should be shifted by self.vocab_size,</span></span><br><span class="line"><span class="string">                  because we need the index starting from zero to mapping non-leaf node</span></span><br><span class="line"><span class="string">                </span></span><br><span class="line"><span class="string">                3. In case of full binary tree, the number of non leaf node always equals to vocab_size - 1.</span></span><br><span class="line"><span class="string">                  The index of BST root node in node_list is 2 * vocab_size - 2,</span></span><br><span class="line"><span class="string">                  and we shift vocab_size to get the actual index of root node: vocab_size - 2</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            node_list[a].node_path = <span class="built_in">list</span>(<span class="built_in">reversed</span>([p - self.vocab_size <span class="keyword">for</span> p <span class="keyword">in</span> point]))</span><br><span class="line">            </span><br><span class="line">            self.node_dict[node_list[a].value] = node_list[a]</span><br><span class="line">            </span><br><span class="line">        self.root = node_list[<span class="number">2</span> * vocab_size - <span class="number">2</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure><p>建樹過程參考 Word2Vec 作者 Tomas Mikolov 的 c code，思路如下：</p><ol><li>建一個  Array，左半邊放 leaf node ，右半邊放 non leaf node<ul><li>leaf node 按照 frequency 降序排列</li></ul></li><li>bottom up building tree<ul><li>從 Array 中間位置向右半邊填 non leaf node</li><li>each iteration 都從 leaf node 跟 已填完的 non leaf node 找兩個 frequency 最小的 node，做為  child node 填入當下 non leaf node</li></ul></li></ol><p><img src="https://i.imgur.com/H7do73N.png" style="zoom: 67%;" /></p><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>用 huffman tree 實作 Hierarchical Softmax</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HierarchicalSoftmaxLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, freq_dict</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">## in w2v c implement, syn1 initial with all zero</span></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.syn1 = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size + <span class="number">1</span>,</span><br><span class="line">            embedding_dim=embedding_dim,</span><br><span class="line">            padding_idx=vocab_size</span><br><span class="line">            </span><br><span class="line">        )</span><br><span class="line">        torch.nn.init.constant_(self.syn1.weight.data, val=<span class="number">0</span>)</span><br><span class="line">        self.huffman_tree = HuffmanTree(freq_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, neu1, target</span>):</span></span><br><span class="line">        <span class="comment"># neu1: [b_size, embedding_dim]</span></span><br><span class="line">        <span class="comment"># target: [b_size, 1]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># turns:[b_size, max_code_len_in_batch]</span></span><br><span class="line">        <span class="comment"># paths: [b_size, max_code_len_in_batch]</span></span><br><span class="line">        turns, paths = self._get_turns_and_paths(target)</span><br><span class="line">        paths_emb = self.syn1(paths) <span class="comment"># [b_size, max_code_len_in_batch, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        loss = -F.logsigmoid(</span><br><span class="line">            (turns.unsqueeze(<span class="number">2</span>) * paths_emb * neu1.unsqueeze(<span class="number">1</span>)).<span class="built_in">sum</span>(<span class="number">2</span>)).<span class="built_in">sum</span>(<span class="number">1</span>).mean()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_turns_and_paths</span>(<span class="params">self, target</span>):</span></span><br><span class="line">        turns = []  <span class="comment"># turn right(1) or turn left(-1) in huffman tree</span></span><br><span class="line">        paths = []</span><br><span class="line">        max_len = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> target:</span><br><span class="line">            n = n.item()</span><br><span class="line">            node = self.huffman_tree.node_dict[n]</span><br><span class="line">            </span><br><span class="line">            code = target.new_tensor(node.code).<span class="built_in">int</span>()  <span class="comment"># in code, left node is 0; right node is 1</span></span><br><span class="line">            turn = torch.where(code == <span class="number">1</span>, code, -torch.ones_like(code))</span><br><span class="line">            </span><br><span class="line">            turns.append(turn)</span><br><span class="line">            paths.append(target.new_tensor(node.node_path))</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> node.code_len &gt; max_len:</span><br><span class="line">                max_len = node.code_len</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        turns = [F.pad(t, pad=(<span class="number">0</span>, max_len - <span class="built_in">len</span>(t)), mode=<span class="string">&#x27;constant&#x27;</span>, value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> turns] </span><br><span class="line">        paths = [F.pad(p, pad=(<span class="number">0</span>, max_len - p.shape[<span class="number">0</span>]), mode=<span class="string">&#x27;constant&#x27;</span>, value=net.hs.vocab_size) <span class="keyword">for</span> p <span class="keyword">in</span> paths]</span><br><span class="line">        <span class="keyword">return</span> torch.stack(turns).<span class="built_in">int</span>(), torch.stack(paths).long()</span><br></pre></td></tr></table></figure><ul><li>syn1 表 $W’$ 裡面的 vector 對應到 huffman tree non leaf node 的 vector<ul><li>實作上 $W’$  row vector 才有意義</li></ul></li><li>neu1 即 $\text{h}$ 為 hidden layer 的輸出</li><li>target 為 center word $w_O$</li><li>function _get_turns_and_paths 中<ul><li>實作時 -1 表 turn left ; 1 表 turn right ，其實兩者只要相反就好，因爲對於 binary classification<ul><li>$p(\text{true}) = \sigma(x)$ ⇒ $p(\text{false}) = 1- \sigma(x) = \sigma(-x)$</li><li>只是  $\sigma$ 裡的正負號對換而已</li></ul></li></ul></li></ul><h2 id="CBOW-Hierarchical-Softmax"><a href="#CBOW-Hierarchical-Softmax" class="headerlink" title="CBOW + Hierarchical Softmax"></a>CBOW + Hierarchical Softmax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOWHierarchicalSoftmax</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, fre_dict</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.hs = HierarchicalSoftmaxLayer(vocab_size, embedding_dim, fre_dict)</span><br><span class="line">        torch.nn.init.xavier_uniform_(self.syn0.weight.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, context, target</span>):</span></span><br><span class="line">        <span class="comment"># context: [b_size, 2 * window_size]</span></span><br><span class="line">        <span class="comment"># target: [b_size]</span></span><br><span class="line">        neu1 = self.syn0(context.long()).mean(dim=<span class="number">1</span>)  <span class="comment"># [b_size, embedding_dim]</span></span><br><span class="line">        loss = self.hs(neu1, target.long())</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>neu1 為 average of context words’ vector</li></ul><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>訓練過程省略，有興趣請見 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Embedding">seed9D/hands-on-machine-learning</a></p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>訓練語料是聖經，看看 jesus  跟  christ 的相近詞</p><p>In : </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)</span><br><span class="line">cosinSim.get_synonym(<span class="string">&#x27;christ&#x27;</span>)</span><br></pre></td></tr></table></figure><p>out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;christ&#x27;</span>, <span class="number">1.0</span>),</span><br><span class="line"> (<span class="string">&#x27;hope&#x27;</span>, <span class="number">0.78780156</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.7656436</span>),</span><br><span class="line"> (<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.74575657</span>),</span><br><span class="line"> (<span class="string">&#x27;faith&#x27;</span>, <span class="number">0.7190881</span>),</span><br><span class="line"> (<span class="string">&#x27;godliness&#x27;</span>, <span class="number">0.7005944</span>),</span><br><span class="line"> (<span class="string">&#x27;offences&#x27;</span>, <span class="number">0.70045626</span>),</span><br><span class="line"> (<span class="string">&#x27;grace&#x27;</span>, <span class="number">0.6946964</span>),</span><br><span class="line"> (<span class="string">&#x27;dear&#x27;</span>, <span class="number">0.666232</span>),</span><br><span class="line"> (<span class="string">&#x27;willing&#x27;</span>, <span class="number">0.66131693</span>)]</span><br></pre></td></tr></table></figure><p>In </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cosinSim.get_synonym(<span class="string">&#x27;jesus&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Out</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.9999999</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.8051339</span>),</span><br><span class="line"> (<span class="string">&#x27;grace&#x27;</span>, <span class="number">0.75879383</span>),</span><br><span class="line"> (<span class="string">&#x27;church&#x27;</span>, <span class="number">0.7542972</span>),</span><br><span class="line"> (<span class="string">&#x27;christ&#x27;</span>, <span class="number">0.74575657</span>),</span><br><span class="line"> (<span class="string">&#x27;manifest&#x27;</span>, <span class="number">0.7415799</span>),</span><br><span class="line"> (<span class="string">&#x27;believed&#x27;</span>, <span class="number">0.7215627</span>),</span><br><span class="line"> (<span class="string">&#x27;faith&#x27;</span>, <span class="number">0.7198993</span>),</span><br><span class="line"> (<span class="string">&#x27;godliness&#x27;</span>, <span class="number">0.7091305</span>),</span><br><span class="line"> (<span class="string">&#x27;john&#x27;</span>, <span class="number">0.7015951</span>)]</span><br></pre></td></tr></table></figure><p>In</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cosinSim.get_synonym(<span class="string">&#x27;god&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Out</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.9999999</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.8051339</span>),</span><br><span class="line"> (<span class="string">&#x27;grace&#x27;</span>, <span class="number">0.75879383</span>),</span><br><span class="line"> (<span class="string">&#x27;church&#x27;</span>, <span class="number">0.7542972</span>),</span><br><span class="line"> (<span class="string">&#x27;christ&#x27;</span>, <span class="number">0.74575657</span>),</span><br><span class="line"> (<span class="string">&#x27;manifest&#x27;</span>, <span class="number">0.7415799</span>),</span><br><span class="line"> (<span class="string">&#x27;believed&#x27;</span>, <span class="number">0.7215627</span>),</span><br><span class="line"> (<span class="string">&#x27;faith&#x27;</span>, <span class="number">0.7198993</span>),</span><br><span class="line"> (<span class="string">&#x27;godliness&#x27;</span>, <span class="number">0.7091305</span>),</span><br><span class="line"> (<span class="string">&#x27;john&#x27;</span>, <span class="number">0.7015951</span>)]</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://github.com/tmikolov/word2vec">https://github.com/tmikolov/word2vec</a><ul><li>c code</li></ul></li><li>基于Numpy实现Word2Vec Hierarchical Softmax CBOW and SkipGram模型 <a href="http://ziyangluo.tech/2020/02/29/W2VHierarchical/">http://ziyangluo.tech/2020/02/29/W2VHierarchical/</a></li><li><a href="https://github.com/ilyakhov/pytorch-word2vec">https://github.com/ilyakhov/pytorch-word2vec</a></li><li><a href="https://github.com/weberrr/pytorch_word2vec">https://github.com/weberrr/pytorch_word2vec</a></li><li>other<ul><li>Binary Tree: Intro(簡介) <a href="http://alrightchiu.github.io/SecondRound/binary-tree-introjian-jie.html#fullcomplete">http://alrightchiu.github.io/SecondRound/binary-tree-introjian-jie.html#fullcomplete</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 實作 Skipgram with Negative Sampling</title>
      <link href="Pytorch-Implement-Skipgram-with-Negative-Sampling/"/>
      <url>Pytorch-Implement-Skipgram-with-Negative-Sampling/</url>
      
        <content type="html"><![CDATA[<h1 id="Skipgram-with-Negative-Sampling"><a href="#Skipgram-with-Negative-Sampling" class="headerlink" title="Skipgram with Negative Sampling"></a>Skipgram with Negative Sampling</h1><p>skipgram  的思想是用中心詞 center word 去預測兩側的 context words </p><script type="math/tex; mode=display">P(context|center; \theta)</script><a id="more"></a><p><img src="https://i.imgur.com/snzt3hg.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h2 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h2><p> 令 $w_I$ 表 input 的 center word ; $w_{O,j}$ 表 target 的 第 $j$ 個 context word。</p><p> 則 Negative Sampling 下的 objective function</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \sigma(\text{v}^\top_{w_I} \text{v}'_{w_{O,j}}) +  \sum^M_{\substack{i=1 \\ \tilde{w}_i \sim Q}}\exp(\text{v}^\top_{w_I} \text{v}'_{\tilde{w}_i})]</script><ul><li>$\tilde{w}_i$ 為從 distribution $Q$ sample 出的 word</li><li>M 為 從  $Q$  sample 出的 $\tilde{w}$ 數量</li></ul><p>第一項為 input center word  $w_I$ 與 target  context word $w_{O,j}$ 產生的 loss</p><p>第二項為 negative sample 產生的 loss ，共 sample 出 $M$  個  word </p><p>有興趣看從 softmax 推導到 NEG的，參閱 <a href="/negative-sampling-in-word2vec/" title="Negative Sampling 背後的數學">Negative Sampling 背後的數學</a></p><h2 id="Negative-Sample-NEG"><a href="#Negative-Sample-NEG" class="headerlink" title="Negative Sample (NEG)"></a>Negative Sample (NEG)</h2><p>目標是從一個分佈 $Q$ sample 出 word $\tilde{w}$</p><p> 實作上從 vocabulary $V$ sample  出 ${w}_i$ 的 probability $P(w_i)$ 為</p><script type="math/tex; mode=display">P(w_i) = \cfrac{f(w_i)^{\alpha}}{\sum^M_{j=0}(f(w_j)^\alpha)}</script><ul><li>$f(w_i)$  為 $w_i$ 在 corpus 的 frequency count</li><li>$\alpha$  為 factor, 通常設為  $0.75$，其作用是 increase  the probability for less frequency words and decrease the probability for more frequent words</li></ul><p>每個 word  $w_i$ 都有個被 sample 出的 probability $P(w_i)$， 目的是從 $P(w)$ sample 出 $M$ 個  word 做為 negative 項</p><p>網路上常見的實現方法是調用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.multinomial(sample_size, pvals)</span><br></pre></td></tr></table></figure><p>此法應該是透過 inverse CDF 來 sample word，每筆 training data 都調用一次的話運算效率不高</p><p>Word2Vec 作者 Tomas Mikolov 在他的 c code  中，採用了一種近似方式，其思想是在極大的抽樣次數下 $M = 1e8$，word  的 probability 越高代表其 frequency 越大，也就是在 M 中所占份額 shares 越多。</p><p>例如 yellow 的 probability 最大，理應在 M=30 中佔據較多的份額。</p><p><img src="https://i.imgur.com/lCdJO7Y.png" alt=""></p><ul><li>$P(\text{blue}) = \frac{2}{30}$</li><li>$P(\text{green}) = \frac{6}{30}$</li><li>$P(\text{yellow}) = \frac{10}{30}$</li><li>$P(\text{red}) = \frac{5}{30}$</li><li>$P(\text{gray}) = \frac{7}{30}$</li></ul><p>所以事先準備好一張 size 夠大的 table ($M = 1e8$)，根據 word frequency 給予相應的 shares ，真正要 sample word 的時候，只要從 $M$ 中 uniform random 出一個 index $m$ ， index $m$ 對應到的 word 就是被 sample 出的  word  $\tilde{w}$，是個以空間換取時間的做法。</p><h3 id="Seeing-is-Believing"><a href="#Seeing-is-Believing" class="headerlink" title="Seeing is Believing"></a>Seeing is Believing</h3><p>做了一下測試 ，10000 次迭代，每次取  6 個  negatvie sample 的情景下，Tomas Mikolov 的近似思路比較有效率，而且是碾壓性的</p><p><img src="https://i.imgur.com/YLbFdt4.png" style="zoom: 50%;" /></p><p>但在 一次 sample 較多 word 的時候，multinomial 較有效率，可能 numpy 內部有做平行化的關係</p><p><img src="https://i.imgur.com/MXK06wG.png" style="zoom:50%;" /></p><h1 id="Pytorch-Skipgram-with-Negative-Sampling"><a href="#Pytorch-Skipgram-with-Negative-Sampling" class="headerlink" title="Pytorch Skipgram with Negative Sampling"></a>Pytorch Skipgram with Negative Sampling</h1><h2 id="Negative-Sample"><a href="#Negative-Sample" class="headerlink" title="Negative Sample"></a>Negative Sample</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NegativeSampler</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, corpus, sample_ratio=<span class="number">0.75</span></span>):</span></span><br><span class="line">        self.sample_ratio = sample_ratio</span><br><span class="line">        self.sample_table =  self.__build_sample_table(corpus)</span><br><span class="line">        self.table_size = <span class="built_in">len</span>(self.sample_table)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__build_sample_table</span>(<span class="params">self, corpus</span>):</span></span><br><span class="line">        counter = <span class="built_in">dict</span>(Counter(<span class="built_in">list</span>(itertools.chain.from_iterable(corpus))))</span><br><span class="line">        words = np.array(<span class="built_in">list</span>(counter.keys()))</span><br><span class="line">        probs = np.power(np.array(<span class="built_in">list</span>(counter.values())), self.sample_ratio)</span><br><span class="line">        normalizing_factor = probs.<span class="built_in">sum</span>()</span><br><span class="line">        probs = np.divide(probs, normalizing_factor)</span><br><span class="line">        </span><br><span class="line">        sample_table = []</span><br><span class="line"></span><br><span class="line">        table_size = <span class="number">1e8</span></span><br><span class="line">        word_share_list = np.<span class="built_in">round</span>(probs * table_size)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         the higher prob, the more shares in  sample_table</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> w_idx, w_fre <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_share_list):</span><br><span class="line">            sample_table += [words[w_idx]] * <span class="built_in">int</span>(w_fre)</span><br><span class="line"></span><br><span class="line"><span class="comment">#         sample_table = np.array(sample_table) // too slow</span></span><br><span class="line">        <span class="keyword">return</span> sample_table</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, sample_size=<span class="number">6</span></span>):</span></span><br><span class="line">        negatvie_samples = [self.sample_table[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> np.random.randint(<span class="number">0</span>, self.table_size, sample_size)]</span><br><span class="line">        <span class="keyword">return</span> np.array(negatvie_samples)</span><br></pre></td></tr></table></figure><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sampler = NegativeSampler(corpus)</span><br><span class="line">sampler.generate()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([&#39;visiting&#39;, &#39;defiled&#39;, &#39;thieves&#39;, &#39;beyond&#39;, &#39;lord&#39;, &#39;fill&#39;],</span><br><span class="line">      dtype&#x3D;&#39;&lt;U18&#39;)</span><br></pre></td></tr></table></figure><h2 id="Skipgram-NEG"><a href="#Skipgram-NEG" class="headerlink" title="Skipgram + NEG"></a>Skipgram + NEG</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGramNEG</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim) <span class="comment"># |V| x |K|</span></span><br><span class="line">        self.neg_syn1 = nn.Embedding(vocab_size, embedding_dim) <span class="comment"># |V| x |K|</span></span><br><span class="line">        torch.nn.init.constant_(self.neg_syn1.weight.data, val=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, center: torch.Tensor, context: torch.Tensor, negative_samples: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># center : [b_size, 1]</span></span><br><span class="line">        <span class="comment"># context: [b_size, 1]</span></span><br><span class="line">        <span class="comment"># negative_sample: [b_size, negative_sample_num]</span></span><br><span class="line">        embd_center = self.syn0(center)  <span class="comment"># [b_size, 1, embedding_dim]</span></span><br><span class="line">        embd_context = self.neg_syn1(context) <span class="comment"># [b_size, 1, embedding_dim]</span></span><br><span class="line">        embd_negative_sample = self.neg_syn1(negative_samples) <span class="comment"># [b_size, negative_sample_num, embedding_dim]</span></span><br><span class="line">        </span><br><span class="line">        prod_p =  (embd_center * embd_context).<span class="built_in">sum</span>(dim=<span class="number">1</span>).squeeze()  <span class="comment"># [b_size]</span></span><br><span class="line">        loss_p =  F.logsigmoid(prod_p).mean() <span class="comment"># 1</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        prod_n = (embd_center * embd_negative_sample).<span class="built_in">sum</span>(dim=<span class="number">2</span>) <span class="comment"># [b_size, negative_sample_num]</span></span><br><span class="line">        loss_n = F.logsigmoid(-prod_n).<span class="built_in">sum</span>(dim=<span class="number">1</span>).mean() <span class="comment"># 1</span></span><br><span class="line">        <span class="keyword">return</span> -(loss_p + loss_n)</span><br></pre></td></tr></table></figure><ul><li>syn0 對應到 input side 的  matrix $W$</li><li>neg_syn1 對應到 output side 的  matrix $W’$<ul><li>Tomas Mikolov 在 WordVec c code 初始化為 0</li></ul></li><li>loss function<ul><li>loss_p 對應到 $\log \sigma(\text{v}^\top_{w_I} \text{v}’_{w_{O,j}})$</li><li>loos_n 對應到 $\sum^M_{\substack{i=1 \\ \tilde{w}_i \sim Q}}\exp(\text{v}^\top_{w_I} \text{v}’_{\tilde{w}_i})$</li></ul></li></ul><h2 id="Training-Skipgram-Negative-Sampling"><a href="#Training-Skipgram-Negative-Sampling" class="headerlink" title="Training Skipgram + Negative Sampling"></a>Training Skipgram + Negative Sampling</h2><p>訓練過程省略，參閱 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Embedding">seed9D/hands-on-machine-learning</a></p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h3 id="取回-embedding"><a href="#取回-embedding" class="headerlink" title="取回 embedding"></a>取回 embedding</h3><p>簡單的把 syn0 跟 neg_syn1 平均</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">syn0 = model.syn0.weight.data</span><br><span class="line">neg_syn1 = model.neg_syn1.weight.data</span><br><span class="line"></span><br><span class="line">w2v_embedding = (syn0 + neg_syn1) / <span class="number">2</span></span><br><span class="line">w2v_embedding = w2v_embedding.numpy()</span><br><span class="line">l2norm = np.linalg.norm(w2v_embedding, <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">w2v_embedding = w2v_embedding / l2norm</span><br></pre></td></tr></table></figure><h3 id="Cosine-similarity"><a href="#Cosine-similarity" class="headerlink" title="Cosine similarity"></a>Cosine similarity</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineSimilarity</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word_embedding, idx_to_word_dict, word_to_idx_dict</span>):</span></span><br><span class="line">        self.word_embedding = word_embedding <span class="comment"># normed already</span></span><br><span class="line">        self.idx_to_word_dict = idx_to_word_dict</span><br><span class="line">        self.word_to_idx_dict = word_to_idx_dict</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_synonym</span>(<span class="params">self, word, topK=<span class="number">10</span></span>):</span></span><br><span class="line">        idx = self.word_to_idx_dict[word]</span><br><span class="line">        embed = self.word_embedding[idx]</span><br><span class="line">        </span><br><span class="line">        cos_similairty = w2v_embedding @ embed</span><br><span class="line">        </span><br><span class="line">        topK_index = np.argsort(-cos_similairty)[:topK]</span><br><span class="line">        pairs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> topK_index:</span><br><span class="line">            w = self.idx_to_word_dict[i]</span><br><span class="line">            pairs.append((w, cos_similairty[i]))</span><br><span class="line">        <span class="keyword">return</span> pairs</span><br></pre></td></tr></table></figure><p>訓練語料是聖經，看看 jesus  跟  christ 的相近詞</p><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)</span><br><span class="line">cosinSim.get_synonym(<span class="string">&#x27;christ&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;christ&#x27;</span>, <span class="number">1.0</span>),</span><br><span class="line"> (<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.7170907</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.4621805</span>),</span><br><span class="line"> (<span class="string">&#x27;peter&#x27;</span>, <span class="number">0.39412546</span>),</span><br><span class="line"> (<span class="string">&#x27;disciples&#x27;</span>, <span class="number">0.3873747</span>),</span><br><span class="line"> (<span class="string">&#x27;noise&#x27;</span>, <span class="number">0.28152165</span>),</span><br><span class="line"> (<span class="string">&#x27;asleep&#x27;</span>, <span class="number">0.26372147</span>),</span><br><span class="line"> (<span class="string">&#x27;taught&#x27;</span>, <span class="number">0.2422184</span>),</span><br><span class="line"> (<span class="string">&#x27;zarhites&#x27;</span>, <span class="number">0.24168596</span>),</span><br><span class="line"> (<span class="string">&#x27;nobles&#x27;</span>, <span class="number">0.23950878</span>)]</span><br></pre></td></tr></table></figure><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cosinSim.get_synonym(<span class="string">&#x27;jesus&#x27;</span>)</span><br></pre></td></tr></table></figure><p>out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;jesus&#x27;</span>, <span class="number">1.0</span>),</span><br><span class="line"> (<span class="string">&#x27;christ&#x27;</span>, <span class="number">0.7170907</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.5360588</span>),</span><br><span class="line"> (<span class="string">&#x27;peter&#x27;</span>, <span class="number">0.3603956</span>),</span><br><span class="line"> (<span class="string">&#x27;disciples&#x27;</span>, <span class="number">0.3460646</span>),</span><br><span class="line"> (<span class="string">&#x27;church&#x27;</span>, <span class="number">0.2755898</span>),</span><br><span class="line"> (<span class="string">&#x27;passed&#x27;</span>, <span class="number">0.24744174</span>),</span><br><span class="line"> (<span class="string">&#x27;noise&#x27;</span>, <span class="number">0.23768528</span>),</span><br><span class="line"> (<span class="string">&#x27;preach&#x27;</span>, <span class="number">0.23454829</span>),</span><br><span class="line"> (<span class="string">&#x27;send&#x27;</span>, <span class="number">0.2337867</span>)]</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://github.com/tmikolov/word2vec">https://github.com/tmikolov/word2vec</a><ul><li>c code</li></ul></li><li>word2vec的PyTorch实现 <a href="https://samaelchen.github.io/word2vec_pytorch/">https://samaelchen.github.io/word2vec_pytorch/</a><ul><li>CBOW + NEG</li></ul></li><li><a href="https://rguigoures.github.io/word2vec_pytorch/">https://rguigoures.github.io/word2vec_pytorch/</a><ul><li>CBOW + NEG</li></ul></li><li><a href="https://github.com/ilyakhov/pytorch-word2vec">https://github.com/ilyakhov/pytorch-word2vec</a></li><li>Word2Vec Tutorial Part 2 - Negative Sampling <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li><li>基于PyTorch实现word2vec模型 <a href="https://lonepatient.top/2019/01/18/Pytorch-word2vec.html#pytorch%E5%AE%9E%E7%8E%B0">https://lonepatient.top/2019/01/18/Pytorch-word2vec.html</a></li><li>other<ul><li><a href="https://github.com/Adoni/word2vec_pytorch/blob/master/model.py">https://github.com/Adoni/word2vec_pytorch/blob/master/model.py</a></li><li><a href="http://medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4">medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch 實作 Word2Vec with Softmax</title>
      <link href="Pytorch-Implement-Naive-Word2Vec-with-Softmax/"/>
      <url>Pytorch-Implement-Naive-Word2Vec-with-Softmax/</url>
      
        <content type="html"><![CDATA[<p>用  pytorch  實現最簡單版本的  CBOW 與 skipgram，objective function 採用 minimize negative log likelihood with softmax</p><a id="more"></a><h1 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h1><p>CBOW  的思想是用兩側 context 詞預測中間 center 詞，context 詞有數個，視 window size 大小而定</p><script type="math/tex; mode=display">P(center|context;\theta)</script><p><img src="https://i.imgur.com/N0SXV8P.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><p>Condition probability $P(center | context; \theta)$ 中 variable $\textit{center word}$  有限，所以是個 descrete probability，可以轉化成多分類問題來解</p><p>令 $w_O$ 表 center word, $w_I$ 表 input 的 context word，則</p><script type="math/tex; mode=display">P(center|context;\theta) = P(w_O|w_I; \theta) =  \cfrac{\exp(h^\top \text{v}^{'}_{w_{O}})}{\sum_{w_ \in V}\exp(h^\top \text{v}'_{w_i})}</script><ul><li>$h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\cfrac{1}{C}(\text{v}_{w_1} + \text{v}_{w_2}+ …+ \text{v}_{w_C})^T$</li></ul><p>訓練過程 $\text{maximize log of condition probability } P(w_O|w_I; \theta$</p><script type="math/tex; mode=display">\begin{aligned} & \text{maxmize}_\theta  \ \log P(w_O|w_I; \theta)\\&  = \text{minimize}_\theta \ -\log \ P(w_O|w_I; \theta)\\& = \text{minimize}_\theta  \ - \log \cfrac{\exp(h^\top \text{v}^{'}_{w_{O}})}{\sum_{w_i  \in V} \exp(h^\top \text{v}^{'}_{w_i})} \end{aligned}</script><h2 id="Pytorch-CBOW-softmax"><a href="#Pytorch-CBOW-softmax" class="headerlink" title="Pytorch  CBOW + softmax"></a>Pytorch  CBOW + softmax</h2><h3 id="CBOW-softmax-模型定義"><a href="#CBOW-softmax-模型定義" class="headerlink" title="CBOW + softmax 模型定義"></a>CBOW + softmax 模型定義</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOWSoftmax</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.syn1 = nn.Linear(embedding_dim, vocab_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, context, center</span>):</span></span><br><span class="line">        <span class="comment">#  context: [b_size, windows_size]</span></span><br><span class="line">        <span class="comment">#  center: [b_size, 1]</span></span><br><span class="line">        embds = self.syn0(context).mean(dim=<span class="number">1</span>) <span class="comment"># [b_size, embedding_dim]</span></span><br><span class="line">        out = self.syn1(embds)</span><br><span class="line">        </span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        loss = F.nll_loss(log_probs, center.view(-<span class="number">1</span>), reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li><p>syn0 對應到 input 側的 embedding matrix $W$</p></li><li><p>syn1 對應到 output 側的 embedding matrix  $W’$</p></li><li><p>loss 的計算</p><p>$- log \cfrac{\exp(h^\top \text{v}^{‘}_{w_{O}})}{\sum_{w_i  \in V} \exp(h^\top \text{v}^{‘}_{w_i})}$</p></li><li><p>input: context  跟 center 內容都是將 word index  化</p><p><img src="https://i.imgur.com/1PILCdT.png" style="zoom: 50%;" /></p></li><li><p>因爲 context 是由 windows size N 個 words 組成，所以總共有 N 個 word embedding ，常規操作是 sum or  mean</p></li></ul><h3 id="Training-Stage"><a href="#Training-Stage" class="headerlink" title="Training Stage"></a>Training Stage</h3><p>訓練過程省略，有興趣的可以去 github 看 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/blob/main/Embedding/CBOW_softmax.ipynb">seed9D/hands-on-machine-learning</a></p><h3 id="取出-Embedding"><a href="#取出-Embedding" class="headerlink" title="取出 Embedding"></a>取出 Embedding</h3><p>創建一個衡量 cosine similarity的 class</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineSimilarity</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word_embedding, idx_to_word_dict, word_to_idx_dict</span>):</span></span><br><span class="line">        self.word_embedding = word_embedding <span class="comment"># normed already</span></span><br><span class="line">        self.idx_to_word_dict = idx_to_word_dict</span><br><span class="line">        self.word_to_idx_dict = word_to_idx_dict</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_synonym</span>(<span class="params">self, word, topK=<span class="number">10</span></span>):</span></span><br><span class="line">        idx = self.word_to_idx_dict[word]</span><br><span class="line">        embed = self.word_embedding[idx]</span><br><span class="line">        </span><br><span class="line">        cos_similairty = w2v_embedding @ embed</span><br><span class="line">        </span><br><span class="line">        topK_index = np.argsort(-cos_similairty)[:topK]</span><br><span class="line">        pairs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> topK_index:</span><br><span class="line">            w = self.idx_to_word_dict[i]</span><br><span class="line"><span class="comment">#             pairs[w] = cos_similairty[i]</span></span><br><span class="line">            pairs.append((w, cos_similairty[i]))</span><br><span class="line">        <span class="keyword">return</span> pairs</span><br></pre></td></tr></table></figure><p>僅使用 syn0 做為 embedding，記得 L2 norm </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">syn0 = model.syn0.weight.data</span><br><span class="line"></span><br><span class="line">w2v_embedding = syn0 </span><br><span class="line">w2v_embedding = w2v_embedding.numpy()</span><br><span class="line">l2norm = np.linalg.norm(w2v_embedding, <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">w2v_embedding = w2v_embedding / l2norm</span><br></pre></td></tr></table></figure><p>訓練的 corpus 是聖經，所以簡單看下 jesus 與 christ 兩個 word 的相似詞，效果不予置評</p><p><img src="https://i.imgur.com/W2w6p65.png" alt="Pytorch%20Implement%20Naive%20Word2Vec%20with%20Softmax%20ae605d15ce0e403694f9d8049c1f2354/Untitled%202.png"></p><h1 id="Skipgram"><a href="#Skipgram" class="headerlink" title="Skipgram"></a>Skipgram</h1><p>skipgram  的思想是用中心詞 center word 去預測兩側的 context words </p><script type="math/tex; mode=display">P(context|center; \theta)</script><p><img src="https://i.imgur.com/AoD1UHk.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><p>令 $w_I$ 表 input 的 center word， $w_{O,j}$ 表 target 的 第  $j$  個 context word ，則 condition  probability</p><script type="math/tex; mode=display">P(context|center;\theta) = P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) = \prod^C_{c=1 }\cfrac{\exp(h^\top \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(h^\top \text{v}'_{w_i})}</script><ul><li>$h$ 表 hidden layer 的輸出，在 skipgram 實際上就是 $\text{v}_{w_I}$</li></ul><p>Skipgram  的  objective function</p><script type="math/tex; mode=display">\begin{aligned} & -\log P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) \\ & = -\log \prod^C_{c=1}\cfrac{\exp(h^\top \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(h^{\top} \text{v}'_{w_i})}\\ & = -\log \prod^C_{c=1}\cfrac{\exp(\text{v}^\top_{w_I} \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(\text{v}^\top_{w_I} \text{v}'_{w_i})}\\& = -\sum^C_{c=1}\log \cfrac{\exp(\text{v}^\top_{w_I} \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(\text{v}^\top_{w_I} \text{v}'_{w_i})}\end{aligned}</script><h2 id="Pytorch-skipgram-softmax"><a href="#Pytorch-skipgram-softmax" class="headerlink" title="Pytorch  skipgram + softmax"></a>Pytorch  skipgram + softmax</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipgramSoftmax</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim)  <span class="comment"># |V| x |K|</span></span><br><span class="line">        self.syn1 = nn.Linear(embedding_dim, vocab_size)  <span class="comment"># |K| x |V|</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, center, context</span>):</span></span><br><span class="line">        <span class="comment"># center: [b_size, 1]</span></span><br><span class="line">        <span class="comment"># context: [b_size, 1]</span></span><br><span class="line">        embds = self.syn0(center.view(-<span class="number">1</span>))</span><br><span class="line">        out = self.syn1(embds)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        loss = F.nll_loss(log_probs, context.view(-<span class="number">1</span>), reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>syn0 對應到 input 側的 embedding matrix $W$</li><li>syn1 對應到 output 側的 embedding matrix  $W’$</li></ul><p>實際上，skipgram 每筆 training data 只需要 (a center word, a context word) 的 pair 即可</p><p><img src="https://i.imgur.com/dsXqMKo.png" style="zoom:50%;" /></p><p> 所以  loss function 實現上非常簡單</p><script type="math/tex; mode=display">-\log \cfrac{\exp(\text{v}^\top_{w_I} \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(\text{v}^\top_{w_I} \text{v}'_{w_i})}</script><h3 id="Training-Stage-1"><a href="#Training-Stage-1" class="headerlink" title="Training Stage"></a>Training Stage</h3><p>訓練過程省略，有興趣的可以去 github 看 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Embedding">seed9D/hands-on-machine-learning</a></p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>取出 embedding，這次 embedding  嘗試 $(W + W’)/2$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">syn0 = model.syn0.weight.data</span><br><span class="line">syn1 = model.syn1.weight.data</span><br><span class="line"></span><br><span class="line">w2v_embedding = (syn0 + syn1) / <span class="number">2</span></span><br><span class="line">w2v_embedding = w2v_embedding.numpy()</span><br><span class="line">l2norm = np.linalg.norm(w2v_embedding, <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">w2v_embedding = w2v_embedding / l2norm</span><br></pre></td></tr></table></figure><p>一樣看 jesus 跟 christ 的相似詞，感覺似乎比 CBOW 好一點</p><p><img src="https://i.imgur.com/2pS6zCo.png" style="zoom:50%;" /></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html</a></li><li><a href="https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb">https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb</a></li><li>基于PyTorch实现word2vec模型 <a href="https://lonepatient.top/2019/01/18/Pytorch-word2vec.htm">https://lonepatient.top/2019/01/18/Pytorch-word2vec.htm</a></li><li>Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from <a href="http://arxiv.org/abs/1411.2738">http://arxiv.org/abs/1411.2738</a></li><li><a href="https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py">https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Negative Sampling 背後的數學</title>
      <link href="negative-sampling-in-word2vec/"/>
      <url>negative-sampling-in-word2vec/</url>
      
        <content type="html"><![CDATA[<p>以下用 Skip-gram 為例</p><p><img src="https://i.imgur.com/YeDgnQ9.png" style="zoom: 50%;" /></p><a id="more"></a><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h2 id="Noise-Contrastive-Estimation-NCE"><a href="#Noise-Contrastive-Estimation-NCE" class="headerlink" title="Noise Contrastive Estimation (NCE)"></a>Noise Contrastive Estimation (NCE)</h2><blockquote><p>NCE attempts to approximately maximize the log probability of the softmax output</p></blockquote><ul><li>The Noise Contractive Estimation metric intends to differentiate the target word from noise sample using a logistic regression classifier</li></ul><p><img src="https://i.imgur.com/m2gAqLM.png" style="zoom: 50%;" /></p><h3 id="從-cross-entropy-說起"><a href="#從-cross-entropy-說起" class="headerlink" title="從 cross entropy 說起"></a>從 cross entropy 說起</h3><p> True label $y_i$ is 1 only when $w_i$ is the output word:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \sum_{i=1}^V y_i \log p(w_i | w_I) = - \log p(w_O \vert w_I)</script><p>又</p><p>$p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ </p><ul><li>$\text{v}^{‘}_{w_i}$ is  vector of word $w_i$ in $W^{‘}$</li><li>$w_O$ is the output word in $V$</li><li>$w_I$ is the input word in $V$</li></ul><p>代入後</p><script type="math/tex; mode=display">\mathcal{L}_{\theta} = - \log \frac{\exp({\text{v}'_{w_O}}^{\top}{\text{v}_{w_I}})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top}{\text{v}_{w_I} })}= - {\text{v}'_{w_O}}^{\top}{\text{v}_{w_I} } + \log \sum_{i=1}^V \exp({\text{v}'_{w_i} }^{\top}{\text{v}_{w_I}})</script><p>Compute gradient of loss function w.s.t mode’s parameter $\theta$，令 $z_{IO} = {\text{v}’_{w_O}}^{\top}{\text{v}_{w_I}}$ ; $z_{Ii} = {\text{v}’_{w_i}}^{\top}{\text{v}_{w_I}}$</p><script type="math/tex; mode=display">\begin{aligned}\nabla_\theta \mathcal{L}_{\theta}&= \nabla_\theta\big( - z_{IO} + \log \sum_{i=1}^V e^{z_{Ii}} \big) \\ &= - \nabla_\theta z_{IO} + \nabla_\theta \big( \log \sum_{i=1}^V e^{z_{Ii}} \big) \\&= - \nabla_\theta z_{IO} + \frac{1}{\sum_{i=1}^V e^{z_{Ii}}} \sum_{i=1}^V e^{z_{Ii}} \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \sum_{i=1}^V \frac{e^{z_{Ii}}}{\sum_{i=1}^V e^{z_{Ii}}} \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta z_{Ii}\end{aligned}</script><p>可以看出，gradient $\nabla_{\theta}\mathcal{L}_{\theta}$是由兩部分組成 :</p><ol><li>a positive reinforcement for the target word $w_O$,  $\nabla_{\theta}z_{O}$</li><li>a negative reinforcement for all other words $w_i$, which weighted by their probability, $\sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta z_{Ii}$</li></ol><p><strong>Second term actually is just the expectation  of the gradient $\nabla_{\theta}z_{Ii}$ for all words $w_i$ in $V$。</strong></p><p>And probability distribution $Q(\tilde{w})$ could see as the distribution of noise samples</p><h3 id="NCE-sample-原理"><a href="#NCE-sample-原理" class="headerlink" title="NCE sample 原理"></a>NCE sample 原理</h3><p>According to gradient of loss function $\nabla_{\theta}\mathcal{L}_{\theta}$</p><script type="math/tex; mode=display">\nabla_{\theta}\mathcal{L}_{\theta} =- \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta z_{Ii}</script><p>Given an input word $w_I$, the correct output word is known as $w$，我們同時可以從 noise sample distribution $Q$  sample 出 $M$ 個 samples $\tilde{w}_1<br>, \tilde{w}_2, \dots, \tilde{w}_M \sim Q$ 來近似 cross entropy gradient 的後半部分</p><p>現在我們手上有個 correct output word $w$ 和 $M$ 個從 $Q$ sample 出來的 word $\tilde{w}$ ， 假設我們有一個  binary classifier 負責告訴我們，手上的 word $w$，哪個是 correct  $p(d=1 | w, w_I)$，哪個是 negative $p(d=0 | \tilde{w}, w_I)$</p><p>於是 loss function  改寫成：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) + \sum_{i=1, \tilde{w}_i \sim Q}^M \log p(d=0|\tilde{w}_i, w_I) ]</script><p>According to the law of large numbers $E_{p(x)} [ f(x)] \approx \frac{1}{n} \sum^{n}_{i=1}f(x_i)$，we could simplify:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) +  M \mathbb{E}_{\tilde{w}_i \sim Q} \log p(d=0|\tilde{w}_i, w_I)]</script><p>$p(d |w, w_I)$ 可以從 joint probability $p(d, w|w_I)$ 求</p><ol><li><p>$p(d, w | w_I) =<br>\begin{cases}<br>  \frac{1}{M+1} p(w \vert w_I) &amp; \text{if } d=1 \\<br>  \frac{M}{M+1} q(\tilde{w}) &amp; \text{if } d=0<br>  \end{cases}$</p><ul><li>$d$  is binary value</li><li>$M$ is number of samples，we have a correct output word $w$ and sample M word from distribution $Q$</li></ul></li><li><p>因爲 $p(d| w, w_I) = \frac{p(d, w, w_I)}{p(w, w_I)} = \frac{p(d, w | w_I) p(w_I)}{p(w|w_I)p(w_I)} = \frac{p(d, w| w_I)}{\sum_dp(d,w| w_I)}$</p><p>可以得出</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{Mq(\tilde{w})}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script></li></ol><ul><li>$q(\tilde{w})$ 表從 distribution $Q$ sample 出 word $\tilde{w}$ 的 probability</li></ul><p>最終 loss function of NCE</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_\theta & = - [ \log p(d=1 \vert w, w_I) +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log p(d=0|\tilde{w}_i, w_I)] \\& = - [ \log \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})} +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{p(w \vert w_I) + Mq(\tilde{w}_i)}]\end{aligned}</script><p>$p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 代入 $\mathcal{L}_{\theta}$</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\theta} &= -[log\frac{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})}}{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})+ Mq(\tilde{w})}} + \sum_{\substack{i=1 \\ \tilde{w}_i\sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})}+ Mq(\tilde{w}_i)}]\end{aligned}</script><p>可以看到 normalizer $Z(w) = {\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 依然要 sum up the entire vocabulary，實務上通常會假設 $Z(w)\approx1$，所以 $\mathcal{L}_\theta$ 簡化成:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \frac{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}})}{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}}) + Mq(\tilde{w})} +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}}) + Mq(\tilde{w}_i)}]</script><h3 id="關於-Noise-distribution-Q"><a href="#關於-Noise-distribution-Q" class="headerlink" title="關於 Noise distribution $Q$"></a>關於 Noise distribution $Q$</h3><p>關於 noise distribution $Q$，在設計的時候通常會考慮</p><ul><li>it should intuitively be very similar to the real data distribution.</li><li>it should be easy to sample from.</li></ul><h2 id="Negative-Sampling-NEG"><a href="#Negative-Sampling-NEG" class="headerlink" title="Negative Sampling (NEG)"></a>Negative Sampling (NEG)</h2><p>Negative sampling can be seen as an approximation to NCE</p><ul><li>Noise Contrastive Estimation attempts to approximately maximize the log probability of the softmax output</li><li>The objective of NEG is to <strong>learn high-quality word representations</strong> rather than achieving low perplexity on a test set, as is the goal in language modeling</li></ul><h3 id="從-NCE-說起"><a href="#從-NCE-說起" class="headerlink" title="從 NCE 說起"></a>從 NCE 說起</h3><p>NCE  $p(d|w, w_I)$ equation，$p(d=1 |w, w_I)$ 代表 word $w$ 是 output word 的機率; $p(d=0|w, w_I)$ 表 sample 出 noise word 的機率</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{Mq(\tilde{w})}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><p>NCE 假設  $p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 中的分母 $Z(w) = {\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})} = 1$，所以簡化成</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}&= \frac{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i})}{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) + Mq(\tilde{w})}\end{aligned}</script><h3 id="NEG-繼續化簡"><a href="#NEG-繼續化簡" class="headerlink" title="NEG 繼續化簡"></a>NEG 繼續化簡</h3><p><strong>NEG 繼續假設 $Nq(\tilde{w}) = 1$</strong> 式子變成</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &=\frac{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i})}{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) + 1}&=\frac{1}{1  +\exp(-{\text{v}^{'}_w}^{\top}\text{v}_{w_i})} = \sigma({\text{v}^{'}_w}^{\top}\text{v}_{w_i})\end{aligned}</script><script type="math/tex; mode=display">p(d=0|w, w_I) = 1 - \sigma({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) = \sigma(-{\text{v}^{'}_w}^{\top}\text{v}_{w_i})</script><p>最終得到 loss function </p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \sigma({\text{v}'_{w}}^\top \text{v}_{w_I}) +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \sigma(-{\text{v}'_{\tilde{w}_i}}^\top \text{v}_{w_I})]</script><p>前項是 positive sample $p(d=1 \vert w, w_I)$，後項是 M 個 negative samples $p(d=0|w, w_I) $</p><p>在 skipgram with negative sampling 上</p><ul><li>$\text{v}_{w_I}$ 表 input 的 center word  $w_I$ 的 vector，來自 $W$</li><li>$\text{v}’_{w}$ 表 output side  的一個 context word  $w$ 的 vector， 來自 $W’$</li></ul><p>實作上  skipgram 一個單位的 data sample 只會包含 一個 center word 與 一個 context word  的 pair 對 $(w_I, w_{C,j})$</p><p>參閱  <a href="/Pytorch-Implement-Skipgram-with-Negative-Sampling/" title="Pytorch 實作 Skipgram with Negative Sampling">Pytorch 實作 Skipgram with Negative Sampling</a></p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><ul><li>NEG 實際上目的跟 Hierarchical Softmax 不一樣，並不直接學 likelihood of correct word，而是直接學 words 的 embedding，也就是更好的 word distribution representation</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Learning Word Embedding <a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions</a><ul><li>此篇從 skip gram 講解 negative sampling</li></ul></li><li>On word embeddings - Part 2: Approximating the Softmax <a href="https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches">https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches</a><ul><li>此篇從 CBOW 講解 negative sampling</li></ul></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722">http://arxiv.org/abs/1402.3722</a></li><li>Word2Vec Tutorial Part 2 - Negative Sampling [<a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hierarchical Softmax 背後的數學</title>
      <link href="hierarchical-softmax-in-word2vec/"/>
      <url>hierarchical-softmax-in-word2vec/</url>
      
        <content type="html"><![CDATA[<h1 id="以-CBOW-為例"><a href="#以-CBOW-為例" class="headerlink" title="以 CBOW 為例"></a>以 CBOW 為例</h1><p><img src="https://i.imgur.com/Pbdqtx9.png" style="zoom:50%;" /></p><a id="more"></a><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>$\text{v}’_j$ 表 $W’$  中  j-th columns  vector</li><li>在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是  leaf node  ，跟 column vector $\text{v}’_j$ 與 word $w_i$  不是直接對應的關係</li></ul></li></ul><p>Hierarchical Softmax 優化了原始 softmax 分母 normalization 項需要對整個 vocabulary set 內的所有 word 內積，原始 softmax 如下</p><script type="math/tex; mode=display">p(w | c) = \dfrac{\text{exp}({h^\top \text{v}'_w})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><ul><li>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$，is average of input context words’ vector representation in $W$</li></ul><p>Hierarchical Softmax build a full binary tree to avoid computation over all vocabulary，示意圖：</p><p><img src="https://i.imgur.com/WnaPO5L.png" style="zoom:50%;" /></p><ol><li>每個 leaf node 代表一個 word $w_i$</li><li>Matrix $W^{‘}$  就是所有 non-leaf node $n$ 代表的 vector $\text{v}^{‘}_n$ 集合，與 word $w_i$ 無一對一對應關係</li></ol><p>Binary tree 中每個 node 分岔的 probability 是個 binary classification problem</p><p>$p(n, \text{left}) = \sigma({\text{v}’_n}^{\top} h)$</p><p>$p(n, \text{righ}) = 1 - p(\text{left},n) = \sigma(-{\text{v}’_n}^{\top} h)$</p><ul><li>$\text{v}^{‘}_{n}$  代表 node $n$ 的 vector</li></ul><p>則每個 word $w_O = w_i$ 的 generate probability $p(w_i)$ 為其從 root node 分岔到 lead node 的 probability</p><p>$p(w_i = w_O) = \prod^{L(w_O)-1}_{j=1} \sigma(\mathbb{I}_{\text{turn}}(n(w_O, j), n(w_O, j + 1) \cdot {v^{‘}_{n(w_O, j)}}^{\top}h)$</p><ul><li><p>$w_O$ 表 output word 的意思</p></li><li><p>$L(w_O)$ is the depth of the path leading to the output word $w_O$</p></li><li><p>$\mathbb{I}_{turn}$  is a specially indicator function</p><p>1 if $n(w_O, k+1)$ is the <strong>left</strong> child of $n(w_O, k)$</p><p>-1 if $n(w_O, k+1)$ is the <strong>right</strong> child of $n(w_O, k)$</p></li><li><p>$n(w, j)$ means the $j$ th unit on the path from root to the word $w$</p></li><li><p>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$ average of all context word vector</p></li></ul><h2 id="簡單的例子"><a href="#簡單的例子" class="headerlink" title="簡單的例子"></a>簡單的例子</h2><p>Ex 1: Following the path from the root to leaf node of $w_2$, we can compute the probability of $w_2$ $p(w_2)$:</p><p><img src="https://i.imgur.com/swoUyHO.png" style="zoom:50%;" /></p><p>Ex 2:</p><p><img src="https://i.imgur.com/oomh3Bv.png" alt="" style="zoom: 33%;" /></p><ul><li>$\sum^{V}_{i=1} p(w_i = w_O) = 1$</li></ul><p>probability $p(\text{cat}| context)$,  是由 $ node1  \stackrel{\text{left}}{\to} node \stackrel{\text{right}}{\to}  node 5 \stackrel{\text{right}}{\to}  cat  $  這條路徑組成</p><p>其中 context words  經過  hidden layer 後的輸出為 $h(\text{context words})$</p><h2 id="為什麼-Hierarchical-Softmax-可以減少-Time-Complexity"><a href="#為什麼-Hierarchical-Softmax-可以減少-Time-Complexity" class="headerlink" title="為什麼 Hierarchical Softmax 可以減少 Time Complexity?"></a>為什麼 Hierarchical Softmax 可以減少 Time Complexity?</h2><p>透過  Hierarchical Softmax ， 原本計算 $p(w|c)$  需要求所有 word  $w_i$ 的 vector $\text{v}_{w_i}$對 $h$ 的內積，現在只需要計算路徑上的節點數 $L(w_i) -1$，又 HS 是 full binary tree ，其最大深度為 $\log_2|V|$</p><p><strong>So we only need to evaluate at most $log_2|V|$</strong></p><h1 id="Hierarchical-Softmax-如何-update-參數"><a href="#Hierarchical-Softmax-如何-update-參數" class="headerlink" title="Hierarchical Softmax 如何 update 參數"></a>Hierarchical Softmax 如何 update 參數</h1><h2 id="Error-Funtion-of-Hierarchical-Softmax"><a href="#Error-Funtion-of-Hierarchical-Softmax" class="headerlink" title="Error Funtion of Hierarchical Softmax"></a>Error Funtion of Hierarchical Softmax</h2><p>Error function $E$ is negative log likelihood</p><p><img src="https://i.imgur.com/vYCY3GR.png" style="zoom:50%;" /></p><p><img src="https://i.imgur.com/lmxXtuJ.png" style="zoom:50%;" /></p><ul><li>$L(w_i) -1$ 表 從 root node  到 leaf node of $w_i$ 的 node number</li><li>$[ \cdot ]$表分岔判斷</li><li>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$ average of all context word vector</li></ul><p>And we use <strong>gradient decent</strong> to update $\text{v}^{‘}_j$  and $h$ in $W’$ and $W’$</p><h2 id="Calculate-the-Derivate-E-with-Regard-to-text-v-‘-top-jh"><a href="#Calculate-the-Derivate-E-with-Regard-to-text-v-‘-top-jh" class="headerlink" title="Calculate the Derivate $E$ with Regard to  $\text{v}^{‘\top}_jh$"></a>Calculate the Derivate $E$ with Regard to  $\text{v}^{‘\top}_jh$</h2><p>先求 total loss 對 $\text{v}^{‘\top}_jh$ 的 gradient</p><p><img src="https://i.imgur.com/JejjNsl.png" style="zoom:50%;" /></p><ul><li>$\sigma^{‘}(x) = \sigma(x)[1 - \sigma(x)]$</li><li>$[\log\sigma(x)]^{‘} = 1 - \sigma(x)$   ⇒    $[log(1 - \sigma(x)]^{‘} = -\sigma(x)$</li></ul><h2 id="Calculate-the-Derivate-E-with-Regard-to-text-v-‘-j"><a href="#Calculate-the-Derivate-E-with-Regard-to-text-v-‘-j" class="headerlink" title="Calculate the Derivate $E$ with Regard to  $\text{v}^{‘}_j$"></a>Calculate the Derivate $E$ with Regard to  $\text{v}^{‘}_j$</h2><p>根據 chain rule 可以求出 total loss 對 huffman tree node vector $\text{v}^{‘}_j$ 的 gradient</p><p><img src="https://i.imgur.com/RbiLvNW.png" alt="" style="zoom:50%;" /></p><h3 id="Update-Equation"><a href="#Update-Equation" class="headerlink" title="Update Equation"></a>Update Equation</h3><p><img src="https://i.imgur.com/hLzWOpf.png" style="zoom:50%;" /></p><h2 id="Calculate-the-Derivate-E-with-Regard-to-h"><a href="#Calculate-the-Derivate-E-with-Regard-to-h" class="headerlink" title="Calculate the Derivate $E$ with Regard to $h$"></a>Calculate the Derivate $E$ with Regard to $h$</h2><p> 最後求 total loss 對 hidden layer outpot $h$ 的 gradient</p><p><img src="https://i.imgur.com/BLGopRf.png" style="zoom:50%;" /></p><ul><li>$EH$: an N-dim vector, is the sum of the output vector of all words in the vocabulary, weighted by their prediction error</li></ul><h3 id="Update-Equation-1"><a href="#Update-Equation-1" class="headerlink" title="Update Equation"></a>Update Equation</h3><p>Because hidden vector $h$ is composed with all the context word $w_{I,c}$</p><p><img src="https://i.imgur.com/W4aEVVm.png" style="zoom:50%;" /></p><ul><li>$\text{v}_{w_{I,c}}$ is the input vector of c-th word in input context</li></ul><h1 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h1><p>CBOW + HS 實現  <a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Pytorch 實作 CBOW with Hierarchical Softmax">Pytorch 實作 CBOW with Hierarchical Softmax</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Language Models, Word2Vec, and Efficient Softmax Approximations <a href="https://rohanvarma.me/Word2Vec/">https://rohanvarma.me/Word2Vec/</a></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722">http://arxiv.org/abs/1402.3722</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP Language Model</title>
      <link href="NLP-language-model/"/>
      <url>NLP-language-model/</url>
      
        <content type="html"><![CDATA[<h1 id="General-Form"><a href="#General-Form" class="headerlink" title="General Form"></a>General Form</h1><script type="math/tex; mode=display">p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_1)</script><p>展開來後 $P(w_1, w_2, w_3,…,w_T) = P(w_1)P(x_2|w_1)P(w_3|w_2, w_1)…P(w_T|w_1,…w_{T-1})$</p><a id="more"></a><ul><li>EX</li></ul><p><img src="https://i.imgur.com/4JCLrKg.png" alt="" style="zoom:50%;" /></p><h1 id="Ngram-Model"><a href="#Ngram-Model" class="headerlink" title="Ngram Model"></a>Ngram Model</h1><p>根據 Markov assumption， word $i$ 只跟其包含 $i$ 的連續 $n$ 個 word 有關, 即前面 $n -1$ 個 word</p><script type="math/tex; mode=display">p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1})</script><p>其中 conditional probability 統計 corpus 內的 word frequency，可以看到 ngram 只跟前面 $n-1$ 個 word 有關</p><script type="math/tex; mode=display">p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1}) = \dfrac{count(w_{i-n+1}, \cdots , w_{i-1},w_o)}{count({w_{i-n+1}, \cdots , w_{i-1}})}</script><p>如果 $k=2$ 則稱為 bigram model :</p><script type="math/tex; mode=display">p(w_i|w_1, w_2, ... w_{i-1}) \approx p(w_i|w_{i-1})</script><p>最簡單的實作直接統計 corpus 內的 word frequency 得到 conditional probability:</p><p><img src="https://i.imgur.com/C3DgaYi.png" alt="2 gram model" style="zoom:50%;" /></p><p>但通常泛化性不足，所以用 neural network 與 softmax 來泛化 n gram conditional probability  $p(w_i \: | \: w_{i-1} , \cdots , w_{i-i+1})$</p><h1 id="Neural-Network-Implementation"><a href="#Neural-Network-Implementation" class="headerlink" title="Neural Network Implementation"></a>Neural Network Implementation</h1><p>In neural network, we achieve the same objective using the softmax layer</p><p><img src="https://i.imgur.com/evkVSOl.png" style="zoom: 67%;" /></p><p>$p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top v’_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top v’_{w_i}})}$</p><ul><li>$h$ is the output vector of the penultimate network layer</li><li>$v^{‘}_{w}$ is the output embedding of word $w$</li><li>the inner product $h^\top v’_{w_t}$ computes the unnormalized log probability of word $w_t$</li><li>the denominator normalizes log probability by sum of the log-probabilities of all word in $V$</li></ul><h1 id="Implement-Ngram-model-with-Pytorch"><a href="#Implement-Ngram-model-with-Pytorch" class="headerlink" title="Implement Ngram model with Pytorch"></a>Implement Ngram model with Pytorch</h1><h3 id="Creating-Corpus-and-Training-Pairs"><a href="#Creating-Corpus-and-Training-Pairs" class="headerlink" title="Creating Corpus and Training Pairs"></a>Creating Corpus and Training Pairs</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">test_sentence = <span class="string">&quot;&quot;&quot;When forty winters shall besiege thy brow,</span></span><br><span class="line"><span class="string">And dig deep trenches in thy beauty&#x27;s field,</span></span><br><span class="line"><span class="string">Thy youth&#x27;s proud livery so gazed on now,</span></span><br><span class="line"><span class="string">Will be a totter&#x27;d weed of small worth held:</span></span><br><span class="line"><span class="string">Then being asked, where all thy beauty lies,</span></span><br><span class="line"><span class="string">Where all the treasure of thy lusty days;</span></span><br><span class="line"><span class="string">To say, within thine own deep sunken eyes,</span></span><br><span class="line"><span class="string">Were an all-eating shame, and thriftless praise.</span></span><br><span class="line"><span class="string">How much more praise deserv&#x27;d thy beauty&#x27;s use,</span></span><br><span class="line"><span class="string">If thou couldst answer &#x27;This fair child of mine</span></span><br><span class="line"><span class="string">Shall sum my count, and make my old excuse,&#x27;</span></span><br><span class="line"><span class="string">Proving his beauty by succession thine!</span></span><br><span class="line"><span class="string">This were to be new made when thou art old,</span></span><br><span class="line"><span class="string">And see thy blood warm when thou feel&#x27;st it cold.&quot;&quot;&quot;</span>.split()</span><br><span class="line"></span><br><span class="line">trigrams = [([test_sentence[i], test_sentence[i + <span class="number">1</span>]], test_sentence[i + <span class="number">2</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_sentence) - <span class="number">2</span>)]</span><br><span class="line">vocab = <span class="built_in">set</span>(test_sentence)</span><br><span class="line">word_to_idx = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br></pre></td></tr></table></figure><h3 id="Define-N-Gram-Model"><a href="#Define-N-Gram-Model" class="headerlink" title="Define N Gram Model"></a>Define N Gram Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NGramLanguageModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NGramLanguageModel, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line"></span><br><span class="line">        out = self.linear2(out)</span><br><span class="line"></span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br></pre></td></tr></table></figure><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">10</span></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">net = NGramLanguageModel(<span class="built_in">len</span>(vocab), EMBEDDING_DIM, CONTEXT_SIZE)</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> trigrams:</span><br><span class="line">        context_idxs = torch.tensor([word_to_idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> context], dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        </span><br><span class="line">        log_probs = net(context_idxs)</span><br><span class="line">        loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.data</span><br><span class="line">    print(<span class="string">&quot;epcoh &#123;&#125; loss &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, total_loss))</span><br><span class="line">    losses.append(total_loss)</span><br></pre></td></tr></table></figure><h3 id="Fetch-Embedding"><a href="#Fetch-Embedding" class="headerlink" title="Fetch Embedding"></a>Fetch Embedding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emb = net.embeddings(torch.tensor([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vocab))])).detach().numpy()</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>on word embeddings <a href="https://ruder.io/word-embeddings-1/">https://ruder.io/word-embeddings-1/</a></li><li><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html">https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2Vec 從原理到實現</title>
      <link href="word2vec-from-theory-2-implement/"/>
      <url>word2vec-from-theory-2-implement/</url>
      
        <content type="html"><![CDATA[<p>這篇是在 notion 整理的筆記大綱，只提供綱要性的說明</p><h1 id="預備知識"><a href="#預備知識" class="headerlink" title="預備知識"></a>預備知識</h1><ul><li><p>language model： NLP 語言模型</p><p>參閱 <a href="/NLP-language-model/" title="NLP Language Model">NLP Language Model</a></p></li><li><p>huffman tree</p></li></ul><h1 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h1><h2 id="兩種網路結構"><a href="#兩種網路結構" class="headerlink" title="兩種網路結構"></a>兩種網路結構</h2><p><img src="https://i.imgur.com/395NfQN.png" alt="CBOW and skipgram" style="zoom:67%;" /></p><a id="more"></a><h3 id="Continuous-bag-of-words-CBOW-amp-Softmax"><a href="#Continuous-bag-of-words-CBOW-amp-Softmax" class="headerlink" title="Continuous bag of words (CBOW) &amp; Softmax"></a>Continuous bag of words (CBOW) &amp; Softmax</h3><blockquote><p>CBOW feeds $n$ words around the target word $w_t$ at each step</p></blockquote><script type="math/tex; mode=display">P(center|context;\theta)</script><p><img src="https://i.imgur.com/JTnC7Ko.png" alt="CBOW" style="zoom: 50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h4 id="CBOW-的-Objective-Function"><a href="#CBOW-的-Objective-Function" class="headerlink" title="CBOW 的 Objective Function"></a>CBOW 的 Objective Function</h4><p>$J_\theta = \frac{1}{V}\sum\limits_{t=1}^V\ \text{log} \space p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})$</p><p>其中</p><script type="math/tex; mode=display">p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})  = \cfrac{\exp(h^\top \text{v}^{'}_{w_{t}})}{\sum_{w_i \in V}\exp(h^\top \text{v}'_{w_i})}</script><ul><li>$n$ 表 window size</li><li>$w_t$ 表 CBOW target center word </li><li>$w_i$ 表 word $i$ in vocabulary $V$</li><li>$\text{v}_{w_i}$ 表 matrix $W$ 中，代表 $w_i$ 的那個  row vector</li><li>$\text{v}’_{w_i}$ 表 matrix $W’$ 中，代表 $w_i$ 的那個  column vector</li><li>$h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\cfrac{1}{C}(\text{v}_{w_1} + \text{v}_{w_2}+ …+ \text{v}_{w_C})^T$</li></ul><p>Because there are multiple contextual words, we construct the input vector by averaging each words’s distribution representation</p><h3 id="Skipgram-amp-Softmax"><a href="#Skipgram-amp-Softmax" class="headerlink" title="Skipgram &amp; Softmax"></a>Skipgram &amp; Softmax</h3><blockquote><p>skipgram uses the centre word to predict the surrounding words instead of using the surrounding words to predict the centre word</p></blockquote><script type="math/tex; mode=display">P(context|center; \theta)</script><p><img src="https://i.imgur.com/aYoLDWN.png" alt="Skipgram" style="zoom: 50%;" /></p><h4 id="Skipgram-的-Objective-Function"><a href="#Skipgram-的-Objective-Function" class="headerlink" title="Skipgram 的 Objective Function"></a>Skipgram 的 Objective Function</h4><script type="math/tex; mode=display">J_\theta = \frac{1}{V}\sum\limits_{t=1}^V\ \sum\limits_{-n \leq j \leq n , \neq 0} \text{log} \space p(w_{t+j} \: | \: w_t)</script><p>其中</p><script type="math/tex; mode=display">p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({h^\top \text{v}'_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><ul><li>$n$ 為 window size</li><li>$w_{t+j}$ 表 skipgram target 第 j 個 context word</li><li>$w_t$ 為 skipgram input 的 center word</li><li>skip-gram 有兩個獨立的 embedding matrix $W$ and $W^{‘}$<ul><li>$W$ : $V \times  N$ , $V$ is vocabulary size; N is vector dimension</li><li>output matrix $W^{‘}$: $N \times V$, encoding the meaning of context</li></ul></li><li>$\text{v}^{‘}_{w_i}$ is  column vector of word $w_i$ in $Ｗ^{‘}$</li><li>$h$ is the hidden layer’s output</li></ul><p>事實上，skip-gram 沒有 hidden layer, 因爲 input 只有一個 word,  $h$  就是 word embedding  $\text{v}_{w_t}$of the word $w_t$ in $W$。</p><p>所以  </p><p> $p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({\text{v}^\top_{w_t} \text{v}’_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({\text{v}^\top_{w_t} \text{v}’_{w_i}})}$</p><h2 id="兩種-loss-function-優化"><a href="#兩種-loss-function-優化" class="headerlink" title="兩種 loss function 優化"></a>兩種 loss function 優化</h2><p>原始 softmax 作為 objective function， 分母必須對所有 word $w_i$ in vocabulary 與 vector $ h$ 內積，造成運算瓶頸</p><script type="math/tex; mode=display">p(w_O | w_I) = \dfrac{\text{exp}({h^\top \text{v}'_{w_{O}}})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><p>所以 word2Vec 論文中採用兩種 objective function 的優化方案，Hierarchical Softmax  與 Negatvie Sampling</p><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>原理推導請參閱  <a href="/hierarchical-softmax-in-word2vec/" title="Hierarchical Softmax 背後的數學">Hierarchical Softmax 背後的數學</a></p><blockquote><p>Hierarchical softmax build a full binary tree to avoid computation over all vocabulary</p></blockquote><p><img src="https://i.imgur.com/Kqj88Gk.png" alt=""></p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>原理推導請參閱  <a href="/negative-sampling-in-word2vec/" title="Negative Sampling 背後的數學">Negative Sampling 背後的數學</a></p><blockquote><p>negative sampling did further simplification because it focuses on learning high-quality word embedding rather than modeling the likelihood of correct word in natural language.</p></blockquote><ul><li>In implement，negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them</li></ul><h1 id="實現-WordVec"><a href="#實現-WordVec" class="headerlink" title="實現 WordVec"></a>實現 WordVec</h1><ul><li>skip gram + softmax  <a href="/Pytorch-Implement-Naive-Word2Vec-with-Softmax/" title="Pytorch 實作 Word2Vec with Softmax">Pytorch 實作 Word2Vec with Softmax</a></li><li>CBOW + softmax  <a href="/Pytorch-Implement-Naive-Word2Vec-with-Softmax/" title="Pytorch 實作 Word2Vec with Softmax">Pytorch 實作 Word2Vec with Softmax</a></li><li>CBOW + hierarchical softmax <a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Pytorch 實作 CBOW with Hierarchical Softmax">Pytorch 實作 CBOW with Hierarchical Softmax</a></li><li>CBOW + negatove sampling</li><li>skip gram + hierarchical softmax</li><li>skip gram + negative sampling <a href="/Pytorch-Implement-Skipgram-with-Negative-Sampling/" title="Pytorch 實作 Skipgram with Negative Sampling">Pytorch 實作 Skipgram with Negative Sampling</a></li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Skip gram 與 CBOW 實際上都 train 了兩個  embedding matrix $W$ and $W’$ </p><ul><li>$W:$ 在 C implement  稱作 $syn0$。</li><li>$W’$:<ul><li>若採用 hierarchical softmax 稱為  $syn1$</li><li>若採用 negative sampling 叫 $syn1neg$ </li></ul></li></ul><p>根據性質，$syn0$ 與 $syn1neg$ 是一體兩面的，本身都可以代表 word embedding，皆可使用。</p><p>而代表 $W’$ 的 $syn1$ 因爲連結的是 huffman tree 的 non leaf node，所以其本身對 word $w_i$ 沒直接意義。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p>On word embeddings - Part 1 <a href="https://ruder.io/word-embeddings-1/">https://ruder.io/word-embeddings-1/</a></p></li><li><p>On word embeddings - Part 2: Approximating the Softmax <a href="https://ruder.io/word-embeddings-softmax/">https://ruder.io/word-embeddings-softmax/</a></p></li><li><p>Learning Word Embedding <a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy</a></p></li><li><p>other</p><ul><li>Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from <a href="http://arxiv.org/abs/1411.2738">http://arxiv.org/abs/1411.2738</a></li><li>Language Models, Word2Vec, and Efficient Softmax Approximations <a href="https://rohanvarma.me/Word2Vec/">https://rohanvarma.me/Word2Vec/</a></li></ul><ul><li><p><a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a></p></li><li><p><a href="https://www.cnblogs.com/pinard/p/7160330.html">word2vec原理(一) CBOW与Skip-Gram模型基础</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a></p></li><li><p><a href="http://shomy.top/2017/07/28/word2vec-all/">Word2vec数学原理全家桶</a></p></li></ul><ul><li><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li><li>Word2Vec-知其然知其所以然 <a href="https://www.zybuluo.com/Dounm/note/591752#word2vec-%E7%9F%A5%E5%85%B6%E7%84%B6%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6">https://www.zybuluo.com/Dounm/note/591752#word2vec-知其然知其所以然</a></li></ul></li><li><p>C source code</p><ul><li>Word2Vec源码解析 <a href="https://www.cnblogs.com/neopenx/p/4571996.html">https://www.cnblogs.com/neopenx/p/4571996.html</a></li></ul></li><li><p>應用</p><ul><li>小白看Word2Vec的正确打开姿势|全部理解和应用 <a href="https://zhuanlan.zhihu.com/p/120148300">https://zhuanlan.zhihu.com/p/120148300</a></li><li>推荐系统从零单排系列(六)—Word2Vec优化策略层次Softmax与负采样 [<a href="https://zhuanlan.zhihu.com/p/66417229">https://zhuanlan.zhihu.com/p/66417229</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一步步透視 GBDT Classifier</title>
      <link href="GBDT-Classifier-step-by-step/"/>
      <url>GBDT-Classifier-step-by-step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a><strong>TL;DR</strong></h1><ul><li>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$<ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x)$</li></ul></li><li>訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}$ 的方向<ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li><li>GBDT classifier 常用的 loss function 為 cross entropy</li><li>classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$  跟 $probability$  有關，得將 $F(x)$ 通過 $\textit{sigmoid function }$ 獲得  probability<ul><li>$p = \sigma(F(x))$</li></ul></li></ul><p>GBDT 簡介在 <a href="https://seed9d.github.io/GBDT-Rregression-Tree-Step-by-Step/#GBDT-%E7%B0%A1%E4%BB%8B">一步步透視 GBDT Regression Tree</a></p><p>直接進入正題吧</p><a id="more"></a><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>GBDT classification tree algorithm 跟 regression  tree 並無不同</p><p><img src="https://i.imgur.com/bfBpmPD.png" style="zoom:50%;" /></p><h2 id="Input-Dat-and-Loss-Function"><a href="#Input-Dat-and-Loss-Function" class="headerlink" title="Input Dat and Loss Function"></a>Input Dat and Loss Function</h2><blockquote><p>Input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/xUdzKge.png" alt="Data"></p><ul><li>target $y_i$: who loves Troll2</li><li>features of $x_i$: “likes popcorn”, “Age”,  “favorite”</li></ul><p>Our goal is using $x_i$ to predict someone like Trolls 2 or not</p><p>loss function 為  cross entropy</p><script type="math/tex; mode=display">\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​</script><p><strong>值得注意的是，GBDT - classifier $F(x)$  輸出的是  $log(odds)$ 而不是 $probability$</strong></p><p>要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\textit{sigmoide function}$</p><script type="math/tex; mode=display">\textit{The probability of Loving Troll 2 } = \sigma(F(x)) = p</script><ul><li><p>$log(odds)$ 轉換成 $probability$ 公式</p><script type="math/tex; mode=display">  p = \cfrac{e^{\log(odds)}}{1 + e^{\log(odds)}}</script></li></ul><h2 id="Step-1-Initial-model-with-a-constant-value-F-0-X"><a href="#Step-1-Initial-model-with-a-constant-value-F-0-X" class="headerlink" title="Step 1 Initial model with a constant value $F_0(X)$"></a>Step 1 Initial model with a constant value $F_0(X)$</h2><p><img src="https://i.imgur.com/vZnfhjM.png" alt="初始 data samples" style="zoom:67%;" /></p><p>初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可</p><p><img src="https://i.imgur.com/0g9VDEd.png" alt=""></p><p>計算完，得到 $F_0(x) = 0.69$，每個  data point 的初始 prediction 都一樣就是 $F_0(x)$。</p><p>$F_0(x)$ 是 $\log(odds)$ 若要計算 probability of loving Troll 2 呢？</p><p><img src="https://i.imgur.com/L6ilbXq.png" alt=""></p><p>$\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。</p><p><img src="https://i.imgur.com/TvWxTvg.png" alt=""></p><ul><li>ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$</li><li>ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving  Troll 2 </li></ul><h2 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  <strong>with new tree $f_m(x)$</strong></li></ol><hr><h3 id="At-Epoch-m-1"><a href="#At-Epoch-m-1" class="headerlink" title="At Epoch m = 1"></a>At Epoch m = 1</h3><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>classification 問題中  residual 為 predicted probability  與 observed label $y$ 之間的差距</p><p>$residual = observed - \textit{predicted probability}$</p><p><img src="https://i.imgur.com/wr9RAF2.png" alt="residual" style="zoom:67%;" /></p><ul><li>true label 為 1</li><li>false label 為 0</li></ul><p><strong>注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$  輸出的是 $log(odds)$</strong></p><p>計算各 data sample 的 residual  後：</p><p><img src="https://i.imgur.com/Xrwhgxy.png" style="zoom:67%;" /></p><ul><li>ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$</li><li>ep_0_prob 表 $F_0(x)$ predicted probability，$\sigma(F_0(x))$</li><li>ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/MywnkEq.png" style="zoom:67%;" /></p><p>建樹為一般 fit regression tree  的過程，criterion 為 mean square error，假設找到的樹結構為</p><p><img src="https://i.imgur.com/FRJtKz0.png" alt=""></p><p>可以看到綠色為 leaf node，所有的 data  sample $x$ 都被歸到特定 leaf node 下</p><p><img src="https://i.imgur.com/obH8T1T.png" alt="" style="zoom:67%;" /></p><ul><li>ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下</p><script type="math/tex; mode=display">\cfrac{\sum residual_i}{\sum [\textit{previous probability} \times \textit{(1 - previous probability)}]}</script><ul><li>分子是 each leaf node 下的 data sample $x$ 的 residual 和</li><li>分母的 previous probability 為 $m -1$  步 GBDT 輸出的 probability $p = \sigma(F(x))$ 。<br>在這個 epoch 是指 $F_0(x)$</li></ul><p>經過計算後，每個 leaf node 輸出</p><p><img src="https://i.imgur.com/j7I1oVk.png" alt=""></p><p><img src="https://i.imgur.com/Sasd4Ei.png" style="zoom:67%;" /></p><ul><li>ep_0_prob  表 $\sigma(F_0(x))$ 計算出的 probability of loving Troll2</li><li>ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob</li><li>ep_1_leaf_output 表 data sample $x$ 在  tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值</li></ul><h4 id="D-update-F-1-x-with-new-tree-f-1-x"><a href="#D-update-F-1-x-with-new-tree-f-1-x" class="headerlink" title="(D) update $F_1(x)$  with new tree $f_1(x)$"></a>(D) update $F_1(x)$  <strong>with new tree $f_1(x)$</strong></h4><p>現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 </p><script type="math/tex; mode=display">F_1(x) = F_0(x) + \textit{learning rate} \times  f_1(x)</script><p>輸出的 probability 為 $\sigma(F_1(x))$</p><p>令 $\textit{learnign rate = 0.8}$，得到 epoch 2 每個  data sample 的 $\log(odds)$  prediction 與 probability prediction</p><p><img src="https://i.imgur.com/xt6rxMA.png" style="zoom:67%;" /></p><ul><li>ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$</li><li>ep_1_prob 為 $F_1(x)$ 輸出的 probability $\sigma(F_1(x))$</li></ul><hr><h3 id="At-Epoch-m-2"><a href="#At-Epoch-m-2" class="headerlink" title="At Epoch m = 2"></a>At Epoch m = 2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_1(x)$"></a>(A) Calculate Residuals of $F_1(x)$</h4><p>計算上一步 $\textit{residual of } F_1(X)$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p><img src="https://i.imgur.com/mIWXSGC.png" style="zoom:67%;" /></p><ul><li>ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\sigma(F_1(x))$</li><li>ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>用 data sample x 的 columns “like_popcor”, “age”, “favorite_color”  擬合 ep_2_residual   build a new tree $f_2(x)$</p><p><img src="https://i.imgur.com/levLwV4.png" style="zoom:67%;" /></p><p> 假設得到 $f_2(x)$ 的樹結構：</p><p><img src="https://i.imgur.com/XMBr1KE.png" alt=""></p><p> 每個 data sample 對應的 leaf index<img src="https://i.imgur.com/qM6crwo.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_index 表 data sample  對應到 $f_2(x)$  上的 leaf node index</li></ul><h4 id="D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$"></a>(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$</h4><p>計算 $f_2(x)$ 下每個 leaf node 的輸出:</p><p><img src="https://i.imgur.com/MOaIis1.png" alt=""></p><p>對應到 data sample 上:</p><p><img src="https://i.imgur.com/yKumKHj.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node  下會有一樣的值</li></ul><h3 id="Update-F-2-x-with-New-Tree-f-2-x"><a href="#Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="Update $F_2(x)$  with New Tree $f_2(x)$"></a>Update $F_2(x)$  with New Tree $f_2(x)$</h3><p>到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為</p><p>$F_2(x) = F_0(x) + \nu(f_1(x) + f_2(x))$</p><ul><li>$\nu$ 為 learning rate，假設為 0.8</li></ul><p>GBDT 輸出的 probability 為 $\sigma(F_2(x))$，計算 epoch 2 的 prediction of  probability of loving troll2:</p><p><img src="https://i.imgur.com/6Lxi8mZ.png" style="zoom:67%;" /></p><ul><li>love_toll2: our target</li><li>ep_0_pre 表 $F_0(x)$</li><li>ep_1_leaf_output 表 data sample x​  在第一顆樹 $f_1(x)$ 的輸出值</li><li>ep_2_leaf_output 表 data sample x 在第二顆樹  $f_2(x)$  的輸出值</li><li>ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$</li><li>ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\sigma(F_2(x))$</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p>Output GBDT fitted model $F_M(x)$</p></blockquote><p>把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/MHQLmVE.png" style="zoom:67%;" /></p><p>$F_M(x)$ 的每棵樹 $f_m(x)$  都是去 fit  $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p>所以 $F_m(x)$ 又可以寫成</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><p>這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住：</p><ul><li>$F_m(x)$ 輸出 $log(odds)$</li><li>$residual$  的計算與 probability 有關</li></ul><h1 id="GBDT-Classifier-背後的數學"><a href="#GBDT-Classifier-背後的數學" class="headerlink" title="GBDT Classifier 背後的數學"></a>GBDT Classifier 背後的數學</h1><h3 id="Q-為什麼用-cross-entropy-做為-loss-function"><a href="#Q-為什麼用-cross-entropy-做為-loss-function" class="headerlink" title="Q: 為什麼用 cross entropy 做為 loss function ?"></a>Q: 為什麼用 cross entropy 做為 loss function ?</h3><p>在分類問題上，我們預測的是 $\textit{The probability of loving Troll 2}$  $P(Y|x)$，$\textit{}$ 以 $maximize$ $\textit{log likelihood}$ 來解 $P(Y|x)$。</p><p>令 GBDT - classification tree 的  probability prediction 為 $P(Y| x) = \sigma(F(x))$，則 objective function 為 </p><script type="math/tex; mode=display">\text{log (likelihood of the obersved data given the prediction) }  \\= \sum_{i=1}^N [y_i \log(p) + (1-y_i)\log(1-p)]</script><ul><li>$p = P(Y=1|x)$，表 the probability of loving movie Troll 2</li><li>$y_i$ : observation  of data sample $x_i$ loving Troll 2 or not<ul><li>$y \in \{1, 0\}$</li></ul></li></ul><p>而 $\text{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成</p><script type="math/tex; mode=display">\text{objective function} = - \sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)</script><p>所以 $\text{loss function} = -[y \log(p) + (1-y)\log(1-p)]$</p><p>把 loss function 用 $odds$ 表示：</p><script type="math/tex; mode=display">\begin{aligned} -[y \log(p) + (1-y)\log(1-p)] & = -y\log(p)-(1-y)\log(1-p) \\ &= -y\log(p)-\log(1-p) + y\log(1-p) \\ &= -y[\log(p) - \log(1-p)] - \log(1-p)  \\ & = -y\log(odds) - \log(1-p) \\ &= -y\log(odds) + \log(1 + \exp(\log(odds))) \end{aligned}</script><ul><li>第三個等號 到 第四個等號用到 $odds=\cfrac{p}{1-p}$</li><li>第四個等號 到 第五個等號用到 $p = \cfrac{\exp(\log(odds))}{1 + \exp(\log(odds))}$ 這個結論<ul><li>$\log(1-p) = \log(1- \cfrac{\exp(\log(odds))}{1 + \exp(\log(odds))}) = \log(\cfrac{1}{1 + \exp(\log(odds))}) = -\log(1 + \exp(\log(odds)))$</li></ul></li></ul><p>把 loss  function 表示成 odds 的好處是， $ -y\log(odds) + \log(1 + \exp(\log(odds)))$ 對 $log(odds)$  微分形式很簡潔</p><script type="math/tex; mode=display">\cfrac{d}{d \ log(odds)} -ylog(odds) + log(1 + \exp^{log(odds)}) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}} = -y + p</script><p>loss function 對 $log(odds)$ 的微分，既可以以 $\log(odds)$ 表示，也可以以 probability $p$ 表示</p><ul><li>以 $log(odds)$ 表示：  $\cfrac{d}{d \log(odds)}L(y_i, p) = -y -\cfrac{\exp(\log(odds))}{1 + \exp(\log(odds))}$</li><li>以 $p$ 表示：$\cfrac{d}{d \log(odds)}L(y_i, p) = -y + p$</li></ul><p>用 $p$ 表示時，loss function 對 $log(odds)$ 的微分</p><script type="math/tex; mode=display">-y + p = \text{ -(observed  - predicted) = negative residual}</script><h3 id="Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false"><a href="#Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false" class="headerlink" title="Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?"></a>Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?</h3><blockquote><p>來自 Step 1 的問題</p></blockquote><p>根據選定的  loss function </p><script type="math/tex; mode=display">\text{loss function} = -[y \log(p) + (1-y)\log(1-p)]</script><ul><li>$P(Y=1|x) = p$ 為出現正類的 probability</li><li>$y \in \{1, 0\}$</li></ul><p>將 loss  function 以 $\log(odds)$ 表示</p><script type="math/tex; mode=display">-[y \log(p) + (1-y)\log(1-p)] = -[y\log(odds) + \log(1 + \exp(log(odds)))]</script><p>$F_0(x)$ 為能使 $\textit{cost function}$ 最小的 $\log(odds): \gamma$</p><script type="math/tex; mode=display">F_0(x) = argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma) = argmin_\gamma \sum^n_{i=1} -[y_i\log(odds) + \log(1 + \exp(\log(odds)))]</script><ul><li>$n$ 為 number of data sample $x$</li></ul><p>令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$</p><p>cost  function 對 $log(odds)$  微分取極值：</p><script type="math/tex; mode=display">\begin{aligned}& \cfrac{d}{d \log(odds)}\sum^n_{i=1} -[y_i\log(odds) + \log(1 + \exp(log(odds)))]   \\ & = \cfrac{d}{d \log(odds)}\sum^{n^{(1)}}_i -(\log(odds) + \log(1 + exp(log(odds)))) - \sum^{n^{(0)}}_j (0 * \log(odds) + \log(1 + \exp(\log(odds))))  \\& = \cfrac{d}{d\log(odds)} -n^{(1)} \times (\log(odds) + \log(1 + \exp(\log(odds)))) - n^{(0)} \times \log(1 + \exp(\log(odds))) \\ & =0\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} & n^{(1)} \times(-1 + \cfrac{e^{\log(odds)}}{1 + e^{\log(odds)}})   + n^{(0)} \times(\cfrac{e^{\log(odds)}}{1 + e^{\log(odds)}}) \\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\ & = - n^{(1)} + n p \\ &= 0 \end{aligned}</script><p>移項得到  $p$</p><script type="math/tex; mode=display">p = \cfrac{n^{(1)}}{n}</script><script type="math/tex; mode=display">\log(odds) = \cfrac{p}{1-p} = \cfrac{n^{(1)}}{n^{(0)}}</script><p>故得證，給定 $\text{loss function }  = -[y \log(p) + (1-y)\log(1-p)]$， 能使 $argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma)$  的 $\gamma$ 為 </p><script type="math/tex; mode=display">log(odds)= \cfrac{n^{(1)}}{n^{(0)}}</script><script type="math/tex; mode=display">\therefore F_0(x) = \cfrac{n^{(1)}}{n^{(0)}}</script><h3 id="Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h3><blockquote><p>問題來自 Step 2 - (A)</p></blockquote><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum -[y_i \log(p_i) + (1-y_i)\log(1-p_i)]</script><ul><li>$y$ 為 target label</li><li>$p = P(Y=1|x)$ 表正類的 probability</li></ul><p>注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy </p><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x) \hAar   F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \  L(y, F_{m-1}(x)) \\ &= -(-(y - p)) \\ &= -(-(\text{observed} - \text{predict probability})) \\ &= - \text{negative residual} \\ & = \text{residual} \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual</strong></p><h3 id="Q-leaf-node-的輸出公式怎麼來的？"><a href="#Q-leaf-node-的輸出公式怎麼來的？" class="headerlink" title="Q: leaf node 的輸出公式怎麼來的？"></a>Q: leaf node 的輸出公式怎麼來的？</h3><blockquote><p>問題來自 Step 2-(C)</p></blockquote><p>在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\gamma_{jm}$，使的 cost function 最小</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &= argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma) \\ &= argmin_\gamma \sum_{x_i \in R_{j, m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + e^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$j$ 表 leaf node index</li><li>$m$ 表第 $m$ 步</li><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合</li></ul><p>將 loss function 以 $\log(odds)$ 表示後的 objective function</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + e^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$-[y \log(p) + (1-y)\log(1-p)] = -[y\log(odds) + \log(1 + e^{\log(odds)})]$<ul><li>$F_{m-1}(x)$ 輸出為 $\log(odds)$</li></ul></li></ul><p>cost function  對 $\gamma$  微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近  loss function 處理</p><script type="math/tex; mode=display">f(x) \approx f(a) + f'(a)(x-a) + \cfrac{1}{2}f''(a)(x-a)^2</script><p>讓  2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \gamma)$ 在 $F_{m-1}(x)$ 處展開</p><script type="math/tex; mode=display">L(y_i, F_{m-1}(x_i) + \gamma) \approx L(y_i, F_{m-1}(x_i) ) + \cfrac{d}{d (F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))\gamma   + \cfrac{1}{2} \cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\gamma^2</script><p>將 cost function 對 $\gamma$ 微分取極值，求  $\gamma_{j,m}$</p><script type="math/tex; mode=display">\sum_{x_i \in R_{jm}} \cfrac{d}{d\gamma}  L(y_i, F_{m-1}(x_i), \gamma) \approx \sum_{x_i \in R_{jm}} (\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\gamma) = 0</script><p>移項得到 $\gamma$</p><script type="math/tex; mode=display">\gamma = \cfrac{\sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))} = \cfrac{ \sum_{x_i \in R_{jm} } g_i}{ \sum_{x_i \in R_{jm} } h_i}</script><p>分子是  derivative of Loss function ;   分母是  second derivative of loss function</p><ul><li><p>分子部分:</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\& = \sum \cfrac{d}{d(F_{m-1}(x_i))}  \ y_i \times [F_{m-1}(x_i)  ] - \log(1 + \exp(F_{m-1}(x_i)  )) \\ &=  \sum (y_i - \cfrac{\exp(F_{m-1}(x_i) )}{1 + \exp(F_{m-1}(x_i) )} ）\\& = \sum_{x_i \in R_{jm}} (y_i -p_i) \end{aligned}</script><ul><li>$F_{m-1}(x_i)$  是 $m-1$  步時 $classifier$  輸出的 $\log(odds)$</li></ul><ul><li><strong>分子部分為 $\text{summation of residual}$</strong></li></ul></li><li><p>分母部分</p></li></ul><script type="math/tex; mode=display">\begin{aligned}& \sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\ & = \sum_{x_i \in R_{jm} } \cfrac{d^2}{d \, \ (F_{m-1}(x_i))^2} \, -[y_i  \times F_{m-1}(x_i) - \log(1 + \exp(F_{m-1}(x_i)))] \\ &= \sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)}-[y_i  - \cfrac{\exp(F_{m-1}(x_i) )}{1 + \exp(F_{m-1}(x_i) )}] \\ & =\sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \exp(F_{m-1}(x_i)))^{-1} \times \exp(F_{m-1}(x_i))]  \\ & = \sum_{x_i \in R_{jm} }-[(1 + e^{F_{m-1}(x_i)})^{-2} \exp(F_{m-1}(x_i))\times \exp(F_{m-1}(x_i)) - (1+ \exp(F_{m-1}(x_i)))^{-1}  \times \exp(F_{m-1}(x_i))  ]\\&=  \sum_{x_i \in R_{jm} } \cfrac{-\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} \quad + \quad \cfrac{\exp(F_{m-1}(x_i))}{1+ \exp(F_{m-1}(x_i))} = \sum_{x_i \in R_{jm} }\cfrac{-\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2}   \ + \ \cfrac{-\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} \times \cfrac{(1 + \exp(F_{m-1}(x_i)))}{1 + \exp(F_{m-1}(x_i))}\\&= \sum_{x_i \in R_{jm} } \cfrac{-\exp(2 * F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} + \cfrac{\exp(F_{m-1}(x_i)) + \exp(2 * F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} = \sum_{x_i \in R_{jm} }\cfrac{\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} \\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))} \times \cfrac{1}{(1 + \exp(F_{m-1}(x_i)))}\\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp(\log(odds)_i)}{1 + \exp(\log(odds)_i)} \times \cfrac{1}{1 + \exp(\log(odds)_i)}\\ &= \sum_{x_i \in R_{jm} } p_i \times (1-p_i)\end{aligned}</script><p>綜合分子分母，能使 $F_m(x)$  cost function 最小化的  tree  $f_m(x)$   第 $j$ 個  leaf node 輸出為</p><script type="math/tex; mode=display">\gamma_{jm}= \cfrac{\sum_{x_i \in R_{jm})} (y_i - p_i)}{\sum_{x_i \in R_{jm} }(p_i \times (1- p_i))} = \cfrac{\text{summation of  residuals }}{\text{summantion of (previous probability $\times$ (1 - previoous probability))}}</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><h2 id="Data-Sample"><a href="#Data-Sample" class="headerlink" title="Data Sample"></a>Data Sample</h2><blockquote><p>learning by doing it</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">12</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">87</span>, <span class="string">&#x27;gree&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">44</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">19</span>, <span class="string">&#x27;red&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">32</span>, <span class="string">&#x27;green&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">14</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;like_popcorn&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;love_troll2&#x27;</span>]</span><br><span class="line">target = <span class="string">&#x27;love_troll2&#x27;</span></span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p><a href="https://www.youtube.com/watch?v=jxuNLH5dXCs&amp;">Gradient Boost Part 3 (of 4): Classification</a></p></li><li><p><a href="https://www.youtube.com/watch?v=StWY5QWMXCw&amp;t">Gradient Boost Part 4 (of 4): Classification Details</a></p></li></ul><ul><li>Gradient Boosting In Classification: Not a Black Box Anymore! <a href="https://blog.paperspace.com/gradient-boosting-for-classification/">https://blog.paperspace.com/gradient-boosting-for-classification/</a><ul><li>statquest 整理</li></ul></li><li>StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! <a href="https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s">https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s</a></li><li>The Logit and Sigmoid Functions <a href="https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/">https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/</a></li><li>Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends <a href="https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979">https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一步步透視 GBDT Regression Tree</title>
      <link href="GBDT-Rregression-Tree-Step-by-Step/"/>
      <url>GBDT-Rregression-Tree-Step-by-Step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><ul><li><p>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$</p><ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x) $</li></ul></li><li><p>訓練中的 GBDT，每棵新樹  $f_m(x)$ 都去擬合  target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient  decent}$  的方向 </p><ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li></ul><p><img src="https://i.loli.net/2021/01/23/ZWgsyMOw5LoQxFz.png" alt="Golf Boosting"></p><a id="more"></a><p>結論說完了，想看數學原理請移駕 <a href="#math">背後的數學</a>，想看例子從無到有生出 GBDT 的請到 <a href="#alg">Algorithm - step by step</a> ，想離開得請按上一頁。</p><h1 id="GBDT-簡介"><a href="#GBDT-簡介" class="headerlink" title="GBDT 簡介"></a>GBDT 簡介</h1><p>GBDT-regression tree 簡單來說，訓練時依序建立 trees $\{ f_1(x), f_2(x), …. , f_M(x)\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。</p><p>所以 GBDT - regression  tree 的訓練是 sequentially ，無法以並行訓練加速。</p><p>我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下:</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu\sum^m_{i=1}f_i(x) = F_{m-1}(x)  + \nu f_m(x)</script><ul><li>$\nu$ 為 learning rate</li></ul><p>GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。</p><p>差多遠即 residual 的概念：</p><script type="math/tex; mode=display">\textit{residual = observed - predicted}</script><p>因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual</p><p><a name="alg"><a> </p><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>Algorithm 參考了 statQuest 對 GBDT 的講解，連結放在  reference，必看！</p><p>GBDT-regression tree 擬合 algorithm：</p><h2 id="Input-Data-and-Loss-Function"><a href="#Input-Data-and-Loss-Function" class="headerlink" title="Input Data and Loss Function"></a><img src="https://i.loli.net/2021/01/23/6BuRsPyvaM7GULc.png" alt="Algorithm of GBDT">Input Data and Loss Function</h2><blockquote><p>input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/yYJ9IpX.png" alt=""></p><p>接下來都用這組簡單的數據，其中</p><ul><li>Target $y_i$ : weight which is what we want to predict</li><li>$x_i$: features 的組成有 身高，喜歡的顏色，性別</li></ul><p>目標是用 $x_i$ 的 height, favorite color, gender  來預測  $y_i$ 的 weight</p><p>loss function 為 square error </p><script type="math/tex; mode=display">L(y_i, F(x)) = \cfrac{1}{2}(\textit{observed - predicted}) ^2 = \cfrac{1}{2}(y_i^2 - F(x))^2</script><ul><li><p>square error commonly use in Regression with Gradient Boost</p></li><li><p>$\textit{observed - predicted}$  is called  $residual$</p></li><li><p>$y_i$ are observed value</p></li><li><p>$F(x)$: the function which give us the predicted value</p><ul><li><p>也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><script type="math/tex; mode=display">F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)</script></li></ul></li></ul><p><a name="step1"></a></p><h2 id="Step-1-Initialize-Model-with-a-Constant-Value"><a href="#Step-1-Initialize-Model-with-a-Constant-Value" class="headerlink" title="Step 1 Initialize Model with a Constant Value"></a>Step 1 Initialize Model with a Constant Value</h2><p>初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可</p><p><img src="https://i.imgur.com/uamGKg1.png" alt=""></p><p>取  weight 平均得到 $F_0(x) = 71.2 = \textit{average weight}$</p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  with new tree $f_m(x)$</li></ol><hr><h3 id="At-epoch-m-1"><a href="#At-epoch-m-1" class="headerlink" title="At epoch m=1"></a>At epoch m=1</h3><p><a name="step2A"></a></p><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals </p><p>​    <script type="math/tex">residuals = \textit{observed weight - predicted weight}</script></p><p>而 $F_0(x) = \textit{average weight = 71.17}$</p><p>計算 residual 後: </p><p><img src="https://i.imgur.com/FmhobDH.png" alt=""></p><ul><li>epoch_0_prediction 表 $F_0(x)$ 輸出</li><li>epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的  residual </p><p>用 columns $\textit{height, favorite, color, gender}$  預測 $residuals$ 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/Vg5cKjw.png" alt=""></p><p>建樹的過程為一般的 regression tree building 過程，target 就是 residuals。</p><p>假設我們找到分支結構是</p><p><img src="https://i.imgur.com/qmuUdME.png" alt=""></p><p>綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$  特定的 leaf node 下。</p><p><img src="https://i.imgur.com/C0P4ukP.png" alt=""></p><ul><li>epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index</li></ul><p><a name="step2C"></a></p><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值</p><p>直覺地對每個 leaf node 內的 data sample $x$  weight 值取平均，得到輸出值</p><p><img src="https://i.imgur.com/wruHARq.png" alt=""></p><p><img src="https://i.imgur.com/ausJcrf.png" alt="Untitled 9"></p><ul><li>epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值</li></ul><p><a name="step2D"></a></p><h4 id="D-Update-F-1-x-with-New-Tree-f-1-x"><a href="#D-Update-F-1-x-with-New-Tree-f-1-x" class="headerlink" title="(D) Update $F_1(x)$  with New Tree $f_1(x)$"></a>(D) Update $F_1(x)$  with New Tree $f_1(x)$</h4><p>此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$</p><p>現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \textit{learning rate } \times f_1(x)$</p><p>假設 $\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下</p><p><img src="https://i.imgur.com/1fNc1dF.png" alt=""></p><ul><li>epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值</li></ul><hr><h3 id="At-epoch-m-2"><a href="#At-epoch-m-2" class="headerlink" title="At epoch m=2"></a>At epoch m=2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_{1}(x)$"></a>(A) Calculate Residuals of $F_{1}(x)$</h4><p>m = 2 新的 residual between observed weight and $F_1(x)$如下</p><p><img src="https://i.imgur.com/NMS5YJ0.png" alt=""></p><ul><li>epoch_1_prediction 為 $F_1(x)$  的輸出</li><li>epoch_2_residual 為 observed weight  與 predicted weight $F_1(x)$ 的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>建一顆新樹擬合 epoch 2 (A)  得出的 residual</p><p><img src="https://i.imgur.com/bcUyFaW.png" alt=""></p><ul><li>epoch_2_residual 為 $f_2(x)$ 要擬合的 target</li></ul><p>假設 $f_2(x)$ 擬合後樹結構長這樣</p><p><img src="https://i.imgur.com/pJJZRC6.png" alt=""></p><p><img src="https://i.imgur.com/eNDlBn4.png" alt=""></p><ul><li>epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$</h4><p>決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個  leaf node 下的 data sample $x$ 取平均</p><p><img src="https://i.imgur.com/SM21atv.png" alt=""></p><ul><li>epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是  residual epoch_2_residual</li></ul><h4 id="D-Update-F-2-x-with-New-Tree-f-2-x"><a href="#D-Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="(D) Update $F_2(x)$  with New Tree $f_2(x)$"></a>(D) Update $F_2(x)$  with New Tree $f_2(x)$</h4><p>到目前為止我們建立了兩顆 $tree$  $f_1(x), f_2(x)$，假設  $\textit{learning rate = 0.1}$，則 $F_2(x)$ 為</p><script type="math/tex; mode=display">F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))</script><p>每個  data sample  在 $F_2(x)$ 的 predict 值如下圖：</p><p><img src="https://i.imgur.com/wbbpfNc.png" alt=""> </p><ul><li>weight: out target value</li><li>epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值</li><li>epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual</li><li>epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual</li><li>epoch_2_prediction:  $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p> Output GBDT fitted model $F_M(x)$</p></blockquote><p>Step 3 輸出模型 $F_M(x)$，把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/ZPUv6Cr.png" alt=""></p><p>$F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$  與上一步 predicted value $\hat{y}$ $F_{m-1}(x)$ 的  $residual$</p><p><a name="math"></a></p><h1 id="GBDT-Regression-背後的數學"><a href="#GBDT-Regression-背後的數學" class="headerlink" title="GBDT Regression 背後的數學"></a>GBDT Regression 背後的數學</h1><h2 id="Q-Loss-function-為什麼用-mean-square-error"><a href="#Q-Loss-function-為什麼用-mean-square-error" class="headerlink" title="Q:  Loss function 為什麼用 mean square error ?"></a>Q:  Loss function 為什麼用 mean square error ?</h2><p>​    </p><p>選擇 $\cfrac{1}{2}(\textit{observed - predicted}) ^2$  當作 loss function  的好處是對 $F(X)$ 微分的形式簡潔</p><script type="math/tex; mode=display">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script><p>其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\textit{negative  residual}$</p><p>​    $-(y_i - F(X)) = \textit{-(observed - predicted) = negative residual }  $</p><p>而我們知道 $F(X)$ 在  loss function $L(y_i, F(X))$ 的 gradient 就是 $\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$</p><h2 id="Q：為什麼初始-F-0-X-直接對-targets-取平均？"><a href="#Q：為什麼初始-F-0-X-直接對-targets-取平均？" class="headerlink" title="Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？"></a>Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？</h2><p><a href="#step1">問題來自 step 1</a></p><p>我們要找的是能使 cost function $\sum^n_{i=1}L(y_i, \gamma)$ 最小的那個輸出值 $\gamma$ 做為  $F_0(X)$。</p><p>$F_0(x) = argmin_r \sum^n_{i=1} L(y_i,\gamma)$</p><ul><li><p>$F_0(x)$ 初始化的  function，其值是常數</p></li><li><p>$\gamma$  refers to the predicted values</p></li><li><p>$n$ 是 data sample 數</p></li></ul><p><em>Proof:</em></p><p>已知</p><p>​    <script type="math/tex">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script></p><p>所以 cost function 對 $\gamma $ 微分後，是所有 sample data 跟 $\gamma$ 的 negative residual 和 為 0</p><p>​    <script type="math/tex">\sum^n_{i=1}L(y_i, \gamma) = -(y_1 - \gamma) - (y_2 - \gamma)-  ... -(y_n - \gamma) = 0</script></p><p>移項得到 $\gamma$</p><p>​    <script type="math/tex">\gamma = \cfrac{y_1 + y_2 + .... + y_n}{n}</script></p><p>正是所有 target value 的 mean，故得證</p><p>​    <script type="math/tex">F_0(x) = \gamma = \textit{the average of targets value}</script> </p><h2 id="Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h2><p><a href="#step2A">問題來自 step 2A</a></p><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum \cfrac{1}{2}(y_i^2 - F(x))^2</script><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)\hAar F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} \ L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \ L(y, F_{m-1}(x)) \\ &= -(-(y - F_{m-1}(x))) \\ &= -(-(observed - predicted )) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent  的方向和值， 也就是 residual</strong> </p><p>​    $f_m(x)$  = $\textit{gradient decent}$   = $\textit{negative gradient}$  = $residual$</p><p><strong>GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\textit{gradient  decent}$  的方向得到新的 $F(x)$</strong></p><p><strong>這也正是為什麼叫做 gradient boost</strong> 。</p><p>by the way，step 2-(A) compute residuals:</p><script type="math/tex; mode=display">r_{im} = -[\cfrac{\partial L(y_i, F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \ \textit{for i = 1,....,n}</script><ul><li>$i$: sample number</li><li>$m$: the tree we are trying to build</li></ul><h2 id="Q：個別-leaf-node-的輸出為什麼是取平均-？"><a href="#Q：個別-leaf-node-的輸出為什麼是取平均-？" class="headerlink" title="Q：個別 leaf node 的輸出為什麼是取平均 ？"></a>Q：個別 leaf node 的輸出為什麼是取平均 ？</h2><p><a href="#step2C">問題來自 step 2C </a></p><p>在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\gamma$ 使 cost function  最小</p><p>$\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma)$</p><ul><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合</li><li>$F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$  上的輸出值為 $\gamma_{j,m}$</li></ul><script type="math/tex; mode=display">\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2</script><p>直接對  $\gamma$ 微分</p><script type="math/tex; mode=display">\begin{aligned}\cfrac{d}{d \gamma} \ \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2 =  \ \sum_{x_i \in R_{j,m}}-(y_i - F_{m-1}(x_i) - \gamma) = 0\end{aligned}</script><p>移項</p><script type="math/tex; mode=display">\gamma = \cfrac{1}{n_{jm}} \sum_{x_i \in R_{j,m}}y_i - F_{m-1}(x_i)</script><ul><li>$n_{jm}$ is the number of data sample in leaf node $j$ at step $m$</li></ul><p>白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均</p><p>事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $  都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。</p><h2 id="Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？"><a href="#Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？" class="headerlink" title="Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？"></a>Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？</h2><p><a href="#step2D">問題來自 step 2D </a></p><script type="math/tex; mode=display">\begin{aligned}F_m(x) & = F_{m-1}(x) + \nu f_m(x) \\ &= F_{m-1}(x) + \nu \sum^{J_m}_{j=1}\gamma_{jm}I(x \in R_{jm})\end{aligned}</script><ul><li>$F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)$</li><li>$\nu$ learning rate</li><li>$J_m$ m 步 的 leaf node 總數</li><li>$\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出</li></ul><p>$F_m(x)$ 展開來就是</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu(f_1(x) + f_2(x)+ ...+f_m(t))</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><blockquote><p>seeing is believing</p></blockquote><p>太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙</p><p>人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶</p><blockquote><p>learning by doing it</p></blockquote><p>所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">88</span>),</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Greem&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">76</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">56</span>),</span><br><span class="line">    (<span class="number">1.8</span>, <span class="string">&#x27;Red&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">73</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Green&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">77</span>),</span><br><span class="line">    (<span class="number">1.4</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">57</span>)   </span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>]</span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><blockquote><p>always get your hands dirty</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h2 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h2><ul><li><p><a href="https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=75s">Gradient Boost Part 1 (of 4): Regression Main Ideas</a></p></li><li><p><a href="https://www.youtube.com/watch?v=2xudPOBz-vs">Gradient Boost Part 2 (of 4): Regression Details</a></p></li></ul><p>ccd comment: 上面兩個必看</p><ul><li>A Step by Step Gradient Boosting Decision Tree Example <a href="https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/">https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/</a><ul><li>真 step by step</li></ul></li><li>Gradient Boosting Decision Tree Algorithm Explained <a href="https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4">https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4</a><ul><li>including sklearn 實作</li></ul></li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li>Gradient Boosting Decision Tree <a href="http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95">http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法</a></li><li>提升树算法理论详解 <a href="https://hexinlin.top/2020/03/01/GBDT/">https://hexinlin.top/2020/03/01/GBDT/</a></li><li>梯度提升树(GBDT)原理小结 <a href="https://www.cnblogs.com/pinard/p/6140514.html">https://www.cnblogs.com/pinard/p/6140514.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="ckky7073i000fsxe298cp6ib4/"/>
      <url>ckky7073i000fsxe298cp6ib4/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
