<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>一步步透視 GBDT Regression Tree</title>
      <link href="GBDT-Rregression-Tree-Step-by-Step/"/>
      <url>GBDT-Rregression-Tree-Step-by-Step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><ul><li><p>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$</p><ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x) $</li></ul></li><li><p>訓練中的 GBDT，每棵新樹  $f_m(x)$ 都去擬合  target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient  decent}$  的方向 </p><ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li></ul><p><img src="https://i.loli.net/2021/01/23/ZWgsyMOw5LoQxFz.png" alt="Golf Boosting"></p><a id="more"></a><p>結論說完了，想看數學原理請移駕 <a href="#math">背後的數學</a>，想看例子從無到有生出 GBDT 的請到 <a href="#alg">Algorithm - step by step</a> ，想離開得請按上一頁。</p><h1 id="GBDT-簡介"><a href="#GBDT-簡介" class="headerlink" title="GBDT 簡介"></a>GBDT 簡介</h1><p>GBDT-regression tree 簡單來說，訓練時依序建立 trees $\{ f_1(x), f_2(x), …. , f_M(x)\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。</p><p>所以 GBDT - regression  tree 的訓練是 sequentially ，無法以並行訓練加速。</p><p>我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下:</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu\sum^m_{i=1}f_i(x) = F_{m-1}(x)  + \nu f_m(x)</script><ul><li>$\nu$ 為 learning rate</li></ul><p>GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。</p><p>差多遠即 residual 的概念：</p><script type="math/tex; mode=display">\textit{residual = observed - predicted}</script><p>因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual</p><p><a name="alg"><a> </p><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>Algorithm 參考了 statQuest 對 GBDT 的講解，連結放在  reference，必看！</p><p>GBDT-regression tree 擬合 algorithm：</p><h2 id="Input-Data-and-Loss-Function"><a href="#Input-Data-and-Loss-Function" class="headerlink" title="Input Data and Loss Function"></a><img src="https://i.loli.net/2021/01/23/6BuRsPyvaM7GULc.png" alt="Algorithm of GBDT">Input Data and Loss Function</h2><blockquote><p>input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/yYJ9IpX.png" alt=""></p><p>接下來都用這組簡單的數據，其中</p><ul><li>Target $y_i$ : weight which is what we want to predict</li><li>$x_i$: features 的組成有 身高，喜歡的顏色，性別</li></ul><p>目標是用 $x_i$ 的 height, favorite color, gender  來預測  $y_i$ 的 weight</p><p>loss function 為 square error </p><script type="math/tex; mode=display">L(y_i, F(x)) = \cfrac{1}{2}(\textit{observed - predicted}) ^2 = \cfrac{1}{2}(y_i^2 - F(x))^2</script><ul><li><p>square error commonly use in Regression with Gradient Boost</p></li><li><p>$\textit{observed - predicted}$  is called  $residual$</p></li><li><p>$y_i$ are observed value</p></li><li><p>$F(x)$: the function which give us the predicted value</p><ul><li><p>也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><script type="math/tex; mode=display">F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)</script></li></ul></li></ul><p><a name="step1"></a></p><h2 id="Step-1-Initialize-Model-with-a-Constant-Value"><a href="#Step-1-Initialize-Model-with-a-Constant-Value" class="headerlink" title="Step 1 Initialize Model with a Constant Value"></a>Step 1 Initialize Model with a Constant Value</h2><p>初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可</p><p><img src="https://i.imgur.com/uamGKg1.png" alt=""></p><p>取  weight 平均得到 $F_0(x) = 71.2 = \textit{average weight}$</p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  with new tree $f_m(x)$</li></ol><hr><h3 id="At-epoch-m-1"><a href="#At-epoch-m-1" class="headerlink" title="At epoch m=1"></a>At epoch m=1</h3><p><a name="step2A"></a></p><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals </p><p>​    <script type="math/tex">residuals = \textit{observed weight - predicted weight}</script></p><p>而 $F_0(x) = \textit{average weight = 71.17}$</p><p>計算 residual 後: </p><p><img src="https://i.imgur.com/FmhobDH.png" alt=""></p><ul><li>epoch_0_prediction 表 $F_0(x)$ 輸出</li><li>epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的  residual </p><p>用 columns $\textit{height, favorite, color, gender}$  預測 $residuals$ 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/Vg5cKjw.png" alt=""></p><p>建樹的過程為一般的 regression tree building 過程，target 就是 residuals。</p><p>假設我們找到分支結構是</p><p><img src="https://i.imgur.com/qmuUdME.png" alt=""></p><p>綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$  特定的 leaf node 下。</p><p><img src="https://i.imgur.com/C0P4ukP.png" alt=""></p><ul><li>epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index</li></ul><p><a name="step2C"></a></p><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值</p><p>直覺地對每個 leaf node 內的 data sample $x$  weight 值取平均，得到輸出值</p><p><img src="https://i.imgur.com/wruHARq.png" alt=""></p><p><img src="https://i.imgur.com/ausJcrf.png" alt="Untitled 9"></p><ul><li>epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值</li></ul><p><a name="step2D"></a></p><h4 id="D-Update-F-1-x-with-New-Tree-f-1-x"><a href="#D-Update-F-1-x-with-New-Tree-f-1-x" class="headerlink" title="(D) Update $F_1(x)$  with New Tree $f_1(x)$"></a>(D) Update $F_1(x)$  with New Tree $f_1(x)$</h4><p>此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$</p><p>現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \textit{learning rate } \times f_1(x)$</p><p>假設 $\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下</p><p><img src="https://i.imgur.com/1fNc1dF.png" alt=""></p><ul><li>epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值</li></ul><hr><h3 id="At-epoch-m-2"><a href="#At-epoch-m-2" class="headerlink" title="At epoch m=2"></a>At epoch m=2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_{1}(x)$"></a>(A) Calculate Residuals of $F_{1}(x)$</h4><p>m = 2 新的 residual between observed weight and $F_1(x)$如下</p><p><img src="https://i.imgur.com/NMS5YJ0.png" alt=""></p><ul><li>epoch_1_prediction 為 $F_1(x)$  的輸出</li><li>epoch_2_residual 為 observed weight  與 predicted weight $F_1(x)$ 的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>建一顆新樹擬合 epoch 2 (A)  得出的 residual</p><p><img src="https://i.imgur.com/bcUyFaW.png" alt=""></p><ul><li>epoch_2_residual 為 $f_2(x)$ 要擬合的 target</li></ul><p>假設 $f_2(x)$ 擬合後樹結構長這樣</p><p><img src="https://i.imgur.com/pJJZRC6.png" alt=""></p><p><img src="https://i.imgur.com/eNDlBn4.png" alt=""></p><ul><li>epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$</h4><p>決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個  leaf node 下的 data sample $x$ 取平均</p><p><img src="https://i.imgur.com/SM21atv.png" alt=""></p><ul><li>epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是  residual epoch_2_residual</li></ul><h4 id="D-Update-F-2-x-with-New-Tree-f-2-x"><a href="#D-Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="(D) Update $F_2(x)$  with New Tree $f_2(x)$"></a>(D) Update $F_2(x)$  with New Tree $f_2(x)$</h4><p>到目前為止我們建立了兩顆 $tree$  $f_1(x), f_2(x)$，假設  $\textit{learning rate = 0.1}$，則 $F_2(x)$ 為</p><script type="math/tex; mode=display">F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))</script><p>每個  data sample  在 $F_2(x)$ 的 predict 值如下圖：</p><p><img src="https://i.imgur.com/wbbpfNc.png" alt=""> </p><ul><li>weight: out target value</li><li>epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值</li><li>epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual</li><li>epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual</li><li>epoch_2_prediction:  $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p> Output GBDT fitted model $F_M(x)$</p></blockquote><p>Step 3 輸出模型 $F_M(x)$，把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/ZPUv6Cr.png" alt=""></p><p>$F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$  與上一步 predicted value $\hat{y}$ $F_{m-1}(x)$ 的  $residual$</p><p><a name="math"></a></p><h1 id="GBDT-Regression-背後的數學"><a href="#GBDT-Regression-背後的數學" class="headerlink" title="GBDT Regression 背後的數學"></a>GBDT Regression 背後的數學</h1><h2 id="Q-Loss-function-為什麼用-mean-square-error"><a href="#Q-Loss-function-為什麼用-mean-square-error" class="headerlink" title="Q:  Loss function 為什麼用 mean square error ?"></a>Q:  Loss function 為什麼用 mean square error ?</h2><p>​    </p><p>選擇 $\cfrac{1}{2}(\textit{observed - predicted}) ^2$  當作 loss function  的好處是對 $F(X)$ 微分的形式簡潔</p><script type="math/tex; mode=display">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script><p>其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\textit{negative  residual}$</p><p>​    $-(y_i - F(X)) = \textit{-(observed - predicted) = negative residual }  $</p><p>而我們知道 $F(X)$ 在  loss function $L(y_i, F(X))$ 的 gradient 就是 $\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$</p><h2 id="Q：為什麼初始-F-0-X-直接對-targets-取平均？"><a href="#Q：為什麼初始-F-0-X-直接對-targets-取平均？" class="headerlink" title="Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？"></a>Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？</h2><p><a href="#step1">問題來自 step 1</a></p><p>我們要找的是能使 cost function $\sum^n_{i=1}L(y_i, \gamma)$ 最小的那個輸出值 $\gamma$ 做為  $F_0(X)$。</p><p>$F_0(x) = argmin_r \sum^n_{i=1} L(y_i,\gamma)$</p><ul><li><p>$F_0(x)$ 初始化的  function，其值是常數</p></li><li><p>$\gamma$  refers to the predicted values</p></li><li><p>$n$ 是 data sample 數</p></li></ul><p><em>Proof:</em></p><p>已知</p><p>​    <script type="math/tex">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script></p><p>所以 cost function 對 $\gamma $ 微分後，是所有 sample data 跟 $\gamma$ 的 negative residual 和 為 0</p><p>​    <script type="math/tex">\sum^n_{i=1}L(y_i, \gamma) = -(y_1 - \gamma) - (y_2 - \gamma)-  ... -(y_n - \gamma) = 0</script></p><p>移項得到 $\gamma$</p><p>​    <script type="math/tex">\gamma = \cfrac{y_1 + y_2 + .... + y_n}{n}</script></p><p>正是所有 target value 的 mean，故得證</p><p>​    <script type="math/tex">F_0(x) = \gamma = \textit{the average of targets value}</script> </p><h2 id="Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h2><p><a href="#step2A">問題來自 step 2A</a></p><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum \cfrac{1}{2}(y_i^2 - F(x))^2</script><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)\hAar F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} \ L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \ L(y, F_{m-1}(x)) \\ &= -(-(y - F_{m-1}(x))) \\ &= -(-(observed - predicted )) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent  的方向和值， 也就是 residual</strong> </p><p>​    $f_m(x)$  = $\textit{gradient decent}$   = $\textit{negative gradient}$  = $residual$</p><p><strong>GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\textit{gradient  decent}$  的方向得到新的 $F(x)$</strong></p><p><strong>這也正是為什麼叫做 gradient boost</strong> 。</p><p>by the way，step 2-(A) compute residuals:</p><script type="math/tex; mode=display">r_{im} = -[\cfrac{\partial L(y_i, F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \ \textit{for i = 1,....,n}</script><ul><li>$i$: sample number</li><li>$m$: the tree we are trying to build</li></ul><h2 id="Q：個別-leaf-node-的輸出為什麼是取平均-？"><a href="#Q：個別-leaf-node-的輸出為什麼是取平均-？" class="headerlink" title="Q：個別 leaf node 的輸出為什麼是取平均 ？"></a>Q：個別 leaf node 的輸出為什麼是取平均 ？</h2><p><a href="#step2C">問題來自 step 2C </a></p><p>在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\gamma$ 使 cost function  最小</p><p>$\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma)$</p><ul><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合</li><li>$F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$  上的輸出值為 $\gamma_{j,m}$</li></ul><script type="math/tex; mode=display">\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2</script><p>直接對  $\gamma$ 微分</p><script type="math/tex; mode=display">\begin{aligned}\cfrac{d}{d \gamma} \ \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2 =  \ \sum_{x_i \in R_{j,m}}-(y_i - F_{m-1}(x_i) - \gamma) = 0\end{aligned}</script><p>移項</p><script type="math/tex; mode=display">\gamma = \cfrac{1}{n_{jm}} \sum_{x_i \in R_{j,m}}y_i - F_{m-1}(x_i)</script><ul><li>$n_{jm}$ is the number of data sample in leaf node $j$ at step $m$</li></ul><p>白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均</p><p>事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $  都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。</p><h2 id="Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？"><a href="#Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？" class="headerlink" title="Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？"></a>Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？</h2><p><a href="#step2D">問題來自 step 2D </a></p><script type="math/tex; mode=display">\begin{aligned}F_m(x) & = F_{m-1}(x) + \nu f_m(x) \\ &= F_{m-1}(x) + \nu \sum^{J_m}_{j=1}\gamma_{jm}I(x \in R_{jm})\end{aligned}</script><ul><li>$F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)$</li><li>$\nu$ learning rate</li><li>$J_m$ m 步 的 leaf node 總數</li><li>$\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出</li></ul><p>$F_m(x)$ 展開來就是</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu(f_1(x) + f_2(x)+ ...+f_m(t))</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><blockquote><p>seeing is believing</p></blockquote><p>太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙</p><p>人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶</p><blockquote><p>learning by doing it</p></blockquote><p>所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">88</span>),</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Greem&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">76</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">56</span>),</span><br><span class="line">    (<span class="number">1.8</span>, <span class="string">&#x27;Red&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">73</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Green&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">77</span>),</span><br><span class="line">    (<span class="number">1.4</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">57</span>)   </span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>]</span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><blockquote><p>always get your hands dirty</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h2 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h2><p><a href="https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=75s">Gradient Boost Part 1 (of 4): Regression Main Ideas</a></p><p><a href="https://www.youtube.com/watch?v=2xudPOBz-vs">Gradient Boost Part 2 (of 4): Regression Details</a></p><p>ccd comment: 上面兩個必看</p><ul><li>A Step by Step Gradient Boosting Decision Tree Example <a href="https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/">https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/</a><ul><li>真 step by step</li></ul></li><li>Gradient Boosting Decision Tree Algorithm Explained <a href="https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4">https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4</a><ul><li>including sklearn 實作</li></ul></li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li>Gradient Boosting Decision Tree <a href="http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95">http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法</a></li><li>提升树算法理论详解 <a href="https://hexinlin.top/2020/03/01/GBDT/">https://hexinlin.top/2020/03/01/GBDT/</a></li><li>梯度提升树(GBDT)原理小结 <a href="https://www.cnblogs.com/pinard/p/6140514.html">https://www.cnblogs.com/pinard/p/6140514.html</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="ckk9uqyt00003c2e2396x36pq/"/>
      <url>ckk9uqyt00003c2e2396x36pq/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
