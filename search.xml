<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>透視 XGBoost(4) 神奇 optimization 在哪裡？</title>
      <link href="XGBoost-cool-optimization/"/>
      <url>XGBoost-cool-optimization/</url>
      
        <content type="html"><![CDATA[<p>XGBoost 不只在算法上的改造，真正讓 XGBoost  大放異彩的是與工程優化的結合，這讓 XGBoost 在工業上可以應用於大規模數據的處理。</p><ul><li>Approximate Greedy Algorithm</li><li>Weighted Quantile Sketch &amp; Parallel Learning</li><li>Sparsity-Aware Split Finding</li><li>System Design</li></ul><h1 id="Approximate-Greedy-Algorithm"><a href="#Approximate-Greedy-Algorithm" class="headerlink" title="Approximate Greedy Algorithm"></a>Approximate Greedy Algorithm</h1><blockquote><p>XGBoost 有兩種 split finding  策略，Exact Greedy and Approximate Greedy</p></blockquote><p>data sample 量小的情況下， XGB tree 在建立分支時，會逐一掃過每個可能分裂點 (已對特徵值排序) 計算 similarity score  跟 gain， 即是 greedy  algorithm。</p><p><img src="https://i.imgur.com/UKj8mpu.png" alt=""></p><p>greedy algorithm 會窮舉所有可能分裂點，因此能達到最準確的結果，因為他把所有可能的結果都看過一遍後選了一個最佳的。</p><p>但如果 data sample  不僅數量多 (row)，特徵維度也多 (columns)，採用 greedy algorithm，雖然準確卻也沒效率。</p><a id="more"></a><p>因此在數據量大的情況下，XGBoost 只會選出多個切分點，稱為 candidate splits ，這樣就能大幅降低計算量，此即 Approximate Greedy Algorithm，candidate splits 越多也意味更加耗時。</p><p><img src="https://i.imgur.com/YHuqQZp.png" style="zoom: 33%;" /></p><h2 id="怎麼找-Candidate-Splits"><a href="#怎麼找-Candidate-Splits" class="headerlink" title="怎麼找 Candidate Splits ?"></a>怎麼找 Candidate Splits ?</h2><blockquote><p>XGBoost proposes candidate splitting points according to percentiles of feature distribution</p></blockquote><p>XGBoost  原則上是利用 data sample 的 feature 分佈的 quantiles 尋找 candidate splits，為什麼說是原則上呢，因為跟一般的 quantiles 有點不同，這留到 Weighted Quantile Sketch 章節再細說。</p><h3 id="phi-Quantiles-計算"><a href="#phi-Quantiles-計算" class="headerlink" title="$\phi$ - Quantiles 計算"></a>$\phi$ - Quantiles 計算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data:  [<span class="number">39.</span> <span class="number">21.</span> <span class="number">24.</span> <span class="number">61.</span> <span class="number">81.</span> <span class="number">11.</span> <span class="number">89.</span> <span class="number">56.</span> <span class="number">12.</span> <span class="number">51.</span>]</span><br><span class="line">sort:  [<span class="number">11.</span> <span class="number">12.</span> <span class="number">21.</span> <span class="number">24.</span> <span class="number">39.</span> <span class="number">51.</span> <span class="number">56.</span> <span class="number">61.</span> <span class="number">81.</span> <span class="number">89.</span>]</span><br><span class="line">rank:  [ <span class="number">1.</span>  <span class="number">2.</span>  <span class="number">3.</span>  <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>  <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span> <span class="number">10.</span>]</span><br></pre></td></tr></table></figure><p>以上總共 $N=10$ 筆 data  則</p><ul><li>$\phi = 0.1$ ⇒  0.1  quantile   ⇒ $0.1*10 = 1$ ⇒  rank = 1 ⇒ 11，故 0.1 quantile = 11</li><li>$\phi = 0.5$ ⇒  0.5  quantile   ⇒ $0.5 * 10 = 5$ ⇒  rank = 5 ⇒ 39， 故 0.5 quantile = 39</li></ul><p>可以發現 $\phi N$ 就是在找 rank 與其對應的 data  值，下面是一個  <code>quantiles = [0.1, 0.3, 0.5, 0.6, 0.9]</code> 的例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">quantiles  [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.9</span>]</span><br><span class="line">rank query [<span class="number">1.0</span>, <span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>, <span class="number">9.0</span>]</span><br><span class="line">data query  [<span class="number">11</span>, <span class="number">21</span>, <span class="number">39</span>, <span class="number">51</span>, <span class="number">81</span>]</span><br></pre></td></tr></table></figure><p>但 $\phi$ - Quantiles 在數據量大時難以對所有 data sample 排序，更遑論找確切的 candidate splits。</p><p>所以 XGBoost 採用了 $\epsilon \text{-approximate} \ \phi  \text{-quantiles}$ 的思想來得到近似的 candidate splite ，容許 $\phi N$ 找出的 rank 有一定誤差 $\varepsilon N$。 </p><h3 id="epsilon-text-approximate-phi-text-quantiles"><a href="#epsilon-text-approximate-phi-text-quantiles" class="headerlink" title="$\epsilon \text{-approximate} \ \phi  \text{-quantiles}$"></a>$\epsilon \text{-approximate} \ \phi  \text{-quantiles}$</h3><p>$\epsilon \text{-approximate}$ 容許找出的 rank 落在一定範圍內： </p><script type="math/tex; mode=display">⁍</script><ul><li>$\varepsilon=0.1$ , 0.1 quantile ⇒ {11, 12}</li><li>$\varepsilon =0.1$ , 0.2 quantile ⇒ {11, 12, 21}</li><li>$\varepsilon =0.1$  ,  0.3 quantile ⇒ {12, 21, 24}</li></ul><p>換句話說，在區間內的值都可以作為 candidate splits，當 $\varepsilon =0.1$    <code>quantiles = [0.1, 0.3, 0.5, 0.6, 0.9]</code> 下，可以接受的 rank 與對應的 data 值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">quantile: <span class="number">0.1</span></span><br><span class="line"> query rank: [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"> query data: [<span class="number">11</span>, <span class="number">12</span>]</span><br><span class="line">quantile: <span class="number">0.3</span></span><br><span class="line"> query rank: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"> query data: [<span class="number">12</span>, <span class="number">21</span>, <span class="number">24</span>]</span><br><span class="line">quantile: <span class="number">0.5</span></span><br><span class="line"> query rank: [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"> query data: [<span class="number">24</span>, <span class="number">39</span>, <span class="number">51</span>]</span><br><span class="line">quantile: <span class="number">0.6</span></span><br><span class="line"> query rank: [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"> query data: [<span class="number">39</span>, <span class="number">51</span>, <span class="number">56</span>]</span><br><span class="line">quantile: <span class="number">0.9</span></span><br><span class="line"> query rank: [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line"> query data: [<span class="number">61</span>, <span class="number">81</span>, <span class="number">89</span>]</span><br></pre></td></tr></table></figure><h3 id="ε-Approximate-Quantile-Summary"><a href="#ε-Approximate-Quantile-Summary" class="headerlink" title="ε-Approximate Quantile Summary"></a>ε-Approximate Quantile Summary</h3><blockquote><p>An ε-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of εN</p></blockquote><p>因爲在區間內的值都可以作為 candidate splits，可以看出 quantiles 之間 query 出來的 rank 會有 overlapping，於是我們可以只選取特定幾個 quantiles，每個 quantile 只需要保存 $[\lfloor (\phi -\epsilon)\times N \rfloor , \lfloor (\phi + \epsilon) \times N\rfloor ]$ 中的代表的 rank 值 (最小值 and 最大值) 即可。</p><p>例如 $\varepsilon = 0.1$ ，<code>quantiles = [0.1, 0.3, 0.5, 0.6, 0.9]</code> ，分別選定 rank 區間內的最大值做為 summary:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">quantiles:     [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.6</span>, <span class="number">0.9</span>]</span><br><span class="line">rank summary:  [<span class="number">2</span>,   <span class="number">4</span>,   <span class="number">6</span>,    <span class="number">7</span>,  <span class="number">10</span>]</span><br><span class="line">query data:    [<span class="number">12</span>, <span class="number">24</span>,  <span class="number">51</span>,   <span class="number">56</span>,  <span class="number">89</span>]</span><br></pre></td></tr></table></figure><p>在使用時也很簡單：</p><ul><li>找 0.1 quantile ⇒ rank =2 ⇒ 12 ⇒ 0.1 quantile = 12</li><li>找 0.8 quantile ⇒ rank = $0.8 * 10 = 8$ ⇒ 與 8 最接近的 rank 為 7 ⇒ 56 ⇒ 0.8 quantile = 56</li></ul><p>因此在 data samples 很大的情況下，一台運算機器無法將所有 data samples 存放進 memory 時，可以透過近似的方式找出 quantile 代表的值做為 candidate split。</p><h1 id=""><a href="#" class="headerlink" title=" "></a> </h1><h1 id="Weighted-Quantile-Sketch-amp-Parallel-Learning"><a href="#Weighted-Quantile-Sketch-amp-Parallel-Learning" class="headerlink" title="Weighted Quantile Sketch &amp; Parallel Learning"></a>Weighted Quantile Sketch &amp; Parallel Learning</h1><p>上面提到 XGBoost 在數據量大的時候，會採用 Approximate Greedy Algorithm 選出多個 candidate splits ，加快運算效率。</p><p>問題是分佈運算 (Parallel Learning) 時要怎麼 approximate quantiles？</p><h3 id="Parallel-Learning-下的-quantiles"><a href="#Parallel-Learning-下的-quantiles" class="headerlink" title="Parallel Learning 下的  quantiles"></a>Parallel Learning 下的  quantiles</h3><p>簡單來說， data samples 會被拆成很多份在不同的運算單元計算某個特徵值的 gain and similarity score，但計算過程需要全局 data samples 的特徵 distribution 才有辦法算出 approximate quantiles ，因此各個運算單元會彙整 (merge) 出一張 approximate histogram，然後在這張 histogram 上計算 $\varepsilon$-approximate quantiles </p><p><img src="https://i.imgur.com/AbduJM0.png" style="zoom:50%;" /></p><h2 id="什麼是-Weighted-Quantile-Sketch"><a href="#什麼是-Weighted-Quantile-Sketch" class="headerlink" title="什麼是 Weighted Quantile Sketch?"></a>什麼是 Weighted Quantile Sketch?</h2><p>每筆 data sample 都帶有 weight，XGBoost 在 計算 approximate quantiles 時，盡量確保每個 quantiles 之間的 $\text{sum of data weight}$ 是差不多的的，而一般的 quantiles 是確保 quantiles 之間的 data 數量相同，此即 weighted quantile </p><p>下面這張 weighted quantile sketch 示意圖，展示每個 quantile 的 sum of data weight 皆等於 10，</p><p><img src="https://i.imgur.com/AcNFVLz.png" style="zoom: 33%;" /></p><p>示意圖，參考就好</p><h2 id="為什麼需要-Weighted-Quantile-Sketch？"><a href="#為什麼需要-Weighted-Quantile-Sketch？" class="headerlink" title="為什麼需要 Weighted Quantile Sketch？"></a>為什麼需要 Weighted Quantile Sketch？</h2><blockquote><p>Put observation with low confidence into quantiles with fewer observation</p></blockquote><p>以分類問題當例子，假設我們有六筆 data samples 如下</p><p><img src="https://i.imgur.com/TXF7jQr.png" style="zoom:33%;" /></p><p>如果按照數量決定 quantiles，紅框內的兩個 data sample  不管怎樣都會被分在一起分不開。</p><p>再看 classification leaf node 的 output value 公式</p><script type="math/tex; mode=display">\text{Output Value} = \cfrac{(\sum \text{residual}_i)}{\sum[(\text{previous probability}_i) (1 - \text{previous probability}_i)] + \lambda}</script><p>兩者的 residual  一正一負，剛好會互相 cancel，這對於 predicted probability 的收斂有負面影響。</p><p>如果依照 weight 來切分，盡可能使每個 quantile 的 sum of weight 相等， 就有機會將兩者分開到不同 leaf node 下</p><p><img src="https://i.imgur.com/nrDFXhT.png" style="zoom:33%;" /></p><p>data sample $x_i$ 的 weight 應該要反映出 $F(x_i)$ 的預測值的 confidence，在分類問題中，predicted probability  在 0.5 附近時，代表 classifier  其實不確定 data sample $x_i$ 屬於 positive or negative， 所以 XGBoost 利用 weight quantile 加大 $x_i$的 weight 讓 XGB tree  $f$ 在分裂時更可以考慮到 $x_i$。</p><p>Weight Quantile 的實際作用為讓 low confidence 的 data sample 有更高的 weight，以便在計算 candidate split 時， low confidence 的 data sample 能跟 hight confidence 的 data sample 分開來 split</p><h2 id="那每筆-data-的-weight-怎麼來的？"><a href="#那每筆-data-的-weight-怎麼來的？" class="headerlink" title="那每筆 data 的 weight 怎麼來的？"></a>那每筆 data 的 weight 怎麼來的？</h2><p>每筆 data 的 weight 其實就是通用 objective function 裡的 <strong>second derivative</strong> of the loss function $h_i$</p><p>參見 <a href="/XGBoost-General-Objective-Function/" title="透視 XGBoost(3) 蘋果樹下的 objective function">透視 XGBoost(3) 蘋果樹下的 objective function</a></p><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m \end{aligned}</script><ul><li>$g_i$ 為 first derivative of loss function  related to data sample $x_i$ :  $g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, F_{m-1}(x_i))$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$:  $h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$</li><li>$T_m$ is the number of leaf in tree $f_m$</li><li>$\tau$ 表對 $T_m$ 的 factor</li><li>$\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$  第 $j$ 個 leaf node 的輸出值</li></ul><p><strong>Proof:</strong></p><p>從 objective function 開始推導</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}& = [\sum_i^n( g_if_m(x_i) + \cfrac{1}{2}h_if_m(x_i)^2) ] + \Omega(f_m)  \\& = \sum^n_{i=1}\cfrac{1}{2} h_i(f_m(x_i) + \cfrac{g_i}{h_i})^2 + \Omega(f_m) - \cfrac{1}{2}\sum^n_{x}\cfrac{g_i^2}{h_i} \\& = \sum^n_{i=1}\cfrac{1}{2} h_i(f_m(x_i) + \cfrac{g_i}{h_i})^2 + \Omega(f_m) + constant\end{aligned}</script><ul><li>regularization term  $\Omega({f_m} ) =\tau T_m + \cfrac{1}{2}\lambda \sum^T_{j} \gamma_{m,j}^2$</li><li>$g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, F_{m-1}(x_i))$，已知項</li><li>$h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$ ，已知項</li></ul><p>仔細觀察，each data sample is weighted by $h_i$ </p><script type="math/tex; mode=display">\cfrac{1}{2} h_i(f_m(x_i) -(-\cfrac{g_i}{h_i}))^2 \hAar \text{weight}(x - b)^2</script><ul><li>因為是求 weight ，係數項只是 scalar，所有 data sample 都乘一樣的值，相當於不用乘</li><li>論文中這一段堆導應該是筆誤了，少了一個負號</li></ul><p>故得證 each data sample $x_i$’s weight is actually its second derivative of the loss function $h_i$</p><script type="math/tex; mode=display">\text{data sample weight } = h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, F_{m-1}(x_i))</script><h3 id="Regression-下-data-sample-weight"><a href="#Regression-下-data-sample-weight" class="headerlink" title="Regression 下 data sample weight"></a>Regression 下 data sample weight</h3><blockquote><p>已知 data sample $x_i$’s weight is actually its second derivative of the loss function，來求 regression 的 data sample weight 。</p></blockquote><p>Regression 的 loss function 通常是 square error ， $l(y_i, \hat{y_i}) = \cfrac{1}{2}(y_i - \hat{y_i})^2$  </p><p>我們已知 square error 的 $h_i = \cfrac{d}{d \ \hat{y}_i} -(y_i - \hat{y_i}) = 1$</p><p>所以事實上，loss function 為 square error 的 regression 所有 data sample weight 都是 1</p><h3 id="Classification-下-data-sample-weight"><a href="#Classification-下-data-sample-weight" class="headerlink" title="Classification 下 data sample weight"></a>Classification 下 data sample weight</h3><p>classification 在 cross entropy/logit loss 作為 loss function 時，second derivative of the loss function $h_i$:</p><script type="math/tex; mode=display">h_i = \cfrac{d^2}{d (F_{m-1}(x_i))^2}l(y_i, F_{m-1}(x_i) = p_i \times (1-p_i)</script><ul><li>previous probability $p_i$ 表 data sample $x_i$ 在  $F_{m-1}(x_i)$ 輸出的  probability</li></ul><p>classification 的 sample weight 是個開口向下的拋物線，兩側越往 $p_i=0.5$ 靠近 sample weight 越大。這很好理解，當 classifier 預測出的 probability 為 0.5 時，對這筆 data sample 是 low confidence 的，所以得加大其 weight 凸顯他。</p><p><img src="https://i.imgur.com/JBZxzNm.png" style="zoom:33%;" /></p><h1 id="Sparsity-Aware-Split-Finding"><a href="#Sparsity-Aware-Split-Finding" class="headerlink" title="Sparsity-Aware Split Finding"></a>Sparsity-Aware Split Finding</h1><p>XGBoost  有兩種 missing values (NAs) 處理機制，一種是最簡單的，直接指定 leaf node 方向，EX: missing value 直接放到  leaf node 。</p><p>另一種是 learning from data，訓練時對於特徵中的 missing values (NAs)，在分裂計算 gain 時處理。</p><p>XGBoost  會分別考慮將 missing values  放到 left leaf node and right leaf node 的情形下計算 $\text{Gain}_{Left}, \text{Gain}_{Right}$</p><p>選擇能讓 gain 最大的那一側做為 missing values 所在 leaf node ，在 inference 階段遇到此特徵的 missing value 時將之歸入</p><p>EX：</p><p>假設我們有數筆  complete data 和兩筆含有 missing  values 的 data</p><p><img src="https://i.imgur.com/3w2Lstm.png" style="zoom:50%;" /></p><p><img src="https://i.imgur.com/0OVDOLf.png" style="zoom:50%;" /></p><p> 分別計算 $Gain_{Left}$ and $Gain_{Right}$ 後選擇 $\max(\text{Gain}_{Left}, \text{Gain}_{Right})$  最大的那邊作為 missing values (NAs) 的歸屬 leaf node</p><p><img src="https://i.imgur.com/KdIWcV6.png" style="zoom: 67%;" /></p><h1 id="System-Design"><a href="#System-Design" class="headerlink" title="System Design"></a>System Design</h1><p>XGBoost 一系列 System 上的 optimization 都是圍繞著 Memory Block 展開的</p><h2 id="Column-Block-for-Parallel-Learning"><a href="#Column-Block-for-Parallel-Learning" class="headerlink" title="Column Block for Parallel Learning"></a>Column Block for Parallel Learning</h2><p>block 是個連續的 memory 區塊，XGBoost 將 data sample 以 CSC (Compressed Sparse Column) format 存進 block 中，這麼做的好處是</p><ol><li>XGBoost  的 input 常常為大規模的 sparse data samples，使用 CSC 可以以 column 為 index 僅存放非 0 元素，有效減少 data sample size in memory 。</li><li>CSC  因為是 columns index ，所以在 training tree  時，能快速定位到確切的 column</li><li>在建 XGB tree  $f_m(x)$  時須依據不同 column (X feature column) 的 feature value sorting data，才能 split data ，有了 block 之後，只需一開始對全局所有 columns 的 feature value sorting 一次，即可在之後建任何 XGB tree  $f_m(x)$ 時使用</li></ol><p><img src="https://i.imgur.com/DsVlh0v.png" alt=""></p><p>在 exact greedy algorithm，全局只有一個 block 計算 split;  在 approximate algorithms 中，因爲 data samples size 過大導致一運算單元容不下，所以 data samples 會根據 rows 切分成多個 blocks 分散到多個運算單元。</p><p>也因為根據 row 切分成不同 blocks，所以在 multi blocks 之上運算的 Weighted Quantile Sketch 也能 parallel for split finding</p><h3 id="Time-Complexity-of-Split-Finding"><a href="#Time-Complexity-of-Split-Finding" class="headerlink" title="Time Complexity of Split Finding"></a>Time Complexity of Split Finding</h3><p>若不事先對 block 內的 columns sorting，則整體 time complexity 為</p><script type="math/tex; mode=display">O(Kd \| \text{x}\|_0 \log n)</script><ul><li>$K$ it total tree in XGBoost</li><li>$d$  is the maximum depth of the tree</li><li>$| \text{x}|_0$  number of non-missing entries</li><li>$| \text{x}|_0 \log n$ related to sorting algorithm ，每一棵樹的每一層都要 sorting  一次的意思</li></ul><p>若事先對 block  columns sorting 則  Exact greedy algorithm time complexity 為</p><script type="math/tex; mode=display">O(Kd \|\text{x}\|_0 + \|\text{x}\|_0 \log n)</script><ul><li>$O(| \text{x}|_0 \log n)$ 為事先 sorting</li><li>$O(Kd |\text{x}|_0 )$ 為建 K 個 tree</li></ul><p>Approximate algorithm 的 time complexity 為</p><script type="math/tex; mode=display">O(Kd \|\text{x}\|_0  + \|\text{x}\|_0 \log B)</script><ul><li>單建一顆 tree 的 complexity 為 $| \text{x} |_0 \log q$，$q$ is the number of proposal split candidates in dataset</li><li>$B$ is the maximum number of rows in each block。因爲 parallel computing 所以 $O$ 只記入最耗時的那個 block</li></ul><p>綜合以上，事先對 block 內的 columns sorting 加上 approximate algorithm 的 proposal candidate split，可以大幅降低 time complexity。</p><h2 id="Cache-Aware-Access"><a href="#Cache-Aware-Access" class="headerlink" title="Cache-Aware Access"></a>Cache-Aware Access</h2><p>使用 memory block structure 對 columns sorting，對於 split finding 能有效降低 time complexity，但同時會造成 CPU cache $g_i$ $h_i$ 時的 cache miss rate 上升。</p><ul><li>$g_i$ first derivative of loss function</li><li>$h_i$ second derivative of loss function</li></ul><p>$g_i \ h_i$的計算跟當前 $x_i$ 的 loss 有關，所以當 split finding  由小到大取時，$g_i \ h_i$ 在 memory 中實際上是無序的存放的，得透過 pointer 對應之，CPU 要 cache 時，不容易 copy 需要的 $g_i \ h_i$ 到 cache，造成 cache missing </p><p><img src="https://i.imgur.com/ZeMRcEw.png" alt=""></p><p>在 exact greedy algorithm，使用 cache-aware prefetching  algorithm，使用另一個</p><p>thread 提前 access (prefetch) $g_i \ h_i$ 到  memory buffer 中讓 CPU cache 住。當 training thread  要提取時，直接就可以從 cache 拿，就算沒 cache 住，在  memory 中也是連續空間。</p><p>在 multi block 下的 approximate greedy algorithm 得對 each block size  合理設置，也就是 each block 下的 row number (data sample 數) 有關。</p><p>a block size 過大 cache 會裝不下，造成 cache missing ;  a block size 過小會使 total block number 過多，使的 parallel 運算沒效率，是個 trade-off 的參數。</p><p>經過實驗，block  size  $2^{16}$ 是個相對合適的設置。</p><p><img src="https://i.imgur.com/00pPuhe.png" style="zoom:67%;" /></p><h2 id="Blocks-for-out-of-core-Computation"><a href="#Blocks-for-out-of-core-Computation" class="headerlink" title="Blocks for out-of-core Computation"></a>Blocks for out-of-core Computation</h2><p>當 data samples 大到所有的計算單元的 memory 也無法容納時，XGBoost 會使用 out-of-core。我們知道 XGBoost  以 block 為單位切分 data sample，無法被 memory 裝下的 blocks 會被成塊且連續的存入 hard disk。</p><p>計算時也會開一條 thread 去 hard disk pre-fetch blocks into memory，這樣 XGBoost 就不會消耗過多時間再 I/O 上，可以一邊訓練一邊讀取 block。</p><h3 id="Block-Compression"><a href="#Block-Compression" class="headerlink" title="Block Compression"></a>Block Compression</h3><p>存放在 hard disks 上的 out-of-core blocks 會被壓縮，減少讀取 I/O 的耗時。</p><p>壓縮方式大致上為保留 column index，對 row index 進行壓縮，只保留  beginning index of row in block，然後 利用一個 16bit 的 integer 儲存 row index 的 offset，類似 C 裡面的  pointer 跟 array 的關係。</p><h3 id="Block-Sharding"><a href="#Block-Sharding" class="headerlink" title="Block Sharding"></a>Block Sharding</h3><p>當有多個 hard disks 可供存放時，會將 data 劃分到多個 hard disks 上，好處是 pre-fetch thread 就可以同時讀取多個 hard disks 提高 I/O throughput 。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a></li><li>Why XGBoost Is So Effective? <a href="https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289">https://towardsdatascience.com/why-xgboost-is-so-effective-3a193951e289</a></li><li><p>Weighted Quantile Sketch</p><ul><li>ε-approximate quantiles <a href="http://www.mathcs.emory.edu/~cheung/Courses/584/Syllabus/08-Quantile/Greenwald.html">http://www.mathcs.emory.edu/~cheung/Courses/584/Syllabus/08-Quantile/Greenwald.html</a></li><li>xgboost之分位点算法 <a href="https://datavalley.github.io/2017/09/11/xgboost%E6%BA%90%E7%A0%81%E4%B9%8B%E5%88%86%E4%BD%8D%E7%82%B9">https://datavalley.github.io/2017/09/11/xgboost源码之分位点</a></li><li>XGBoost解读(2)—近似分割算法 <a href="https://yxzf.github.io/2017/04/xgboost-v2/">https://yxzf.github.io/2017/04/xgboost-v2</a></li><li>Need help understanding xgboost’s approximate split points proposal <a href="https://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal">https://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal</a></li><li>Regression prediction intervals with XGBoost <a href="https://towardsdatascience.com/regression-prediction-intervals-with-xgboost-428e0a018b">https://towardsdatascience.com/regression-prediction-intervals-with-xgboost-428e0a018b</a></li></ul></li><li><p>block</p><ul><li>blocks-for-out-of-core-computation <a href="https://www.hrwhisper.me/machine-learning-xgboost/#blocks-for-out-of-core-computation">https://www.hrwhisper.me/machine-learning-xgboost/#blocks-for-out-of-core-computation</a></li></ul></li></ul><ul><li><a href="https://www.youtube.com/watch?v=oRrKeUCEbq8&amp;t=0s">XGBoost Part 4 (of 4): Crazy Cool Optimizations</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>透視 XGBoost(3) 蘋果樹下的 objective function</title>
      <link href="XGBoost-General-Objective-Function/"/>
      <url>XGBoost-General-Objective-Function/</url>
      
        <content type="html"><![CDATA[<h1 id="XGBoost-General-Objective-Function"><a href="#XGBoost-General-Objective-Function" class="headerlink" title="XGBoost General Objective Function"></a>XGBoost General Objective Function</h1><p>XGboost 是 gradient boosting machine 的一種實作方式，xgboost 也是建一顆新樹 $f_m(x)$ 去擬合上一步模型輸出 $F_{m-1}(x)$ 的 $\text{residual}$</p><script type="math/tex; mode=display">\begin{aligned}F_m(x) & = F_{m-1}(x) + f_m(x)  \\F_{m-1}(x) &= F_0(x) + \sum_{i=0}^{m-1}f_{m-1}(x)\end{aligned}</script><p>不同的是， XGBoost 用一種比較聰明且精準的方式去擬合 residual 建立專屬 XGBoost 的蘋果樹 $f_m(x)$</p><p>此篇首先推導了 XGBoost 的通用 Objective Function，然後解釋為何 second order Taylor expansion 可以讓 XGBoost 收斂更快更準確</p><a id="more"></a><h1 id="XGBoost’s-Objective-Function"><a href="#XGBoost’s-Objective-Function" class="headerlink" title="XGBoost’s Objective Function"></a>XGBoost’s Objective Function</h1><p><strong>XGBoost</strong>  的 objective function 由 loss function term $l(y_i, \hat{y_i})$ 和 regularized term  $\Omega$ 組成</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L}& = [\sum_i^n l(y_i, \hat{y_i}) ] + \sum_{m}\Omega({f_m}) \end{aligned}</script><ul><li>$\mathcal{L}$ 表 objective function</li><li>$l(y_i, \hat{y_i})$ 表 loss function<ul><li>in regression, loss function $l(y_i, \hat{y_i})$ commonly use square error</li><li>in classification，最大化 log likelihood 就是最小化 cross entropy</li></ul></li><li>$f_m$ 表 第 m 步 XGB tree</li><li>regularized term  $\Omega$  跟 XGB tree $f_m(x)$ leaf node number $T_m$ 與 each leaf node output  $\gamma$  有關<ul><li>$\Omega(f_m) = \tau T_m + \cfrac{1}{2}\lambda \lVert \gamma \rVert ^2$<ul><li>$\lambda$  is a regularization parameter</li><li>$T_m$ is the number of leaf in tree $f_m$</li><li>$\tau$ 表對 $T_m$ 的 factor</li></ul></li></ul></li></ul><p>在 第 $m-1$ 步建完 tree $f_{m-1}$時，total cost 為</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L}^{(m-1)}& = [\sum_i^n l(y_i, \hat{y_i}^{(m-1)}) ] + \Omega({f_{(m-1)}}) \\&=[\sum_i^n l(y_i, F_{(m-1)}(x_i) ] + \Omega({f_{(m-1)}})\end{aligned} \tag{4}</script><ul><li>$\hat{y_i}^{(m-1)}$ 表 $F_{m-1}(x_i)$ 預測值</li><li>$f_{m-1}$ 表 第 $m-1$ 步建立的  XGB tree</li></ul><p>進入第 $m$ 步，我們建一顆新樹  $f_m(x)$ 進一步減少 total loss，使得 $\mathcal{L}^{(m)}&lt; \mathcal{L}^{(m-1)}$ ， 則 $m$ 步 cost function 為</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L}^{(m-1)}& = [\sum_i^n l(y_i, \hat{y_i}^{(m-1)}) ] + \Omega({f_{(m-1)}}) \\&=[\sum_i^n l(y_i, F_{(m-1)}(x_i) ] + \Omega({f_{(m-1)}})\end{aligned} \tag{4}</script><p>現在要找出新樹 $f_m(x_i)$ 的每個 leaf node 輸出能使式 (5) $\mathcal{L}^{(m)}$ 最小</p><script type="math/tex; mode=display">\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} [l(y_i, F_{m-1}(x_i) + \gamma)] + \Omega({f_{(m)}}) \tag{6}</script><ul><li>$j$ 表 leaf node index</li><li>$m$ 表第 $m$ 步</li><li>$\gamma_{j,m}$ $m$ 步 $f_m$  第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，所包含的 data sample $x$ 集合</li></ul><p>當 loss function $l(y, \hat{y})$ 為 square error 時，求解式 (6) 不難，但如果是 classification problem 時，就變得很棘手。</p><p>所以 GBDT 對 regression  problem  與 classification  problem 分別用了兩種不同方式求解極值</p><ul><li>for regression ：硬解 derivative ，因爲 MSE 的 derivative 簡單。參閱 <a href="/GBDT-Rregression-Tree-Step-by-Step/" title="一步步透視 GBDT Regression Tree">一步步透視 GBDT Regression Tree</a></li><li>for classification: use the <strong>Second Order Tayler Approximation</strong>  化簡 loss function。參閱 <a href="/GBDT-Classifier-step-by-step/" title="一步步透視 GBDT Classifier">一步步透視 GBDT Classifier</a></li></ul><h2 id="Second-Order-Tayler-Approximation-的步步逼近"><a href="#Second-Order-Tayler-Approximation-的步步逼近" class="headerlink" title="Second Order Tayler Approximation 的步步逼近"></a><strong>Second Order Tayler Approximation 的步步逼近</strong></h2><p>XGBoost 直接以  <strong>Second Order Tayler Approximation</strong> 處理 classification  跟 regression 的 <strong>loss function part $l(y,\hat{y})$</strong></p><p>式(5) 的 <strong>loss function part</strong>  在 $f_m(x)$  附近展開: </p><script type="math/tex; mode=display">\begin{aligned}l(y,\hat{y}^{(m-1)} + f_m(x)) \approx & \quadl(y, \hat{y}^{(m-1)}) + [\cfrac{d}{d \ \hat{y}^{(m-1)}} \ l(y, \hat{y}^{(m-1)})]f_m(x) + \cfrac{1}{2}[\cfrac{d^2}{d \ (\hat{y}^{(m-1)})^2} \ l(y, \hat{y}^{(m-1)})] f_m(x)^2 \\& = l(y, F_{m-1}(x)) + [\cfrac{d}{d \ F_{m-1}} \ l(y, F_{m-1})]f_m(x) + \cfrac{1}{2}[\cfrac{d^2}{d \ F_{m-1}^2} l(y, F_{m-1})] f_m(x)^2  \\&= l(y, F_{m-1}(x)) + gf_m(x) + \cfrac{1}{2}hf_m(x)^2 \end{aligned}\tag{7}</script><ul><li>$\hat{y}^{(m-1)}$ 為  XGB  在 $m-1$ 步的 prediction $F_{m-1}(x)$</li><li>$g = \cfrac{d}{d \ F_{m-1}} \ l(y, F_{m-1})$</li><li>$h = \cfrac{d^2}{d^2 \ F_{m-1}} l(y, F_{m-1})$</li></ul><p>將式 (7) 代入式(5)：</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L}^{(m)}& \approx \ [\sum_i^n(l(y_i, \hat{y}^{(m-1)}_i) +g_if_m(x_i) + \cfrac{1}{2}h_if_m(x_i)^2 ] + \Omega({f_{m}}) \end{aligned} \tag{8}</script><ul><li>$g_i$ 為 first derivative of loss function  related to data sample $x_i$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$</li><li>$f_m(x_i)$ 為 $x_i$  在 XGB tree $f_m(x)$ 的 output value</li></ul><p>式 (8) 即 XGBoost  在 $m$  步的 cost function</p><h2 id="mathcal-L-m-束手就擒"><a href="#mathcal-L-m-束手就擒" class="headerlink" title="$\mathcal{L}^{(m)}$ 束手就擒"></a>$\mathcal{L}^{(m)}$ 束手就擒</h2><p> <strong>Second Order Tayler Approximation 逼近 loss function $l(y, \hat{y})$後 ，</strong>對 $\mathcal{L}^{(m)}$ 求 $\gamma_{j,m}$ 極值，就變得容易多了</p><p>先將 regularization term  $\Omega({f_{m}})$ 展開代入式 (8)，並拿掉之後對微分沒影響的常數項 $l(y_i, \hat{y}^{(m-1)}_i)$</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}& = [\sum_i^n( g_if_m(x_i) + \cfrac{1}{2}h_if_m(x_i)^2) ] + [\tau T_m + \cfrac{1}{2}\lambda \sum^T_{j} \gamma_{m,j}^2]  \\&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m \end{aligned} \tag{9}</script><ul><li>$T_m$ is the number of leaf in tree $f_m$</li><li>$\tau$ 表對 $T_m$ 的 factor</li><li>$\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$  第 $j$ 個 leaf node 的輸出值</li><li>$R_{m,j}$ 表 leaf node j 下的 data sample $x$ 集合</li><li>第一個等式到第二個等式 : 從原本遍歷所有 data sample $x$ 到遍歷所有 leaf node $R_{m,j}$ 下的 data sample</li></ul><p>式 (9) 對 $\gamma_{j,m}^2$ 微分取極值</p><script type="math/tex; mode=display">\cfrac{d}{d \ \gamma_{j, m}} \mathcal{\tilde{L}}^{(m)} = \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j} }g_i) + (\sum_{i \in R_{j,m}}h_i)\gamma_{j,m}] = 0 \tag{10}</script><p>對於已經固定結構了 tree $f_m(x)$，式 (10) leaf node $j$ 的 output value $\gamma_{j,m}$ 為</p><script type="math/tex; mode=display">\gamma_{j,m}^*=- \cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda} \tag{11}</script><ul><li>$g_i$ 為 first derivative of loss function  related to data sample $x_i$ :  $g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, F_{m-1}(x_i))$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$:  $h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$</li></ul><p>將 式(11) $\gamma_{j,m}^*$ 代回 式 (9)，即 objective function 的極值</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m  \\&=\sum^{T_m}_{j=1}[-(\sum_{i \in R_{j,m}}g_i)\cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)(\cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda})^2 ] + \tau T_m  \\& =  \sum^{T_m}_{j=1}[-\cfrac{(\sum g_i)^2}{\sum h_i + \lambda} + \cfrac{1}{2} \cfrac{(\sum g_i)^2}{\sum h_i + \lambda}] + \tau T_m  \\& = -\cfrac{1}{2} \sum^{T_m}_{j=1}[\cfrac{(\sum g_i)^2}{\sum h_i + \lambda} ] + \tau T_m \end{aligned}  \tag{12}</script><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><h3 id="XGBoost-通用-Objective-Function"><a href="#XGBoost-通用-Objective-Function" class="headerlink" title="XGBoost 通用 Objective Function"></a>XGBoost 通用 Objective Function</h3><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m \end{aligned}</script><ul><li>$g_i$ 為 first derivative of loss function  related to data sample $x_i$ :  $g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, F_{m-1}(x_i))$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$:  $h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, F_{m-1}(x_i))$</li><li>$T_m$ is the number of leaf in tree $f_m$</li><li>$\tau$ 表對 $T_m$ 的 factor</li><li>$\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$  第 $j$ 個 leaf node 的輸出值</li></ul><h3 id="Optimal-Output-of-Leaf-Node-j"><a href="#Optimal-Output-of-Leaf-Node-j" class="headerlink" title="Optimal Output of Leaf Node $j$"></a>Optimal Output of Leaf Node $j$</h3><script type="math/tex; mode=display">\gamma_{j,m}^*=- \cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda}</script><h3 id="Optimal-Value-of-Objective-Function"><a href="#Optimal-Value-of-Objective-Function" class="headerlink" title="Optimal Value of Objective Function"></a>Optimal Value of Objective Function</h3><script type="math/tex; mode=display">\mathcal{\tilde{L}}^{*(m)} = \cfrac{1}{2} \sum^{T_m}_{j=1}[\cfrac{(\sum g_i)^2}{\sum h_i + \lambda} ] + \tau T_m</script><h1 id="Why-does-Approximation-with-Taylor-Expansion-Work"><a href="#Why-does-Approximation-with-Taylor-Expansion-Work" class="headerlink" title="Why does Approximation with Taylor Expansion Work?"></a>Why does Approximation with Taylor Expansion Work?</h1><h2 id="Traditional-Gradient-Decent"><a href="#Traditional-Gradient-Decent" class="headerlink" title="Traditional Gradient Decent"></a>Traditional Gradient Decent</h2><p>當我們在做 gradient decent 時，我們是在嘗試 minimize a cost function  $f(x)$，然後讓 $w^{(i)}$ 往 negative gradient 的方向移動</p><script type="math/tex; mode=display">w^{(i+1)} =w^{(i)} - \nabla f(w^{(i)})</script><p>但 gradient $\nabla f(w^{(i)})$ 本身只告訴我們方向，不會告訴我們真正的 the minimum value，我們得到的是每步後盡量靠近 the minimum value 一點，這使的我們無法保證最終到達極值點</p><p>同樣的表達式下的 gradient boost machine 也相似的的問題</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_{m}(x)</script><p>傳統的 GBDT 透過建一顆 regression tree $f_m(x)$ 和合適的  step size $\nu$ (learning rate)  擬合 negative gradient 往低谷靠近，可以說 GBDT tree $f_m(x)$ 本身就代表 loss function 的 $\text{negative graident}$ 的方向 </p><h2 id="Better-Gradient-Decent-Newton’s-Method"><a href="#Better-Gradient-Decent-Newton’s-Method" class="headerlink" title="Better Gradient Decent: Newton’s Method"></a>Better Gradient Decent: Newton’s Method</h2><p>Newton’s method 是個改良自 gradient decent 的做法。他不只單純考慮了 gradient 方向 (即 first order derivative) ，還多考慮了 second order derivative  即所謂 gradient  的變化趨勢，這他更精準的定位極值的<strong>方向</strong> </p><script type="math/tex; mode=display">w^{(i+1)} = w^{(i)} - \cfrac{\nabla f(w^{(i)})}{\nabla ^2f(w^{(i)})} = w^{(i)} - \cfrac{\nabla f(w^{(i)})}{\text{Hess}f(w^{(i)})}</script><p>XGBoost 引入了 Newton’s method 思維，在建立子樹  $f_m(x)$ 不再單純只是 $\text{negatvie gradient}$ 方向 (即 first order derivative) ，還多考慮了 second order derivative 即 gradient 的變化趨勢，這也是為什麼 XGBoost 全稱叫  <strong>Extreme</strong> Gradient  Boosting</p><script type="math/tex; mode=display">f_m(x_i) = \gamma_{j,m}=- \cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda}</script><ul><li>$\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$  第 $j$ 個 leaf node 的輸出值</li><li>$g_i$ 為 first derivative of loss function related to data sample $x_i$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$</li></ul><p>而 loss function $l(y_i,\hat{y_i})$ 會因不同應用: regression , classification, rank 而有不同的 objective function，並不一定存在  second order derivative，所以 XGBoost 利用 Taylor expansion 藉由 polynomial function 可以逼近任何 function 的特性，讓 loss function 在 $f_m(x_i)$ 附近存在 second order derivative </p><script type="math/tex; mode=display">\begin{aligned}l(y,\hat{y}^{(m-1)} + f_m(x)) \approx l(y, F_{m-1}(x)) + gf_m(x) + \cfrac{1}{2}hf_m(x)^2\end{aligned}</script><p>可以說，XGBoos tree 以一種更聰明的方式往 the minimum 移動</p><h2 id="總結-1"><a href="#總結-1" class="headerlink" title="總結"></a>總結</h2><ul><li>一般 GBDT 用 regression tree 擬合 residuals，本質上是往 negative gradient 方向移動</li><li>XGBoost tree $f_m(x)$ 擬合 residuals，同時考慮 gradient 的方向和 gradient 變化趨勢，這讓他朝 optimal value 移動時顯得更加聰明有效<ul><li>gradient 的方向： first order derivative</li><li>gradient 變化趨勢： second order derivative</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a></li></ul><ul><li><p><a href="https://www.youtube.com/watch?v=OtD8wVaFm6E">XGBoost Part 1 (of 4): Regression</a></p></li><li><p><a href="https://www.youtube.com/watch?v=ZVFeW798-2I&amp;t=19s">XGBoost Part 3 (of 4): Mathematical Details</a></p></li></ul><ul><li>Tayler expansion<ul><li>XGBoost Loss function Approximation With Taylor Expansion <a href="https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion">https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion</a></li><li><a href="https://en.wikipedia.org/wiki/Taylor_series#Approximation_and_convergence">https://en.wikipedia.org/wiki/Taylor_series#Approximation_and_convergence</a></li><li><a href="https://en.wikipedia.org/wiki/Taylor&#39;s_theorem">https://en.wikipedia.org/wiki/Taylor’s_theorem</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>透視 XGBoost(2) 圖解 Classification</title>
      <link href="XGBoost-for-classification/"/>
      <url>XGBoost-for-classification/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><ul><li>XGBoost 與  GBDT 相比，同樣都是擬合上一步 $F_{m-1}(x)$ 預測值的 residual 。不同的是 XGBoost 在擬合 residual 時更為聰明有效，他有自己衡量分裂增益 gain 的方式去擬合  residual 建立  XGB tree $f_m(x)$</li></ul><script type="math/tex; mode=display">Gain = Left_{\text{similarity}} + Right_{\text{similarity}}  - Root_{\text{similarity}} \tag{1}</script><ul><li>分裂增益 gain ，的背後原理可以透過 XGBoost  通用 objective function 並填入 classification/regression/rank/etc.. 各自的 loss function  後推導出的 $\text{first derivative of loss function} = g_i \quad \text{and} \quad \text{second derivative of loss function} = h_i$</li><li>XGBoost 的 通用 objective function 由兩部份組成<ul><li>first part is loss function，根據不同任務有不同的 loss function </li><li>second part is regularization term 防止 overfitting，限制 leaf node 輸出值大小 和 leaf nodes 的總數</li></ul></li></ul><a id="more"></a><h2 id="從-Gradient-Boosting-說起"><a href="#從-Gradient-Boosting-說起" class="headerlink" title="從 Gradient Boosting 說起"></a>從 Gradient Boosting 說起</h2><p><img src="https://i.imgur.com/tmbh2r0.png" alt="GBDT Framework" style="zoom: 33%;" /></p><p>XGBoost 跟  gradient boosting algorithm 框架一樣，皆是依序建立多棵樹 $f_1(x), f_2(x), ….,f_M(x)$ 組成模型</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu\sum^m_{i=1} f_i(x) = F_{m-1} + \nu f_{m-1}(x)</script><ul><li>其中第 $m$  步的 tree $f_m(x)$ 是擬合模型 $F_{m-1}(x)$ 預測值 $\text{predicted }$ 與 真實值 $\text{observed}$ 的 $\text{residual}$</li><li>$\nu$ 為 learning rate</li></ul><p>算法差別主要體現在</p><ul><li>objective function 的設計</li><li>Step 2 (B) (C) 建樹，GBDT 是建一顆傳統 regression tree $f_m(x)$ 去擬合 $\text{residual}$;  XGBoost 有自己衡量分裂 gain 的方式去擬合 residual 建立 XGB tree $f_m(x)$</li></ul><p>可以說 XGBoost 用一種比較精準的方式去擬合 residual 建立子樹 $f_m(x)$</p><p>以下章節主要分兩大塊</p><ol><li><strong>XGBoost for Classification：</strong> 藉由 classification  介紹 XGBoost 如何 train 一棵  XGB tree，以圖文方式說明 XGB tree 如何擬合目標到剪枝。此章節不涉及公式證明，只有少量運算，適合快速理解 XGB 訓練流程</li><li><strong>XGBoost Classification Math Background</strong>：此章節深入討論在前一章節中用到的公式原理，並給予證明，適合深入理解 XGBoost 為何 work</li></ol><p>篇幅關係 XGBoost 的優化手段放在 <a href="/XGBoost-cool-optimization/" title="透視 XGBoost(4) 神奇 optimization 在哪裡？">透視 XGBoost(4) 神奇 optimization 在哪裡？</a></p><h1 id="XGBoost-for-Classification"><a href="#XGBoost-for-Classification" class="headerlink" title="XGBoost for Classification"></a>XGBoost for Classification</h1><h2 id="前置-background"><a href="#前置-background" class="headerlink" title="前置 background"></a>前置 background</h2><p>如同在 GBDT - classification 裡一樣 <a href="/GBDT-Classifier-step-by-step/" title="一步步透視 GBDT Classifier">一步步透視 GBDT Classifier</a></p><p>模型輸出 $F_m(X)$ 為 $\log(odds)$，但衡量 residual 時與 probability 有關，兩者互相轉換的公式</p><ul><li>probability $p$ → $\log(odds)$</li></ul><script type="math/tex; mode=display">\log(odds) = \cfrac{p}{(1-p)}</script><ul><li>$\log(odds)$   → $p$</li></ul><script type="math/tex; mode=display">p = \cfrac{\exp^{\log(odds)}}{1 + \exp^{\log(odds)}}</script><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>之後用到的例子，數據如下</p><p>四筆 data samples，目標是用 Drug Dosage 預測藥物是否有效用 (Drug Effectiveness)</p><p><img src="https://i.imgur.com/lqbrSmp.png" style="zoom: 67%;" /></p><h2 id="如何建一顆-XGB-tree-f-m-x"><a href="#如何建一顆-XGB-tree-f-m-x" class="headerlink" title="如何建一顆 XGB tree $f_m(x)?$"></a>如何建一顆 XGB tree $f_m(x)?$</h2><p>Classification  problem 上，XGBoost 會在所有的特徵值中選取最佳分裂點，最終建成一顆 binary tree $f_m(x)$</p><p>建樹的過程涉及</p><ol><li>Fitting Target 擬合目標</li><li>分裂好壞的恆量，如 CART 用 gini gain 衡量分類樹</li><li>Pruning 剪枝</li><li>Output Value 決定每個 leaf node 的唯一輸出</li></ol><h3 id="f-m-x-擬合的目標是什麼？"><a href="#f-m-x-擬合的目標是什麼？" class="headerlink" title="$f_m(x)$ 擬合的目標是什麼？"></a>$f_m(x)$ 擬合的目標是什麼？</h3><p>$f_m(x)$ 擬合的目標是 $\text{residual}$ ，利用   data sample  x  的所有特徵建一顆特殊的 $\text{regression tree}$  去擬合 $\text{residual}$</p><script type="math/tex; mode=display">\text{residual = observed - predicted}</script><p>classification 的  residual  計算與 probability 有關，observed 即 Drug Effectiveness False → 0 True → 1 ; predicted 為模型輸出的 probability $\sigma(F_m(x))$</p><p>以 $m=1$  舉例</p><p><img src="https://i.imgur.com/CiiDUDZ.png" alt="XGBoost%20for%20Classification%2025279fd7fd284e38afd31bf8a6e8463e/Untitled%202.png" style="zoom:33%;" /></p><p><img src="https://i.imgur.com/vCLa8ge.png" style="zoom:50%;" /></p><ul><li>XGBoost  預設的初始值 $F_0(x)$ probability，預設為 0.5，$\log(odds)$ = 0</li><li>ep_0_pre 表 $F_0(x)$ 輸出的 $\log(odds)$</li><li>ep_0_prob 表 $F_0(x)$ 輸出的 probability</li><li>ep_1_residual 表 $m=1$ 時 observed y 與 ep_0_prob 的 residual</li></ul><p>ep_1_residual 即 $m=1$ 時，XGB tree $f_1(x)$  要擬合的目標</p><h3 id="建-tree-時如何衡量分裂點好壞？"><a href="#建-tree-時如何衡量分裂點好壞？" class="headerlink" title="建 tree 時如何衡量分裂點好壞？"></a>建 tree 時如何衡量分裂點好壞？</h3><p>建分支時依序在特徵 $\text{Drug Dosage}$ 的 data sample value $\text{[3, 8,  12, 17]}$ 中尋找最優分裂點切分 residual $\text{[-0.5, 0.5, 0.5, -0.5]}$</p><p><img src="https://i.imgur.com/uL9DBt4.png" style="zoom: 67%;" /></p><p>決定分裂點的優劣取決於 gain</p><script type="math/tex; mode=display">Gain = Left_{\text{similarity}} + Right_{\text{similarity}}  - Root_{\text{similarity}} \tag{1}</script><p>分裂 node ，會產生 left child node $Left$ 與 right child node $Right$，分別計算三者的 similarity score</p><script type="math/tex; mode=display">\text{Similarity Score} =  \cfrac{(\sum \text{residual}_i)^2}{ \sum[\text{previous probability}_i \times (1- \text{previous probability}_i)]+ \lambda} \tag{2}</script><ul><li>$\lambda$ is a regularization parameter， 跟之後的 pruning 有關，這邊先假設 0</li><li>$\text{previous probability}_i$ 為上一步模型輸出的 probability $\sigma(F_{m-1}(x))$</li></ul><p><strong>注意！！ 排序的是 $\text{Drug Dosage}$，分裂點依序在 $\text{Drug Dosage}$ 中找，但被分開到左右子樹的是 $\text{residual}$</strong></p><h3 id="計算-Gain"><a href="#計算-Gain" class="headerlink" title="計算 Gain"></a>計算 Gain</h3><p>以 $\text{Dosage &lt; 15}$ 做為分裂點計算 gain，令 $\lambda =0$</p><p><img src="https://i.imgur.com/UVCKJwp.png" style="zoom:67%;" /></p><ul><li>root similarity $\cfrac{(-0.5 + 0.5 + 0.5 -0.5)^2}{\text{whatever you are}} =0$</li><li>left node similarity:$\cfrac{(-0.5 + 0.5 + 0.5)^2}{(0.5  \times (1-0.5)) +(0.5 \times (1-0.5)) + (0.5 \times(1-0.5)) + 0} = 0.33$</li><li>right node similarity: $\cfrac{0.5^2}{0.5 \times (1-0.5) + 0} = 1$</li><li>$\text{Gain } = 0.33 + 1 -0 = 1.33$</li></ul><p>上面只是示範如何計算 gain ，依序將 $\text{[3, 8,  12, 17]}$ 可能分裂點計算過 gain 後選取最大的 gain 即最優分裂點</p><p>當然這只是第一個分支，往下還有 第二個分支 第三個分支 … etc</p><p>假設最終我們找到的樹結構為：</p><p><img src="https://i.imgur.com/F87svJj.png" style="zoom:67%;" /></p><p>建完一棵樹後，接下來 XGBoost 有多個手段對樹結構 pruning</p><h3 id="如何-Pruning-Cover"><a href="#如何-Pruning-Cover" class="headerlink" title="如何 Pruning - Cover"></a>如何 Pruning - Cover</h3><blockquote><p>Cover is related to the minimum number of Residuals in a leaf</p></blockquote><p>XGBoost 其中一個 pruning 方法叫 <strong>cover</strong>，在 classification 中 <strong>cover</strong> 的運算為 式(2) similarity score 分母中的前項</p><script type="math/tex; mode=display">\text{Cover} = \sum[\text{previous probability}_i \times (1- \text{previous probability}_i)]</script><p>然後會有個 $\text{Cover}_{threshold}$ 的值決定是否 pruning 此 leaf node</p><p>例如，把 $\text{Cover}_{threshold}$設為  1 時，所有小於 threshold 的 leaf node 都會被 pruning：</p><p><img src="https://i.imgur.com/aZuNYb8.png" style="zoom:67%;" /></p><ul><li>XGBoost 規定一顆 XGB tree $f(x)$ 不能只有 root，所以會保留一個分岔</li><li>$\text{Cover}_{threshold}$ 在 XGBoost 參數中叫 min_child_weight</li></ul><h3 id="如何-Pruning-tau"><a href="#如何-Pruning-tau" class="headerlink" title="如何 Pruning - $\tau$"></a>如何 Pruning - $\tau$</h3><p>此法 對  new tree $f_m(x)$ 的  pruning 建立在 gain 上，透過設定一個 gain threshold $\tau$ 決定是否剪枝</p><p><img src="https://i.imgur.com/lQi0RAM.png" style="zoom:67%;" /></p><ul><li>若 $\tau = 2$，則 分裂點 $＂Dosage &lt; 5＂$ 的  $gain = 2.66 &gt; 2 = \tau$，不會被剪枝<ul><li>$\textit{Dosage &lt; 15}$ 的 $gain = 1.33 &lt; 3 = \tau$ ，理應要被 pruning，但因爲子節點沒被剪枝，父節點 $\textit{Dosage &lt; 15}$ 也保留</li></ul></li><li>若 $\tau =3$，則 分裂點 $＂Dosage &lt; 5＂$ 的  $gain = 2.66 &lt; 3 = \tau$ ，會被剪枝<ul><li>$\textit{Dosage &lt; 15}$ 的 $gain = 1.33 &lt; 3 = \tau$，理應要被 pruning，但因為 tree  需要一個根節點，所以保留</li></ul></li></ul><p>P.S. gain threshold $\tau$  在 XGBoost 叫 min_split_loss</p><h3 id="如何決定-leaf-node-的-output-value"><a href="#如何決定-leaf-node-的-output-value" class="headerlink" title="如何決定 leaf node  的 output value?"></a>如何決定 leaf node  的 output value?</h3><p>classification 中決定每個 leaf node 的 output value 公式跟式(2) similarity score 很像，差別在分子部份的 $\text{sum of residuals}$ 是否取平方</p><script type="math/tex; mode=display">\text{Output Value} =  \cfrac{(\sum \text{residual}_i)}{ \sum[\text{previous probability}_i \times (1- \text{previous probability}_i)]+ \lambda} \tag{3}</script><ul><li>$\lambda$ is a regularization parameter，這邊先假設 0</li></ul><p>計算每個 leaf node output value 後</p><p><img src="https://i.imgur.com/eEWVsjD.png" style="zoom:67%;" /></p><h2 id="如何整合多棵樹-f-1-x-f-2-x-…-f-m-x-的輸出"><a href="#如何整合多棵樹-f-1-x-f-2-x-…-f-m-x-的輸出" class="headerlink" title="如何整合多棵樹 $f_1(x), f_2(x),…,f_m(x)$ 的輸出"></a>如何整合多棵樹 $f_1(x), f_2(x),…,f_m(x)$ 的輸出</h2><p>$F_M(x)$  的計算與 GBDT 一樣</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^M_{i=1} f_i(x) = F_{M-1} + \nu f_m(x)</script><p><img src="https://i.imgur.com/fJ8GHuI.png" style="zoom: 50%;" /></p><p>得注意的是，$F_m(x)$ 輸出的是 $\log(odds)$ ，得通過 $p = \sigma(\log(odds))$ ，才會是 predicted probability。</p><p>假設只建立一顆 XGB tree $f_1(x)$，  $\text{learning rate = 0.3}$</p><p><img src="https://i.imgur.com/eEWVsjD.png" style="zoom:67%;" /></p><p>則每個 data samples 新的 prediction  為</p><script type="math/tex; mode=display">F_1(x) = F_0(x) + 0.3* f_1(x)</script><p><img src="https://i.imgur.com/79BuWZx.png" style="zoom:67%;" /></p><ul><li>ep_0_prob: $\sigma(F_0(x))$ 算出的 probability of Drug Effectiveness</li><li>ep_0_pre: $F_0(x)$ 輸出的 predicted log odds</li><li>ep_1_leaf_node_output:  XGB tree  $f_1(x_i)$ 對每個 data sample $x_i$ 的輸出值。公式請見式(3)</li><li>ep_1_pre: $F_1(x)$ 輸出的 predicted log odds, $F_1(x) = F_0(x) + \nu f_1(x)$</li><li>ep_1_prob  $\sigma(F_1(x))$ 算出的 probability of Drug Effectiveness</li></ul><p>再次提醒</p><ul><li>$F_m(x)$ 輸出 $log(odds)$</li><li>$F_m(x)$ 輸出的 probability 為 $\sigma(F_m(x))$</li><li>$\text{residual }$ 的計算與 probability 有關</li></ul><h1 id="XGBoost-Classification-Math-Background"><a href="#XGBoost-Classification-Math-Background" class="headerlink" title="XGBoost Classification Math Background"></a>XGBoost Classification Math Background</h1><h2 id="Regression-的-Similarity-Score-怎麼來的？"><a href="#Regression-的-Similarity-Score-怎麼來的？" class="headerlink" title="Regression 的 Similarity Score 怎麼來的？"></a>Regression 的 Similarity Score 怎麼來的？</h2><blockquote><p>式 (2) similarity score 怎麼得出的</p></blockquote><h3 id="XGBoost-的通用-Objective-Function"><a href="#XGBoost-的通用-Objective-Function" class="headerlink" title="XGBoost 的通用 Objective Function"></a>XGBoost 的通用 Objective Function</h3><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m \end{aligned}\tag{4}</script><ul><li>$g_i$ 為 first derivative of loss function  related to data sample $x_i$ :  $g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, \hat{y_i})$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$:  $h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, \hat{y_i})$</li><li>$T_m$ is the number of leaf in tree $f_m$</li><li>$\tau$ 表對 $T_m$ 的 factor</li><li>$\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$  第 $j$ 個 leaf node 的輸出值</li><li>$R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合</li></ul><p>optimal output $\gamma_{j,m}^*$ of leaf node $j$ </p><script type="math/tex; mode=display">\gamma_{j,m}^*=- \cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda}  \tag{5}</script><p>詳細推導參閱 <a href="/XGBoost-General-Objective-Function/" title="透視 XGBoost(3) 蘋果樹下的 objective function">透視 XGBoost(3) 蘋果樹下的 objective function</a></p><h2 id="Classification-的-Similarity-Score"><a href="#Classification-的-Similarity-Score" class="headerlink" title="Classification 的 Similarity Score"></a>Classification 的 Similarity Score</h2><p>式 (2) classification similarity score 如下</p><script type="math/tex; mode=display">\text{Similarity Score} =  \cfrac{(\sum \text{residual}_i)^2}{ \sum[\text{previous probability}_i \times (1- \text{previous probability}_i)]+ \lambda} \tag{2}</script><p>XGBoost  分裂的目的是要使 式(4) objective function 越小越好，怎麼評判分裂前後收益？式(1) 評價了分裂後 left/right leaf node 可以得到的＂score＂與分裂前 root node 的 ＂score＂ 差值計算分裂前後增益 gain</p><script type="math/tex; mode=display">Gain = Left_{\text{similarity}} + Right_{\text{similarity}}  - Root_{\text{similarity}} \tag{1}</script><p>所以 similarity score 肯定得跟 objective  function 有關，才能正確的恆量 gain，但 similarity score 是 ＂score “ 越大越好， objective function 是要 minimize 的，得越小越好，兩者如何扯上關係？</p><p>觀察一下 objective function</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m \end{aligned} \tag{4}</script><p> 因為我們要衡量單個 node 得 gain，式 (4)中，跟某個 node 直接相關的 part 為 summation of all leaf node 裡面的 equation </p><script type="math/tex; mode=display">[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] \tag{6}</script><ul><li>$R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合</li></ul><p>而 式(5) 給出了單個節點 optimal output value  $\gamma_{j,m}^*$</p><script type="math/tex; mode=display">\gamma_{j,m}^*=- \cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda}  \tag{5}</script><p>將 式(5) 代入 式(6) 可以得出 式(6)的極小值</p><script type="math/tex; mode=display">\begin{aligned}(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2 &= -（\sum_{i \in R_{j,m}}g_i)\cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)(\cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda})^2  \\& =  -\cfrac{(\sum g_i)^2}{\sum h_i + \lambda} + \cfrac{1}{2} \cfrac{(\sum g_i)^2}{\sum h_i + \lambda} \\& = -\cfrac{1}{2}\cfrac{(\sum g_i)^2}{\sum h_i + \lambda} \end{aligned} \tag{7}</script><p>因爲要的是 ＂score＂ ，所以讓 loss 對 x 軸做對稱，拿掉係數項 $\frac{1}{2}$ </p><script type="math/tex; mode=display">\text{Similarity Score} = \cfrac{(\sum g_i)^2}{\sum h_i + \lambda} \tag{8}</script><ul><li>$g_i$ 為 first derivative of loss function  related to data sample $x_i$ :  $g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, \hat{y_i})$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$:  $h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, \hat{y_i})$</li><li>拿掉係數項沒影響，原因是計算  gain (式 (1)) 時只需要相對 (relative)  的值</li></ul><p><img src="https://i.imgur.com/5NywYVb.png" style="zoom: 33%;" /></p><p>式 (8) 中的 $g_i, h_i$ 分別表 loss function $l(y,\hat{y})$ 的 first derivative and second derivative </p><p>classification  常用的 loss function 為 cross entropy/logistic loss</p><script type="math/tex; mode=display">\begin{aligned}l(y_i,\hat{y_i}) &= -[y_i\log(p_i) + (1-y_i)\log(1-p_i)] \\&=-y_i \log(odds) + \log(1 + \exp(\log(odds)) \\&= -y_i F_{m-1}(x_i) + \log(1 + \exp(F_{m-1}(x_i)))\end{aligned}\tag{9}</script><ul><li>$F_{m-1}(x_i)$ 輸出的是 log(odds)</li><li>$p_i$ 為 $F_{m-1}(x_i)$ 輸出的 probability , $p_i= \sigma(F_{m-1}(x_i)) = \cfrac{\exp(F_{m-1}(x_i))}{1 + \exp(F_{m-1}(x_i))}$</li></ul><p>將式 (9) 代入 $g_i, h_i$，得到 cross entropy/logistic loss  下的 $g_i, h_i$</p><script type="math/tex; mode=display">\begin{aligned}g_i &= \cfrac{d}{d \ F_{m-1}} \ l(y_i, \hat{y_i}) \\&= -y_iF_{m-1}(x_i) + \log(1 + \exp(F_{m-1}(x_i)) \\&= -y_i + \cfrac{\exp(F_{m-1}(x_i))}{1 + \exp(F_{m-1}(x_i))} \\& = -(y_i - p_i) = \text{negatvie residual} \\h_i &= \cfrac{d^2}{d^2 \ F_{m-1}} l(y_i, \hat{y_i})  \\&= \cfrac{d^2}{d F_{m-1}^2} [-y_i F_{m-1}(x_i) + \log(1 + \exp(F_{m-1}(x_i)))] \\&= \cfrac{d}{d F_{m-1}} [-y_i + \cfrac{\exp(F_{m-1}(x_i))}{1 + \exp(F_{m-1}(x_i))}] \\&= \cfrac{\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))} \times \cfrac{1}{(1 + \exp(F_{m-1}(x_i)))} \\&= p_i \times (1-p_i)\end{aligned} \tag{10}</script><p>代回 式(8)，得證</p><script type="math/tex; mode=display">\begin{aligned}\text{Similarity Score} &= \cfrac{(\sum_{i \in R_{j,m}} g_i)^2}{\sum_{i \in R_{j,m}} h_i + \lambda} \\& = \cfrac{(\sum-(y_i-p_i))^2}{(\sum p_i (1-p_i)) + \lambda} \\&= \cfrac{(\sum \text{residual}_i)^2}{(\sum \text{previous probability}_i \times (\text{1 - preveious probability}_i) ) + \lambda}\end{aligned}</script><h2 id="Regression-Leaf-output-公式怎麼來的？"><a href="#Regression-Leaf-output-公式怎麼來的？" class="headerlink" title="Regression Leaf output 公式怎麼來的？"></a>Regression Leaf output 公式怎麼來的？</h2><blockquote><p>式(3) each leaf node 的 output 公式怎麼得出的？</p></blockquote><script type="math/tex; mode=display">\text{Output Value} =  \cfrac{(\sum \text{residual}_i)}{ \sum[\text{previous probability}_i \times (1- \text{previous probability}_i)]+ \lambda} \tag{3}</script><p>從通用 objective function 的 optimal output value  $\gamma_{j,m}^*$ 可求出</p><script type="math/tex; mode=display">\gamma_{j,m}^*=- \cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda}  \tag{5}</script><p>將 式 (10)的 $g_i$ $h_i$ 代入式 (5)，得證</p><script type="math/tex; mode=display">\gamma_{j,m}^*=\cfrac{\sum_{i\in R_{j,m}} (y_i - p_i)}{\sum_{i \in R_{j,m}}(p_i \times (1- p_i)) + \lambda}  = \cfrac{(\sum \text{residual}_i)}{ \sum[\text{previous probability}_i \times (1- \text{previous probability}_i)]+ \lambda}</script><ul><li>$p_i$ 為 $F_{m-1}(x_i)$ 輸出的 probability , $p_i= \sigma(F_{m-1}(x_i)) = \cfrac{\exp(F_{m-1}(x_i))}{1 + \exp(F_{m-1}(x_i))}$</li></ul><h2 id="關於-Cover"><a href="#關於-Cover" class="headerlink" title="關於 Cover"></a>關於 Cover</h2><blockquote><p>Cover is related to the minimum number of Residuals in a leaf</p></blockquote><p>在 ＂ 如何 Pruning - Cover＂ 這章節提到 classification 中 <strong>cover</strong> 的運算為 式(2) similarity score 分母中的前項</p><script type="math/tex; mode=display">\text{Cover} = \sum[\text{previous probability}_i \times (1- \text{previous probability}_i)]</script><p>實際上就是 式(8) similarity score 中分母部份的 summation of $h_i$</p><script type="math/tex; mode=display">\text{Cover} = \sum_{i \in R_{j,m}} h_i</script><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a></li><li>What makes “XGBoost” so Extreme? <a href="https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb">https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb</a><ul><li>XGBoost-from-scratch-python</li></ul></li><li>Boosting algorithm: XGBoost <a href="https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d">https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d</a></li><li><a href="https://www.hrwhisper.me/machine-learning-xgboost/#blocks-for-out-of-core-computation">https://www.hrwhisper.me/machine-learning-xgboost/</a></li></ul><ul><li><p><a href="https://www.youtube.com/watch?v=8b1JEDvenQU">XGBoost Part 2 (of 4): Classification</a></p></li><li><p><a href="https://www.youtube.com/watch?v=ZVFeW798-2I&amp;t=19s">XGBoost Part 3 (of 4): Mathematical Details</a></p></li></ul><ul><li><p>My post</p><ul><li><a href="/GBDT-Rregression-Tree-Step-by-Step/" title="一步步透視 GBDT Regression Tree">一步步透視 GBDT Regression Tree</a></li><li><a href="/GBDT-Classifier-step-by-step/" title="一步步透視 GBDT Classifier">一步步透視 GBDT Classifier</a></li><li><a href="/xgboost-for-regression/" title="透視 XGBoost(1) 圖解 Regression">透視 XGBoost(1) 圖解 Regression</a></li><li><a href="/XGBoost-General-Objective-Function/" title="透視 XGBoost(3) 蘋果樹下的 objective function">透視 XGBoost(3) 蘋果樹下的 objective function</a></li><li><a href="/XGBoost-cool-optimization/" title="透視 XGBoost(4) 神奇 optimization 在哪裡？">透視 XGBoost(4) 神奇 optimization 在哪裡？</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>透視 XGBoost(1) 圖解 Regression</title>
      <link href="xgboost-for-regression/"/>
      <url>xgboost-for-regression/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><img src="https://i.imgur.com/aaIKrZN.png" alt="XGBoost"></p><p>XGboost 是 gradient boosting machine 的一種實作方式，xgboost 也是建一顆新樹 $f_m(x)$ 去擬合上一步模型輸出 $F_{m-1}(x)$ 的 $\text{residual}$</p><script type="math/tex; mode=display">\begin{aligned}F_m(x) & = F_{m-1}(x) + f_m(x)  \\F_{m-1}(x) &= F_0(x) + \sum_{i=0}^{m-1}f_{m-1}(x)\end{aligned}</script><p>不同的是，Xgboost 做了大量運算和擬合的優化，這讓他比起傳統 GBDT 更為高效率與有效</p><a id="more"></a><p>跟擬合目標有關的有</p><ul><li>Second Order Tayler Approximation</li><li>Approximate Greedy Algorithm</li><li>Weighted Quantile Sketch</li><li>Sparsity-Aware Split Finding</li></ul><p>跟工程優化有關的有</p><ul><li>Cache - Aware Access</li><li>Block for Out-of-Core Computation</li><li>Parallel Learning</li></ul><p>以下章節主要分兩大塊</p><ol><li><strong>XGBoost for Regression：</strong> 藉由 regression 介紹 XGBoost 如何 train 一棵  XGB tree，以圖文方式說明 XGB tree 如何擬合目標到剪枝。此章節不涉及公式證明，只有少量運算，適合快速理解 XGB 訓練流程</li><li><strong>XGBoost Regression Math Background</strong>：此章節深入討論在前一章節中用到的公式原理，並給予證明，適合深入理解 XGBoost 為何 work</li></ol><p>篇幅關係 XGBoost 的優化手段放在 <a href="/XGBoost-cool-optimization/" title="透視 XGBoost(4) 神奇 optimization 在哪裡？">透視 XGBoost(4) 神奇 optimization 在哪裡？</a></p><h1 id="XGBoost-for-Regression"><a href="#XGBoost-for-Regression" class="headerlink" title="XGBoost for Regression"></a>XGBoost for Regression</h1><h2 id="從-gradient-boosting-說起"><a href="#從-gradient-boosting-說起" class="headerlink" title="從 gradient boosting 說起"></a>從 gradient boosting 說起</h2><p><img src="https://i.imgur.com/pEkt5eh.png" alt="GBDT Framework" style="zoom:33%;" /></p><p>XGBoost 跟  gradient boosting algorithm 框架一樣，皆是依序建立多棵樹 $f_1(x), f_2(x), ….,f_M(x)$ 組成模型</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu\sum^m_{i=1} f_i(x) = F_{m-1} + \nu f_{m-1}(x)</script><ul><li>其中第 $m$  步的 tree $f_m(x)$ 是擬合模型 $F_{m-1}(x)$ 預測值 $\text{predicted }$ 與 真實值 $\text{observed}$ 的 $\text{residual}$</li><li>$\nu$ 為 learning rate</li></ul><p>算法差別主要體現在</p><ul><li>objective function 的設計</li><li>Step 2 (B) (C) 建樹，GBDT 是建一顆傳統 regression tree $f_m(x)$ 去擬合 $\text{residual}$;  XGBoost 有自己衡量分裂 gain 的方式去擬合 residual 建立 XGB tree $f_m(x)$</li></ul><p>可以說 XGBoost 用一種比較精準的方式去擬合 residual 建立子樹 $f_m(x)$</p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>之後用到的例子，數據如下：</p><p>假設有多筆 data sample，目標是用 drug dosage  預測 drug effectiveness </p><p><img src="https://i.imgur.com/SzfnonV.png" style="zoom: 50%;" /></p><h2 id="如何建一顆-XGB-tree-f-m-x"><a href="#如何建一顆-XGB-tree-f-m-x" class="headerlink" title="如何建一顆 XGB tree $f_m(x)$ ?"></a>如何建一顆 XGB tree $f_m(x)$ ?</h2><p>Regression problem 上，XGB 在所有 data sample x 的每個特徵下的值裡尋找最佳分裂點，最終建成一顆 binary tree $f_m(x)$</p><p>建樹的過程涉及</p><ol><li>Fitting Target 擬合目標</li><li>分裂好壞的恆量，如 CART 用 gini gain 衡量分類樹</li><li>Pruning 剪枝</li><li>Output Value 決定每個 leaf node 的唯一輸出</li></ol><h3 id="f-m-x-擬合的目標是什麼？"><a href="#f-m-x-擬合的目標是什麼？" class="headerlink" title="$f_m(x)$ 擬合的目標是什麼？"></a>$f_m(x)$ 擬合的目標是什麼？</h3><p>$f_m(x)$ 擬合的目標是 $\text{residual}$ ，利用   data sample  x  的所有特徵建一顆特殊的 $\text{regression tree}$  去擬合 $\text{residual}$</p><script type="math/tex; mode=display">\text{residual = observed - predicted}</script><p><img src="https://i.imgur.com/5Y17D5q.png" style="zoom:33%;" /></p><p>以 $m=1$  時舉例</p><ul><li>XGBoost  的 predict 初始值 $F_0(x)$，預設皆為 0.5</li></ul><p><img src="https://i.imgur.com/jaMkskD.png" style="zoom: 50%;" /></p><ul><li>ep_0_predict: XGBoost 的初始預測值 $F_0(x)$，預設都是 0.5</li><li>ep_1_residual:  $observed$ 與 $F_0(x)$ 間的  $residual$ 也就是 XGB tree  $f_1(x)$ 要擬合的目標</li></ul><h3 id="建-tree-時如何衡量分裂點好壞？"><a href="#建-tree-時如何衡量分裂點好壞？" class="headerlink" title="建 tree 時如何衡量分裂點好壞？"></a>建 tree 時如何衡量分裂點好壞？</h3><p>建分支時依序在特徵 $\text{Drug Dosage}$ 的 data sample value $\text{[10, 20,  25, 35]}$ 中尋找最優分裂點切分 residual $\text{[-10.5, 6.5, 7.5 -7.5]}$</p><p><img src="https://i.imgur.com/44NNISC.png"  /></p><p>決定分裂點的優劣取決於 $Gain$</p><script type="math/tex; mode=display">Gain = Left_{\text{similarity}} + Right_{\text{similarity}}  - Root_{\text{similarity}} \tag{1}</script><p>分裂 node，會產生 left child node  與  right child node，分別計算三者的  $\text{similarity score}$</p><script type="math/tex; mode=display">\text{Similarity Score} =  \cfrac{(\sum \text{residual})^2}{\text{Number of Residuals } + \lambda} \tag{2}</script><ul><li>$\lambda$ is a regularization parameter，這邊先假設 0</li></ul><p><strong>注意！！ 排序的是 $\text{Drug Dosage}$，分裂點依序在 $\text{Drug Dosage}$ 中找，但被分開到左右子樹的是 $\text{residual}$</strong></p><h3 id="在-data-sample-的特徵-textit-drug-dosage-中找出-gain-最大的做為分裂點"><a href="#在-data-sample-的特徵-textit-drug-dosage-中找出-gain-最大的做為分裂點" class="headerlink" title="在 data sample 的特徵 $\textit{drug dosage}$ 中找出  $gain$  最大的做為分裂點"></a><strong>在 data sample 的特徵 $\textit{drug dosage}$ 中找出  $gain$  最大的做為分裂點</strong></h3><p>以 Dosage &lt; 15 當作分裂點的 $gain = 110.25 + 14.08 - 4 = 120.33$</p><p><img src="https://i.imgur.com/6KGFwF8.png"  /></p><p>以 Dosage &lt; 22.5 當作分裂點的 $gain = 8 + 0 -4 = 4$</p><p><img src="https://i.imgur.com/joY0CTE.png"  /></p><p>顯然以 Dosage &lt; 15 當作分裂點，好於以 Dosage &lt; 22.5 當作分裂點。</p><p>當然，還得把剩下的可能分裂點看完，才能決定最終分裂點。</p><p>找個第一個分裂點，還得找下個子樹的分裂點，如此周而復始，最終得到樹的結構</p><p><img src="https://i.imgur.com/UUv521Z.png"  /></p><h3 id="如何-Pruning"><a href="#如何-Pruning" class="headerlink" title="如何 Pruning ?"></a>如何 Pruning ?</h3><p>XGBoost 對  new tree $f_m(x)$ 的  pruning 建立在 gain 上，透過設定一個 gain threshold $\tau$ 決定是否剪枝</p><p><img src="https://i.imgur.com/UUv521Z.png"  /></p><ul><li>若 $\tau = 130$，則 分裂點 $＂Dosage &lt; 30＂$ 的  $gain = 140.17 &gt; 130 = \tau$，不會被剪枝，因爲子節點沒被剪枝，父節點 $\textit{Dosage &lt; 15}$ 也保留</li><li>若 $\tau =150$，則 分裂點 $＂Dosage &lt; 30＂$ 的  $gain = 140.17 &lt; 150 = \tau$ ，會被剪枝，最終只保留 父節點 $\textit{Dosage &lt; 15}$，因爲需要一個根節點</li></ul><p>P.S. 在 XGBoost 論文裡，gain threshold 的符號是 $\gamma$，但因為 notaion 衝突，這邊換成  $\tau$，對應到工程 hyper parameter min_split_loss </p><h3 id="如何決定-leaf-node-的-output-value"><a href="#如何決定-leaf-node-的-output-value" class="headerlink" title="如何決定 leaf node  的 output value?"></a>如何決定 leaf node  的 output value?</h3><p>決定每個 leaf node 的 output value 公式跟式(2) similarity score 很像，差別在分子部份的 $\text{sum of residuals}$ 是否取平方</p><script type="math/tex; mode=display">\text{output value} = \cfrac{\sum(\text{residuals})}{\text{Number of Residuals} + \lambda}\tag{3}</script><ul><li>$\lambda$ is a regularization parameter，這邊先假設 0</li></ul><p>計算每個 leaf node output value 後</p><p><img src="https://i.imgur.com/bxhTRce.png"  /></p><h3 id="Regularization-term-lambda-的作用"><a href="#Regularization-term-lambda-的作用" class="headerlink" title="Regularization term $\lambda$ 的作用"></a>Regularization term $\lambda$ 的作用</h3><blockquote><p>prevent overfitting in training data</p></blockquote><p>regularization paramater $\lambda$ 的作用是防止 overfitting，$\lambda$ 可以減少 similarity 對 $resiudal$  值的敏感程度</p><script type="math/tex; mode=display">\text{Similarity Score} =  \cfrac{(\sum \text{residual})^2}{\text{Number of Residuals } + \lambda}</script><p>若 $\lambda &gt; 0$ 值越大則分母越大， similarity score 會越小，代表分子  $\text{sum of residual }$ 的作用越小</p><p>similarity score 越小</p><ol><li>similarity score 越小，在分裂階段，分裂點的 $Gain = Left_{Similarity} + Right_{similarity}  - Root_{similarity}$  就越小<ul><li>$\lambda$ 對不同 node  的影響是非線性的</li></ul></li><li>similarity score 越小 ， gain 也越小，在剪枝階段，其被剪枝的可能性越大。</li></ol><h2 id="如何整合多棵樹-f-1-x-f-2-x-…-f-m-x-的輸出"><a href="#如何整合多棵樹-f-1-x-f-2-x-…-f-m-x-的輸出" class="headerlink" title="如何整合多棵樹 $f_1(x), f_2(x),…,f_m(x)$ 的輸出"></a>如何整合多棵樹 $f_1(x), f_2(x),…,f_m(x)$ 的輸出</h2><p>$F_M(x)$ 的計算與 GBDT 一樣</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^M_{i=1} f_i(x) = F_{M-1} + \nu f_m(x)</script><p><img src="https://i.imgur.com/5bY4jeW.png" style="zoom:50%;" /></p><p>假設只建立一顆 XGB tree $f_1(x)$，  $\text{learning rate = 0.8}$</p><p><img src="https://i.imgur.com/mbxR8em.png"  /></p><p>則每個  data sample 新的 prediction 為</p><script type="math/tex; mode=display">F_1(x) = F_0(x) + 0.8* f_1(x)</script><p><img src="https://i.imgur.com/TUAqWpn.png" style="zoom:50%;" /></p><ul><li>ep_0_predict 表  $F_0(x)$ 對每個 data sample 的預測值， XGBoost 初始預設值是 0.5</li><li>ep_1_leaf_output 表 data sample 在  XGB tree $f_1(x)$ 下所屬  leaf node 的輸出值，其值擬合  ep_1_residual</li><li>ep_1_predict 表 $F_1(x)$ 表 data sample 在  $m=1$ 的預測值</li></ul><h1 id="XGBoost-Regression-Math-Background"><a href="#XGBoost-Regression-Math-Background" class="headerlink" title="XGBoost Regression Math Background"></a>XGBoost Regression Math Background</h1><h3 id="XGBoost-的通用-Objective-Function"><a href="#XGBoost-的通用-Objective-Function" class="headerlink" title="XGBoost 的通用 Objective Function"></a>XGBoost 的通用 Objective Function</h3><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m \end{aligned} \tag{4}</script><ul><li>$g_i$ 為 first derivative of loss function  related to data sample $x_i$ :  $g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, \hat{y_i})$</li><li>$h_i$ 為 second derivative of loss function related to data sample $x_i$:  $h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, \hat{y_i})$</li><li>$T_m$ is the number of leaf in tree $f_m$</li><li>$\tau$ 表對 $T_m$ 的 factor</li><li>$\gamma_{j,m}$ 表 $m$ 步 XGB tree $f_m$  第 $j$ 個 leaf node 的輸出值</li><li>$R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合</li></ul><p>optimal output $\gamma_{j,m}^*$ of leaf node $j$ </p><script type="math/tex; mode=display">\gamma_{j,m}^*=- \cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda}  \tag{5}</script><p>詳細推導參閱 <a href="/XGBoost-General-Objective-Function/" title="透視 XGBoost(3) 蘋果樹下的 objective function">透視 XGBoost(3) 蘋果樹下的 objective function</a></p><h2 id="Regression-Leaf-output-公式怎麼來的？"><a href="#Regression-Leaf-output-公式怎麼來的？" class="headerlink" title="Regression Leaf output 公式怎麼來的？"></a>Regression Leaf output 公式怎麼來的？</h2><blockquote><p>式(3) each leaf node 的 output 公式怎麼得出的？</p></blockquote><script type="math/tex; mode=display">\text{output value} = \cfrac{\sum(\text{residuals})}{\text{Number of Residuals} + \lambda}\tag{3}</script><p>從式 (5)，可以直接求 regression 的 optimal leaf node output </p><p>regression 的 loss 通常是 square error ，先將 $l(y_i, \hat{y_i}) = \cfrac{1}{2}(y_i - \hat{y_i})^2$  代入 $h_i, g_i$</p><script type="math/tex; mode=display">\begin{aligned}g_i &= -(y_i - \hat{y}_i) = \text{negative residual} \\h_i &= \cfrac{d}{d \ \hat{y}_i} -(y_i - \hat{y_i}) = 1\end{aligned}</script><p>代入 式(5) 得到 $\gamma_{j,m}^*$</p><script type="math/tex; mode=display">\gamma_{j,m} = \cfrac{\sum_{i \in R_{j,m}}(y_i - F_{m-1}(x_i))}{(\sum_{i \in R_{j,m}}1) + \lambda} = \cfrac{\text{sum of residual}}{\text{number of residual } + \lambda }</script><ul><li>$i \in R_{j,m}$ 表 leaf node $j$ 下的  data sample $x_i$</li><li>$\lambda$  is a regularization parameter</li></ul><p>故得證 XGB tree $f_m(x)$ for regression each leaf node $j$  的輸出為</p><script type="math/tex; mode=display">\gamma_{j,m}  = \cfrac{\text{sum of residual}}{\text{number of residual } + \lambda }</script><h2 id="Regression-的-Similarity-Score-怎麼來的？"><a href="#Regression-的-Similarity-Score-怎麼來的？" class="headerlink" title="Regression 的 Similarity Score 怎麼來的？"></a>Regression 的 Similarity Score 怎麼來的？</h2><blockquote><p>式 (2) similarity score 怎麼得出的</p></blockquote><script type="math/tex; mode=display">\text{Similarity Score} =  \cfrac{(\sum \text{residual})^2}{\text{Number of Residuals } + \lambda} \tag{2}</script><p>XGBoost  分裂的目的是要使 式(4) objective function 越小越好，怎麼評判分裂前後收益？式(1) 評價了分裂後 left/right leaf node 可以得到的＂score＂與分裂前 root node 的 ＂score＂ 差值計算分裂前後增益 gain</p><script type="math/tex; mode=display">Gain = Left_{\text{similarity}} + Right_{\text{similarity}}  - Root_{\text{similarity}} \tag{1}</script><p>所以 similarity score 肯定得跟 objective  function 有關，才能正確的恆量 gain，但 similarity score 是 ＂score “ 越大越好， objective function 是要 minimize 的，得越小越好，兩者如何扯上關係？</p><p>觀察一下 objective function</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{\tilde{L}}^{(m)}&= \sum^{T_m}_{j=1}[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] + \tau T_m \end{aligned}  \tag{4}</script><p>因為我們要衡量單個 node 得 gain，式 (4)中，跟某個 node 直接相關的 part 為 summation of all leaf node 裡面的 equation </p><script type="math/tex; mode=display">[(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2] \tag{6}</script><ul><li>$R_{m, j}$表 m 步 XGB tree $f_m$ 下 第 $j$ 個 leaf node 得 data sample 集合</li></ul><p>將 式(5) 代入 式(6) 可以得出 式(6)的極小值</p><script type="math/tex; mode=display">\begin{aligned}(\sum_{i \in R_{m,j}}g_i) \gamma_{j,m} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)\gamma_{j,m}^2 &= -（\sum_{i \in R_{j,m}}g_i)\cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda} + \cfrac{1}{2}(\sum_{i \in R_{j,m} }h_i + \lambda)(\cfrac{\sum_{i\in R_{j,m}} g_i}{\sum_{i \in R_{j,m}}h_i + \lambda})^2  \\& =  -\cfrac{(\sum g_i)^2}{\sum h_i + \lambda} + \cfrac{1}{2} \cfrac{(\sum g_i)^2}{\sum h_i + \lambda} \\& = -\cfrac{1}{2}\cfrac{(\sum g_i)^2}{\sum h_i + \lambda} \end{aligned} \tag{7}</script><p>因爲要的是 ＂score＂ ，所以讓 loss 對 x 軸做對稱，拿掉係數項 $\frac{1}{2}$ </p><script type="math/tex; mode=display">\text{Similarity Score} = \cfrac{(\sum g_i)^2}{\sum h_i + \lambda} \tag{8}</script><ul><li><p>$g_i$ 為 first derivative of loss function  related to data sample $x_i$ :  $g_i = \cfrac{d}{d \ F_{m-1}} \ l(y_i, \hat{y_i})$</p></li><li><p>$h_i$ 為 second derivative of loss function related to data sample $x_i$:  $h_i = \cfrac{d^2}{d \ F_{m-1}^2} l(y_i, \hat{y_i})$</p></li><li><p>因爲要的是 ＂score＂ ，所以讓 loss 對 x 軸做對稱，拿掉係數項 $\frac{1}{2}$</p><p><img src="https://i.imgur.com/X79go7n.png" style="zoom:33%;" /></p></li></ul><p>式 (8) 中的 $g_i, h_i$ 分別表 loss function $l(y,\hat{y})$ 的 first derivative and second derivative </p><p>當 loss function 為 square error 時</p><ul><li>$g_i = -(y_i - \hat{y}_i) = \text{negative residual}$</li><li>$h_i = \cfrac{d}{d \ \hat{y}_i} -(y_i - \hat{y_i}) = 1$</li></ul><p>代入式(8) 得到 square error 下的 similarity score，得證：</p><script type="math/tex; mode=display">\text{Similarity Score} =  \cfrac{(\sum g_i)^2}{\sum h_i + \lambda} =  \cfrac{(\sum \text{residual})^2}{\text{Number of Residuals } + \lambda}</script><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 13-17-Augu, 785–794. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a></li><li>What makes “XGBoost” so Extreme? <a href="https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb">https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb</a><ul><li>XGBoost-from-scratch-python</li></ul></li><li>Boosting algorithm: XGBoost <a href="https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d">https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d</a></li><li><a href="https://www.hrwhisper.me/machine-learning-xgboost/#blocks-for-out-of-core-computation">https://www.hrwhisper.me/machine-learning-xgboost/</a></li></ul><ul><li><p><a href="https://www.youtube.com/watch?v=OtD8wVaFm6E">XGBoost Part 1 (of 4): Regression</a></p></li><li><p><a href="https://www.youtube.com/watch?v=ZVFeW798-2I&amp;t=19s">XGBoost Part 3 (of 4): Mathematical Details</a></p></li></ul><ul><li><p>my post</p><ul><li><a href="/GBDT-Rregression-Tree-Step-by-Step/" title="一步步透視 GBDT Regression Tree">一步步透視 GBDT Regression Tree</a></li><li><a href="/GBDT-Classifier-step-by-step/" title="一步步透視 GBDT Classifier">一步步透視 GBDT Classifier</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>內容推薦 (3):主題卡片推薦</title>
      <link href="topic-recommendation/"/>
      <url>topic-recommendation/</url>
      
        <content type="html"><![CDATA[<h1 id="什麼是主題卡片推薦？"><a href="#什麼是主題卡片推薦？" class="headerlink" title="什麼是主題卡片推薦？"></a>什麼是主題卡片推薦？</h1><p>主題推薦，就是給定一個概念，然後推薦系統圍繞著這個主題將將相關聯的商品推薦給用戶。</p><p>“主題” 是個超越類目, 產品詞或標籤的存在，他可以是：</p><ul><li>旅行出行</li><li>文青</li><li>寬鬆顯瘦</li><li>愛車人</li><li>追星族</li><li>愛美</li></ul><p>等於是把商品池中的類目關係解構又重構出一個新的子集。</p><p>以用戶端接觸的 UI 來看，主題卡片推薦由兩個構成： Feed 流卡片推薦與 Landing Page 主題推薦。</p><p><img src="https://i.imgur.com/7NNjttp.png" style="zoom: 33%;" /></p><a id="more"></a><p><a href="https://seed9d.github.io/categories/recommendation-system/">推薦系統相關文章</a></p><h2 id="Feeds-流卡片推薦"><a href="#Feeds-流卡片推薦" class="headerlink" title="Feeds 流卡片推薦"></a>Feeds 流卡片推薦</h2><p>Feeds 流卡片推薦是在推薦流裡面展示一張卡片，這張卡片 summarize 了最今用戶點擊的商品，形成一個主題展示給用戶。</p><p>卡片上會有一張圖片以及主題詞吸引用戶點擊，如上圖的 Tea Set 與其搭配圖。</p><p>卡片產生方式主要有二，兩者可以互補使用</p><ul><li>人工運營</li><li>算法自動化</li></ul><p>人工運營即人工事先的定義好商品池跟搭配語，只要用戶的歷史交互商品達到觸發門檻就推薦給他，人工運營的好處是可以精確的定義某類風格而且很難出現 bad case，EX：日系穿搭，文青風 ….</p><p>算法自動化就得利用商品的 label 詞與 product 詞，產生 商品-詞 pair。</p><p>最立即有效的做法，直接將 product and label phrase 按照其在 title 中的重要性程關聯給商品。當用戶達到主題觸發門檻，就從主題下挑選出一個商品以及其中一個 keyword 做為展示詞形成卡片。</p><ul><li>product &amp; label phrase  挖掘參考  <a href="/recognize-keywords-by-entorpy/" title="內容推薦 (1) 關鍵詞識別">內容推薦 (1) 關鍵詞識別</a> </li><li>product and label phrase 在 title 中的重要性程度可以參考  <a href="/title-embedding-with-keywords/" title="內容推薦 (2) Title Embedding with Keyword">內容推薦 (2) Title Embedding with Keyword</a> </li></ul><h2 id="Landing-Page-主題推薦"><a href="#Landing-Page-主題推薦" class="headerlink" title="Landing Page 主題推薦"></a>Landing Page 主題推薦</h2><p>當用戶點擊卡片後會跳轉到此頁，此頁的推薦必須圍繞著 trigger 卡片的主題詞以及商品展開</p><p><img src="https://i.imgur.com/Q376bri.png" style="zoom: 50%;" /></p><p>此頁可以用的召回有</p><ul><li>主題詞下的其他商品召回</li><li>trigger 商品的 label &amp; product 召回</li><li>trigger 商品的 title embedding 召回</li><li>trigger 商品的 image I2I</li><li>…etc</li></ul><p>基本上只要符合主題的都能用，但得注意過多同類型商品會造成用戶疲勞，所以多樣性還是得考慮。</p><h1 id="實作-Tips"><a href="#實作-Tips" class="headerlink" title="實作 Tips"></a>實作 Tips</h1><h2 id="關於效用"><a href="#關於效用" class="headerlink" title="關於效用"></a>關於效用</h2><p>這就得提到為什麼需要在 feed 流展示卡片</p><p>首頁 feeds 流的定位比較偏向讓用戶去逛，所以很大的考量推薦商品列表中商品間的多樣性程度，因此需要另一個空間來將同質性高的商品聚合展示，讓用戶看中一樣商品後不搜索直接點進去開始挑選比較。</p><p>其二是透過卡片上的文字展示，可以起到導引用戶心智的效果</p><p>EX：用戶可能只想買個被子，被 ＂居家＂主題吸引後，不知不覺其他東西都買了。</p><p>另外，加入卡片推薦之後，原本場景轉化肯定是會掉的，因爲卡片相當於是一個可以點擊但不能購買的 entity，得將 Landing page 內的指標歸因回  feeds 流才行。</p><p>事實上，卡片的加入原本就會傷害 feeds 流某些指標的轉化了，因為你提前讓用戶跳轉到其他頁面，所以對於卡片推薦的效用得從多方面評估。</p><h2 id="儲存方式"><a href="#儲存方式" class="headerlink" title="儲存方式"></a>儲存方式</h2><p>要找出 商品 → 標籤詞/產品詞 的關係，肯定會需要一個正排索引，一個類似這樣的結構：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;trigger_key&quot;</span>: <span class="string">&quot;2734070&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;pairs&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;key&quot;</span>: <span class="string">&quot;產品詞A&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;score&quot;</span>: <span class="number">21.713</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;key&quot;</span>: <span class="string">&quot;產品詞B&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;score&quot;</span>: <span class="number">15.63</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同時如果需要 產品詞/標籤詞 → 商品的對應關係，還會需要一個倒排索引，所以在持久化儲存方面得多調研，DynamoDB 的 GSI 是選項。 </p><h2 id="卡片圖片的選擇"><a href="#卡片圖片的選擇" class="headerlink" title="卡片圖片的選擇"></a>卡片圖片的選擇</h2><p>根據 AB 實驗顯示，如果想增加卡片的 CTR 和轉化，放他點擊過的商品的圖片準沒錯，不過記得跳轉進 Landing page 時得展示卡片圖片上的商品，不然會有種被欺騙的感覺。</p>]]></content>
      
      
      <categories>
          
          <category> recommendation system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation system </tag>
            
            <tag> NLP </tag>
            
            <tag> 推薦系統 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>內容推薦 (2) Title Embedding with Keyword</title>
      <link href="title-embedding-with-keywords/"/>
      <url>title-embedding-with-keywords/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在前篇 <a href="/recognize-keywords-by-entorpy/" title="內容推薦 (1) 關鍵詞識別">內容推薦 (1) 關鍵詞識別</a> 中，我們利用 entropy 從商品池的 title  中辨識出 product word  &amp; label word  </p><p>此篇，我們將利用已經辨識出的 product word &amp; label word 回頭對商品池中的商品 title 做 embedding</p><p>當然你也可以直接將所有 title 送進 Word2Vec 硬 train 一發，然後對 title 內的所有的 word vectors 取平均得到 title vector。</p><a id="more"></a><p><a href="https://seed9d.github.io/categories/recommendation-system/">推薦系統相關文章</a></p><h1 id="Weight-Keyword-Embedding"><a href="#Weight-Keyword-Embedding" class="headerlink" title="Weight Keyword Embedding"></a>Weight Keyword Embedding</h1><p>假設我們有一個 title ，我們希望能根據 word 在 title 中的重要程度將他 embedding 化，要怎麼做？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;summer fisherman hat female outdoor sun hat sun hat japanese student basin hat watch travel fishing sun hat male&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="從-CBOW-說起"><a href="#從-CBOW-說起" class="headerlink" title="從 CBOW 說起"></a>從 CBOW 說起</h2><p>CBOW  的思想是用兩側 context words 去預測中間的 center word</p><script type="math/tex; mode=display">P(center|context;\theta)</script><p>換句話說，給定 context words 集合 $w_{I,C}$， word $w_j$ 是 center word $w_O$ 的 probability 越大 是否代表 $w_j$ 在 context $C$ 中越關鍵？</p><script type="math/tex; mode=display">P(w_O = w_j |w_{I,C};\theta)</script><p>如果上面的推測成立的話，CBOW 在 Hierarchical Softmax 下的  objective function: negative log likelihood  </p><script type="math/tex; mode=display">\begin{aligned} &-\log p(w_O| w_I) = -\log \dfrac{\text{exp}({h^\top \text{v}'_O})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})} \\& = - \sum^{L(w_O)-1}_{l=1}  \log\sigma([ \cdot] h^\top \text{v}^{'}_l)\end{aligned} \tag{1}</script><ul><li>CBOW with Hierarchical Softmax 有兩個 matrix $W$  and $W’$<ul><li>$W$ 的 row vector 對應到 word $w_i$ 的 vector</li><li>$W’$ 對應的是 huffman tree non-leaf node 的 vector</li><li>參見  <a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax">Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax</a></li></ul></li><li>$\text{v}’_j$  表 output side matrix $W’$  中  j-th columns  vector，跟任何 word 沒一對一對應關係</li><li>$L(w_i) -1$ 表 huffman tree 中從 root node  到 leaf node of $w_i$ 的 node number</li><li>$[ \cdot ]$表 huffman tree 的分岔判斷<ul><li>$[ \cdot ] = 1$ 表 turn left</li><li>$[ \cdot ] = -1$ 表 turn right</li></ul></li><li>$h = \frac {1}{C} \sum^{C}_{j=1}\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$</li></ul><p>Score function $\log p(w_O| w_I)$ <strong>(沒負號)，</strong> 本質上是對 output word $w_O$ 的打分。</p><p>先改寫一下 score function，等等會用到</p><script type="math/tex; mode=display">\begin{aligned}\log p(w_O| w_I) &= \sum^{L(w_O)-1}_{l=1}\log\sigma([ \cdot] h^\top  \text{v}^{'}_l)  \\&= \sum^{L(w_O)-1}_{l=1} \log(\cfrac{1}{1+ \exp^{- [ \cdot] h^{\top} v_l^{'}}}) \\&= \sum^{L(w_O)-1}_{l=1} [\log(1) -\log(1+ \exp^{- [ \cdot] h^{\top} v_l^{'}}) ]\\& = \sum^{L(w_O)-1}_{l=1}-\log(1 + \exp^{- [ \cdot] h^{\top} v_l^{'}}) \end{aligned} \tag{2}</script><p>有了 式(2) score function ，給定一 title words 集合 $w_{T}$ 只要對 title 裡的每個 word  $w_j \in w_{T}$  ，令 $\log p(w_O=w_j|w_I = w_{T, \lnot j})$，進行打分即可得到每個  word $w_j$ 在 title 裡的重要程度 </p><script type="math/tex; mode=display">\text{weight}_j = \log p(w_O=w_j|w_I = w_{T, \lnot j}) \tag{3}</script><p>而我們要的 title embedding 即 weighted sum of words in title</p><script type="math/tex; mode=display">\text{v}_{\text{title}} = \sum_{w_j \in w_T}\text{weight}_j \times \text{v}_{w_{j}} \tag{4}</script><ul><li>$w_T$: 某 title 的 word 集合</li><li>$\text{v}_{w_{j}}$: word  $w_j$ 對應 matrix $W$ 中 row vector</li></ul><h2 id="Gensim-實作"><a href="#Gensim-實作" class="headerlink" title="Gensim 實作"></a>Gensim 實作</h2><h3 id="Train-CBOW-HS"><a href="#Train-CBOW-HS" class="headerlink" title="Train CBOW + HS"></a>Train CBOW + HS</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line">w2v_model = Word2Vec(</span><br><span class="line">        min_count=<span class="number">3</span>,</span><br><span class="line">        window=<span class="number">5</span>,</span><br><span class="line">        size=<span class="number">100</span>,</span><br><span class="line">        alpha=<span class="number">0.005</span>,</span><br><span class="line">        min_alpha=<span class="number">0.0007</span>,</span><br><span class="line">        hs=<span class="number">1</span>,</span><br><span class="line">        sg=<span class="number">0</span>,</span><br><span class="line">        workers=<span class="number">4</span>,</span><br><span class="line">        batch_words=<span class="number">100</span>,</span><br><span class="line">        cbow_mean = <span class="number">1</span></span><br><span class="line">    )</span><br><span class="line">w2v_model.build_vocab(corpus) <span class="comment"># build huffman tree</span></span><br><span class="line">w2v_model.train(</span><br><span class="line">        corpus,</span><br><span class="line">        total_examples=w2v_model.corpus_count,</span><br><span class="line">        epochs=<span class="number">50</span>,</span><br><span class="line">        report_delay=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>corpus 裡的每個 title，應該已經先行合併 bigram and trigram 的 product &amp; label 詞 ，最好可以去除無用詞，如下 sentence，某些 words 已合併 :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;hyuna_style ins cute wild sunscreen female hand sleeves arm_guard ice_silk sleeves driving anti_ultraviolet ice gloves tide&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="Scoring-Words-in-Title"><a href="#Scoring-Words-in-Title" class="headerlink" title="Scoring Words in Title"></a>Scoring Words in Title</h3><p>訓練完 model  後，對每個 title 內的 words 重要性進行評分:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_log_probs</span>(<span class="params">model, target_w, context_embd: np.ndarray</span>)-&gt; np.ndarray:</span></span><br><span class="line">    turns = (-<span class="number">1.0</span>) ** target_w.code</span><br><span class="line">    path_embd = model.trainables.syn1[target_w.point]</span><br><span class="line">    log_probs = -np.logaddexp(<span class="number">0</span>, -turns * np.dot(context_embd, path_embd.T))</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(log_probs)</span><br></pre></td></tr></table></figure><ul><li>為 式 (2)  的實現，實際上就是 gensim Word2Vec 內的  score_cbow_pair</li><li>word 在 huffman tree  的 path code 是 0/1 code，使用時須轉換成 -1 or 1</li><li>model.trainables.syn1 即 $W’$ ，存放  huffman tree non-leaf node 的 vector</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cal_keyword_score</span>(<span class="params">model, sentence:List[<span class="built_in">str</span>]</span>) -&gt; Dict[str, float]:</span></span><br><span class="line">    word_vocabs = [model.wv.vocab[w] <span class="keyword">for</span> w <span class="keyword">in</span> sentence <span class="keyword">if</span> w <span class="keyword">in</span> model.wv.vocab]</span><br><span class="line">    </span><br><span class="line">    word_importance = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> pos_center, center_w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_vocabs):</span><br><span class="line">        context_w_indices = [w.index <span class="keyword">for</span> pos_w, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_vocabs) <span class="keyword">if</span> pos_center != pos_w]</span><br><span class="line">        context_embed = np.mean(model.wv.vectors[context_w_indices], axis=<span class="number">0</span>)</span><br><span class="line">        log_probs = cal_log_probs(model, center_w, context_embed)</span><br><span class="line">        </span><br><span class="line">        center_w_term = w2v_model.wv.index2word[center_w.index]</span><br><span class="line">        word_importance[center_w_term] = word_importance.get(center_w_term, <span class="number">0</span>) + log_probs</span><br><span class="line">    <span class="keyword">return</span> word_importance</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_keyword_score</span>(<span class="params">model, sentence: List[<span class="built_in">str</span>]</span>) -&gt; np.ndarray:</span></span><br><span class="line">    word_importance = _cal_keyword_score(model, sentence)</span><br><span class="line">    ds = pd.Series(word_importance).sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    scalar = MinMaxScaler(feature_range=(<span class="number">0.1</span>, <span class="number">1</span>))</span><br><span class="line">    array = ds.to_numpy()</span><br><span class="line">    array = scalar.fit_transform(array.reshape(array.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">    ds = pd.Series(array.reshape(-<span class="number">1</span>, ), index=ds.index)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><ul><li>model.wv.vectors 存放 $W$，即訓練完後每個  word 的 vector</li><li>MinMaxScaler: 縮放到 0.1  到  1 是為了方便觀察</li></ul><p>使用方式如下</p><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sent = corpus_with_bigram_trigram[<span class="number">7676</span>]</span><br><span class="line">ds = cal_keyword_score(w2v_model, sent)</span><br><span class="line">print(sent), print(ds)</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;haining&#x27;</span>, <span class="string">&#x27;leather&#x27;</span>, <span class="string">&#x27;male&#x27;</span>, <span class="string">&#x27;stand_collar&#x27;</span>, <span class="string">&#x27;middle_aged&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>, <span class="string">&#x27;fur&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;winter&#x27;</span>, <span class="string">&#x27;cashmere&#x27;</span>, <span class="string">&#x27;thick&#x27;</span>, <span class="string">&#x27;money&#x27;</span>, <span class="string">&#x27;father_loaded&#x27;</span>]</span><br><span class="line">coat             <span class="number">1.000000</span></span><br><span class="line">leather          <span class="number">0.874738</span></span><br><span class="line">fur              <span class="number">0.861750</span></span><br><span class="line">middle_aged      <span class="number">0.812752</span></span><br><span class="line">winter           <span class="number">0.773609</span></span><br><span class="line">male             <span class="number">0.734654</span></span><br><span class="line">stand_collar     <span class="number">0.699505</span></span><br><span class="line">thick            <span class="number">0.676800</span></span><br><span class="line">cashmere         <span class="number">0.642869</span></span><br><span class="line">one              <span class="number">0.546631</span></span><br><span class="line">haining          <span class="number">0.457806</span></span><br><span class="line">father_loaded    <span class="number">0.393533</span></span><br><span class="line">money            <span class="number">0.100000</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><h3 id="Weighted-Sum-of-Word-Vectors"><a href="#Weighted-Sum-of-Word-Vectors" class="headerlink" title="Weighted Sum of Word Vectors"></a>Weighted Sum of Word Vectors</h3><p>從 w2v_model 中取出某 title 內所有 words 的 vector 做 weighed sum</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted_sum_w2v</span>(<span class="params">w2v_model, ds: pd.Series</span>) -&gt; np.ndarray:</span></span><br><span class="line">    ds_  = ds.copy() / <span class="built_in">sum</span>(ds)</span><br><span class="line">    w2v = w2v_model.wv[ds_.index]</span><br><span class="line">    weights = np.expand_dims(ds_.to_numpy(), <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>((w2v * weights), axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><ul><li>得出的 title vector is un-normalized ，要使用前得先  L2-norm</li></ul><p>參閱 bible 做的示例 notebook <a href="https://github.com/seed9D/hands-on-machine-learning/blob/main/Embedding/embeddng_with_keyword_CBOW_HS.ipynb">seed9D/hands-on-machine-learning</a></p><h1 id="Title-Embedding-應用"><a href="#Title-Embedding-應用" class="headerlink" title="Title Embedding 應用"></a>Title Embedding 應用</h1><p>title embedding 最直覺的應用是 content I2I，用戶點擊了 商品 A，我們就可以透過 商品 A 的 title vector 召回 TopK 個最相似 title 的商品推薦給他。</p><p>而 title embedding with keyword weighting 中，我們將 product word 與 label word 在 title 中的重要程度進行 weighted sum，能更準確的表達 title 的意思，不再只是簡單的對 word vector 取平均，連一些無用詞的 vector 也混進去。</p><p>不過 title embedding 在推薦的效果不如利用用戶交互數據訓練出來的 embedding，但因爲每個商品一定會有 title， 很適合作為商品冷啟動召回策略之一使用。在我負責的推薦應用裡，也是利用 title embedding 關聯新商品到有交互數據的舊商品上後讓新商品取得曝光機會。</p><p>title embedding 結合 label 詞 &amp; product 詞的另一個業務應用就是卡片式的主題推薦，類似淘寶上的一個頁面就講一個購物主題，選定一個主題 (ex: 旅遊)與某樣你曾經互動過商品進行推薦</p><p><img src="https://i.imgur.com/EXzpEKq.jpg" alt="" style="zoom:33%;" /></p><p>這個算法側的實作不難，留到下次說吧</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>【不可思议的Word2Vec】 3.提取关键词 <a href="https://spaces.ac.cn/archives/4316">https://spaces.ac.cn/archives/4316</a><ul><li>以 skipgram  角度計算</li></ul></li><li>my post<ul><li><a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax">Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax</a></li><li><a href="/hierarchical-softmax-in-word2vec/" title="Word2Vec (2):Hierarchical Softmax 背後的數學">Word2Vec (2):Hierarchical Softmax 背後的數學</a></li></ul></li><li>gensim<ul><li><a href="https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html">https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html</a></li><li>Gensim Word2Vec Tutorial [<a href="https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial">https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> recommendation system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation system </tag>
            
            <tag> NLP </tag>
            
            <tag> 推薦系統 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>內容推薦 (1) 關鍵詞識別</title>
      <link href="recognize-keywords-by-entorpy/"/>
      <url>recognize-keywords-by-entorpy/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="從內容召回說起"><a href="#從內容召回說起" class="headerlink" title="從內容召回說起"></a>從內容召回說起</h2><p>電商推薦系統多路召回中，通常會有一路召回商品叫內容召回 $\text{content I2I}$</p><p>content I2I 與其他根據用戶行為的召回不同， 最簡單的 content I2I 可以根據 A 商品 title 內容找出其他與 A 相似的 title 的商品，這給了他很強的推薦解釋性</p><p>對於商品冷啟動或者系統冷啟動來說，content I2I 可以在沒有用戶交互訊息時，就可以進行推薦，也不失為一種冷啟動方案。</p><p>在萬物皆可 Embedding 的今天，content I2I 只要把所有商品的 title 送進 word2Vec  硬 train 一發也就完事了</p><p>當然要是這麼簡單，也就不會有這篇了</p><p><img src="https://i.imgur.com/hz7T95F.png" alt=""></p><a id="more"></a><p><a href="https://seed9d.github.io/categories/recommendation-system/">推薦系統相關文章</a></p><h2 id="一言難盡的商品-title"><a href="#一言難盡的商品-title" class="headerlink" title="一言難盡的商品 title"></a>一言難盡的商品 title</h2><p>我司的電商商品 title 為中翻英淘寶商品 title 而來，基本上毫無文法可言</p><p><img src="https://i.imgur.com/WvJYlRV.png" alt=""></p><p>如果用 word2Vec  硬做一發，再以 doc2vector 的思路融合成 sentence vector ，肯定會加入某些糟糕詞彙的 vector。</p><p>諸如此類的怪異詞彙：</p><ul><li>EX: “real time” (應該是實時發貨？) , liu haichang(劉海夾??), two yuan(兩元？), yiwu(義烏？)</li></ul><p>為了讓 vector 能更好的表達句子 title，加上組內對於商品關鍵字有需求，於是就有了以下商品 title  挖掘出中 產品詞 與 標籤詞的識別任務 </p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="關鍵詞識別"><a href="#關鍵詞識別" class="headerlink" title="關鍵詞識別"></a>關鍵詞識別</h2><p>先解釋一下，什麼是產品詞，什麼是標籤詞</p><p>以下是我自己的定義：</p><p><img src="https://i.imgur.com/9ZV7aF5.png" alt="商品 title"></p><p>所有商品都會有自己的 tilte，但肯定會有一個 “產品詞” 去描述這商品到底是賣什麼，他可以是單詞，稱為 unigram，也可以複合詞，bigram or trigram……</p><ul><li>unigram:shirt , blouse</li><li>bigram: apron dress, bermuda shorts</li><li>trigram: buckle strap shoes, denim mini dress</li></ul><p>“ 標籤詞 “ (label words)，定義比較空泛， 狹義一點指那些可以用來形容商品或能凸顯商品特色的詞</p><ul><li>unigram: denim, hipster</li><li>bigram: chinese tunic, cotton padded</li><li>trigram: deep v collar</li></ul><p>廣義一點，也可以包含產品詞，最終還得看業務需求，標籤詞他還能在細分出 ＂屬性詞＂(propery)</p><ul><li>領口：高領， 低領，V 領，深 V ..</li><li>材質：棉，麻 …</li></ul><p>不管怎樣，如果沒有人工去蒐集出詞彙，那就得靠機器自己挖掘詞彙字典。</p><p>問題來了，在我們的商品池中的商品 title，基本上沒什文法可言，詞彙也是中翻英出來的。</p><p>如何找出有意義的詞彙組成的 ＂產品詞＂或＂標籤詞＂就是問題的核心了</p><p>EX:<br>＂long＂,＂sleeved＂ 分別看沒什意義，但合起來變成 bigram words： ＂long sleeved” 就有意義</p><p>那要怎麼衡量 words 跟 words 之間的組合程度呢？</p><p>就是 <strong>熵 entropy了</strong></p><h2 id="Entropy-識別關鍵字"><a href="#Entropy-識別關鍵字" class="headerlink" title="Entropy 識別關鍵字"></a>Entropy 識別關鍵字</h2><h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><p>在 information theory 中 ，entropy 被用來衡量系統內的不確定性程度</p><script type="math/tex; mode=display">H(X) = -\sum_x p(x)\log p(x)</script><p>＂不確定性＂(uncertainty) 跟 information 豐富程度是一體兩面的</p><p>entropy 越高，代表不確定性越高，代表能提供的 information 也更多</p><p>舉例來說，如果明天會下雨的 probability 為 0.5，天氣系統的 entropy (以 2 為底)就是</p><script type="math/tex; mode=display">-\cfrac{1}{2} \log_2\cfrac{1}{2} - -\cfrac{1}{2} \log_2\cfrac{1}{2} = 1</script><p>如果明天會下雨的 probability 為 1，那天氣系統的 entropy 就是</p><script type="math/tex; mode=display">-1log_21 = 0</script><p>代表天氣系統完全沒有不確定性，明天肯定會下雨，對我們無法提供任何 information</p><p>我們可以利用 entropy 來衡量 :</p><ul><li>內聚力：word  跟 word 之間的連結緊密程度，由互消息衡量之</li><li>豐富度：words 本身的自由運用程度，由 left entropy / right entropy 衡量之</li></ul><h3 id="互消息-MI-Mutual-Information"><a href="#互消息-MI-Mutual-Information" class="headerlink" title="互消息 (MI - Mutual Information)"></a>互消息 (MI - Mutual Information)</h3><p>先上公式</p><script type="math/tex; mode=display">I(X,Y) =\sum_{y \in Y}\sum_{x \in X} p(x,y) \log(\cfrac{p(x,y)}{p(x) p(y)})</script><p>再上圖</p><p><img src="https://i.imgur.com/q58vjPv.png" alt=""></p><p>看圖就可以直觀明白，$I(X,Y)$ 可以用來衡量兩個事件彼此的關聯性，直觀上互消息可以用來衡量兩個 word 之間的依賴程度。</p><p>$\text{PMI}$ (point-wise mutual information) 也可以用來來衡量兩個 word  的相關性，他被視為簡化版的 $\text{MI}$</p><script type="math/tex; mode=display">PMI(x,y) = \log \cfrac{p(x,y)}{p(x)p(y)}</script><p>word A  跟 word B 的 PMI(A， B) 或 MI(A，B）value  越高，代表 A, B  越相互依賴，組成一個 term 的可能性越越大</p><p>但從公式上不難看出，MI 是 weighted  過後的 PMI。在實務上，PMI 傾向給 “those word only occur together” 組成的 bigram 較高的分數 ; 而 MI 傾向給 high frequency bigram 更高的分數</p><p>EX:<br>在商品 title 中有個詞 ＂small fresh＂</p><ul><li>joint probability $\text{p(“small”, “fresh”)} = 0.002$</li><li>$\text{p(“small”)} = 0.0058$ $\text{p(“fresh”)}= 0.0035$</li><li>$\text{PMI(“small”, “fresh”)} = 4.59$</li><li>$\text{MI(“small”, “fresh”)} =0.009$</li></ul><p>有另一個詞 ＂fresh loos＂</p><ul><li>join probability $\text{p(“fresh loose”)} = 0.0001$</li><li>$\text{p(“fresh”) =0.0035}$  $\text{p(“loose”)} = 0.0024$</li><li>$\text{PMI(“fresh”, “loose”)} = 2.47$</li><li>$\text{MI(“fresh”, “loose”)} =0.00024$</li></ul><p>顯然，對於 fresh 這個 word 而言，＂small fresh＂比 ＂fresh loose＂成詞程度較高。</p><h3 id="左右熵"><a href="#左右熵" class="headerlink" title="左右熵"></a>左右熵</h3><p>左右熵代表了 word 本身可以自由運用的程度</p><p>我們知道，一個 word A 可以跟左邊的 word L，也可以跟右邊的 word R 組合，而左右熵就是來衡量 word A 組成 phase 的豐富程度</p><script type="math/tex; mode=display">H_L(W) = -\sum_{l \in L}p(\text{l::w | w}) \ \log_2 p(\text{l::w| w}) \\H_R(W) = -\sum_{r \in R}p(\text{r::w | w}) \ \log_2 p(\text{r::w| w})</script><p>EX：</p><p>假設 ＂skirt ＂ 這個產品詞在池子中的鄰字組合計數如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_information</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> - x * math.log(x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_entropy</span>(<span class="params">freqDict</span>):</span></span><br><span class="line">    total_count = <span class="built_in">sum</span>(<span class="built_in">list</span>(freqDict.values()))</span><br><span class="line">    informations = [cal_information(fre / total_count) <span class="keyword">for</span> fre <span class="keyword">in</span> freqDict.values()]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(informations)</span><br><span class="line"></span><br><span class="line">skirt_left = &#123;</span><br><span class="line">    <span class="string">&quot;long skirt&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;midi skirt&quot;</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">&quot;pegged skirt&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;pleated skirt&quot;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&quot;prairie skirt&quot;</span>: <span class="number">20</span></span><br><span class="line">    <span class="string">&quot;printed skirt&quot;</span>: <span class="number">400</span>,</span><br><span class="line">    <span class="string">&quot;sarong skirt&quot;</span>: <span class="number">80</span>,</span><br><span class="line">    <span class="string">&quot;trumpet skirt&quot;</span>: <span class="number">600</span>       </span><br><span class="line">&#125;</span><br><span class="line">skirt_right = &#123;</span><br><span class="line">    <span class="string">&quot;skirt suit&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="string">&quot;skirt dress&quot;</span>: <span class="number">1000</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>則 skirt 的 left entropy 為</p><script type="math/tex; mode=display">E_L(\text{"skirt"}) = 1.82</script><p>right entropy為</p><script type="math/tex; mode=display">E_R(\text{"skirt"}) =0.63</script><p>顯然對 “skirt” 而言，左側語境比右側豐富</p><h3 id="Normalize-Entropy-amp-PMI-amp-MI"><a href="#Normalize-Entropy-amp-PMI-amp-MI" class="headerlink" title="Normalize Entropy &amp; PMI &amp; MI"></a>Normalize Entropy &amp; PMI &amp; MI</h3><p>PMI, MI 與  entropy 的值域是個相對 unbound 的值，造成在使用時比較難拿捏 threshold，得來回比對數值決定成詞標準，解決方法是 normalize 值到固定範圍內:</p><ul><li>Normalizing PMI  into (1, -1)</li></ul><script type="math/tex; mode=display">\text{PMI}_n(x, y) = (\ln\cfrac{p(x,y)}{p(x)p(y)}) / -\ln p(x,y)</script><ul><li>Normalizing MI into (0, 1)</li></ul><script type="math/tex; mode=display">\text{MI}*n(X,Y) = \cfrac{\sum*{x,y} p(x,y) \ln \frac{p(x,y)}{p(x) p(y)}}{-\sum_{x,y}p(x,y)\ln p(x,y)}</script><ul><li>Normalizing entropy into (0, 1)</li></ul><script type="math/tex; mode=display">H_n(X) = -\sum_x \frac{p(x) \log p(x)}{\log n}</script><p>論文研究顯示 Normalized MI &amp; PMI  為對角線趨勢，但依然會有一定失真，所以在使用上得自行拿捏<img src="https://i.imgur.com/dTFYXMh.png" alt="Normalized (Pointwise) Mutual Information in Collocation Extraction" style="zoom:50%;" /></p><p>一般來說</p><ul><li>MI 偏向 high frequency，NMI 會稍微將高頻詞 push down ，低頻詞 pull up</li><li>PMI 偏向 low frequency，NPMI 稍微降低低頻詞的 rank</li></ul><h2 id="Score-成詞分數"><a href="#Score-成詞分數" class="headerlink" title="Score 成詞分數"></a>Score 成詞分數</h2><p>有了度量語境豐富度跟詞彙內聚力的工具後，得進一步定出一個 score 代表＂成詞程度＂，score  越高，代表這個詞成為有意義詞的可能性相對較高。</p><p>先定義一個 candidate phrase 的抽象表達，方便我們計算其成詞 score。</p><p>我們的 Candidate phrase  可以是以下這些組合：</p><ul><li>unigram candidate (special case)<ul><li>[unigram] ,    ex: [skirt]</li></ul></li><li>bigram candidate<ul><li>[unigram] :: [unigram], ex: [long :: skirt]</li></ul></li><li>trigram candidate<ul><li>[unigram] :: [bigram], ex: [casual] :: [long skirt]</li><li>[bigram] :: [unigram] ex: [flower printed]  :: [shirt]</li></ul></li></ul><p>拆分成  [Left] :: [Right] 的形式方便我們泛化處理 candidate phrase</p><p>接下來利用定義好的 candidate phrase 來計算成詞 score</p><p><img src="https://i.imgur.com/YJQTaFk.png" alt="product_label_word"></p><p>這裡給出一個最簡單的 score 計算：</p><script type="math/tex; mode=display">\text{score} = \text{(PMI or MI)} - \min(Right_{\text{left_entropy}}, Left_{\text{right_entropy}}) + \min(\text{right_entropy}, \text{left_entropy})</script><ul><li>$\min(Right_{\text{left_entropy}}, Left_{\text{right_entropy}})$  分別表示，Left side 與 Right  side 各自的語境豐富程度，通常取  min 後的的值越大，代表 Left side 或 Right side 有一側傾向與其他詞結合，candidate 越不可能成詞</li><li>$\min(\text{right_entropy}, \text{left_entropy})$ 表示 candidate 左右兩側語境豐富成度，越大代表 candidate 越可能成詞</li></ul><h2 id="Label-Score-amp-Product-Score"><a href="#Label-Score-amp-Product-Score" class="headerlink" title="Label Score &amp;  Product Score"></a>Label Score &amp;  Product Score</h2><p>有了以上的 background 是時候來說說產品詞跟標籤詞的特性了，大致上</p><ul><li>產品詞在 candidate 會出現在 right side，其左側自由度較高:  left_entropy &gt;  right_entropy</li></ul><p><img src="https://i.imgur.com/HsbGKYa.png" alt="lace blouse" style="zoom: 50%;" /></p><ul><li>標籤詞在 candidate 會出現在 left side，其右側自由度較高: right_entropy &gt; left_entropy</li></ul><p><img src="https://i.imgur.com/QIKT45W.png" alt="short sleeve" style="zoom: 67%;" /></p><p>顯然只有成詞分數 score 不足以將產品詞和標籤詞分離，所以每個 candidate phrase，會針對 label 跟 product 特性計算 label score 跟 product score。</p><p>先上圖：<img src="https://i.imgur.com/vXEteao.png" alt="product_label_recong"></p><ul><li>Right Phrase，表  corpus 內出現在 candidate  right side 的 phrase 集合<ul><li>EX : short sleeve  right side unigram 集合</li></ul></li></ul><p><img src="https://i.imgur.com/FaMp6a8.png" style="zoom: 67%;" /></p><ul><li>Light Phrase，表  corpus 內出現在 candidate  left side 的 phrase 集合</li></ul><p>計算 Right Phrases 集合內每一個 phase 對 candidate phrase 的 sum of  information，代表從所有 right phrases 的角度來看 candidate phrase 的豐富度，值越高代表  candidate 的 right phrases 組成越豐富，其成為 label 的機會越高。</p><p><img src="https://i.imgur.com/gNO1xCI.png" alt="cal_information"></p><p>我們希望單個 $\text{phrase}_i$ 對 candidate phrase $C$ 的 conditional probability $p(\text{phrase}_i::C|\text{phrase}_i)$ 不要太高也不要太低，此時算出的  $\text{information} = - p(\text{phrase}_i::C|\text{phrase}_i) \log p(\text{phrase}_i::C|\text{phrase}_i)$ 恰好是最大</p><script type="math/tex; mode=display">\begin{aligned} I_{L,C} &= \sum_{phase_i \in C_{L}}  - p(\text{phrase}_i::C|\text{phrase}_i) \log p(\text{phrase}\_i::C|\text{phrase}_i)\\ I_{R,C} &= \sum_{phrase_j \in C*{R}}  - p(C::\text{phrase}_j|\text{phrase}_j) \log p(C::\text{phrase}_j|\text{phrase}_j) \end{aligned}</script><ul><li>$I_{L,C}$ 表 left phrases 對 candidate 的 sum of information</li><li>$C_L$ 表 candidate  的 left phrase 集合</li></ul><p>有了 left / right phrases information，一個簡單的  label score and product score 計算如下</p><script type="math/tex; mode=display">\begin{aligned} \text{label score} &= (\text{right_entropy - left_entropy}) + (I_{R,C} - I_{L,C})  \\ \text{product score} & = (\text{left_entropy - right_entropy}) + (I_{L,C} - I_{R,C})\end{aligned}</script><p>P.S. 上面分數計算只是提供一個計算思路，實際使用還是得資料分析</p><h1 id="Engineering"><a href="#Engineering" class="headerlink" title="Engineering"></a>Engineering</h1><h2 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h2><p>candidate phrase  的需要計算的值有</p><ul><li>candidate 的 left entropy and right entropy</li><li>candidate 的 PMI/MI 中的  joint probability / frequency</li><li>left component 的 entropy ; right component 的 entropy</li><li>right phrase information and left phrase information</li><li>… etc</li></ul><p>為了方便計算 candidate 需要兩種 Tries ，一個存 corpus 內所有 sentence 的 prefix tries，另一個存 corpus 內所有 reversed title 的  reversed tries (叫 suffix  tries 怕有歧義)</p><p>以 <code>title = &quot;Masks Scarf Cashmere Sweater Cap&quot;</code> 為例</p><p>首先將 title 所有可能 ngram 取出:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Masks&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Scarf&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cashmere&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Sweater&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;Cap&#x27;</span>]</span><br></pre></td></tr></table></figure><p>build prefix tries:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>build reversed tries:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;Cap&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cap&#x27;</span>, <span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Sweater&#x27;</span>, <span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Cashmere&#x27;</span>, <span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Scarf&#x27;</span>, <span class="string">&#x27;Masks&#x27;</span>],</span><br><span class="line"> [<span class="string">&#x27;Masks&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>當我們的 <code>candidate phrase = [&quot;Cashmere&quot; :: &quot;Sweater&quot;]</code> 時，</p><p>透過 prefix tries 即可找出 <code>[&quot;Cashmere&quot; :: &quot;Sweater&quot;]</code> 右側所有的 phrase node</p><p>透過 reversed tries 即可找出 <code>[&quot;Cashmere&quot; :: &quot;Sweater&quot;]</code> 左側所有的 phrase node</p><h2 id="啟發式辨識流程"><a href="#啟發式辨識流程" class="headerlink" title="啟發式辨識流程"></a>啟發式辨識流程</h2><p><img src="https://i.imgur.com/vstiDmj.png" alt=""></p><p>用 entropy 辨識產品詞與標籤詞本質上是 unsupervised learning 的做法，如果以 threshold 卡 score, label score, product score 判斷結果，肯定事倍工半。辨識過程中加入 clustering/grouping 輔助判斷，多迭代幾遍後就能搜集到高度置信的結果。</p><h3 id="Grouping"><a href="#Grouping" class="headerlink" title="Grouping"></a>Grouping</h3><p>把 candidate phrase 當成 data sample $x$ 的話，其包含的特徵有</p><ul><li>自身統計類：frequency ，probability … etc</li><li>自身 entropy related PMI/MI，NPMI/NMI，left entropy / right entropy</li><li>left / right component related: PMI/MI ，entropy to left/entropy to right</li><li>left phrase/right phrase related: sum of information ，deviation，diversity，total frequency，average frequency，total phrase …etc</li><li>score  類：成詞 score ，label score，product score</li></ul><p>挑出 data samples 裡有鑑別度的特徵丟進  cluster 算法中初步分成四群：</p><ul><li>group A: 獨立成詞<ul><li>一些用法固定的詞彙</li><li>ex : “big code”(這應該是想表示大碼？)，”united state”</li></ul></li><li>group B: label 詞<ul><li>符合 label 詞的特性，右側自由度高</li><li>ex: “short sleeved”</li></ul></li><li>group C: 右側 product 詞<ul><li>符合 product 詞的特性，左側自由度高</li><li>ex: “lace blouse”</li></ul></li><li>group D: 無用詞<ul><li>沒什意義的詞，本身成詞 score 不高</li><li>ex: “lace long”</li></ul></li></ul><p>然後在每個  group  中，分別挑出多個  high confidence and typical data samples ，跟其餘的 data sample 做 KNN/Kmean，來回個幾次做 semi-supervised  。</p><h3 id="Exploration"><a href="#Exploration" class="headerlink" title="Exploration"></a>Exploration</h3><p>隨著置信的 data sample 越多，可以考慮訓練 decision tree，辨識新的 candidate phrases。</p><p>也可以利用  word2Vec 強大的相近詞搜索相似的 產品詞/標籤詞 挖掘辨識新的產品詞/標籤詞</p><p>這兩個做法建立在手頭上的詞彙已能很好區分出產品詞和標籤詞的情況下，例如用 word2Vec 找相近產品詞有奇效：</p><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w2v_model.wv.most_similar(<span class="string">&quot;flight_jacket&quot;</span>, topn=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;bomber_jacket&#x27;</span>, <span class="number">0.8251528739929199</span>),</span><br><span class="line"> (<span class="string">&#x27;flight_suit&#x27;</span>, <span class="number">0.8104218244552612</span>),</span><br><span class="line"> (<span class="string">&#x27;coach_jacket&#x27;</span>, <span class="number">0.7058290243148804</span>),</span><br><span class="line"> (<span class="string">&#x27;workwear_jacket&#x27;</span>, <span class="number">0.7037896513938904</span>),</span><br><span class="line"> (<span class="string">&#x27;jacket&#x27;</span>, <span class="number">0.7035773992538452</span>),</span><br><span class="line"> (<span class="string">&#x27;ma_1&#x27;</span>, <span class="number">0.6985215544700623</span>),</span><br><span class="line"> (<span class="string">&#x27;baseball_uniform&#x27;</span>, <span class="number">0.6333736181259155</span>),</span><br><span class="line"> (<span class="string">&#x27;ma1_pilot&#x27;</span>, <span class="number">0.6201080679893494</span>),</span><br><span class="line"> (<span class="string">&#x27;jackets&#x27;</span>, <span class="number">0.608674168586731</span>),</span><br><span class="line"> (<span class="string">&#x27;denim_jacket&#x27;</span>, <span class="number">0.6048851013183594</span>)]</span><br></pre></td></tr></table></figure><h2 id="Some-Tips"><a href="#Some-Tips" class="headerlink" title="Some Tips"></a>Some Tips</h2><ol><li>candidate phrase proposal：可以透過 TF-IDF, frequency, student-t, PMI 先行召回一批 candidate 再開始辨識</li><li>一次處理一種 ngram</li><li>辨識過程中加入字典輔助<ul><li>product words 黑白字典</li><li>label words 黑白字典</li></ul></li><li>善用 bigram / trigram 可以由其他 phrase 組合出，可以省去很多計算量<ul><li>產品詞組成<ul><li>unigram 產品詞<ul><li>[unigram], ex： skirt</li></ul></li><li>bigram 產品詞<ul><li>[unigram label] :: [unigram product] , ex: long skirt</li><li>[unigram] :: [unigram],  ex: phone shell (手機殼 …)</li></ul></li><li>trigram 產品詞<ul><li>[bigram label] :: [unigram product], ex: long sleeved blouse</li><li>[unigram label] :: [bigram product], ex: little black dress</li></ul></li></ul></li><li>標籤詞組成<ul><li>unigram 標籤詞<ul><li>[unigram],  ex:  slim</li></ul></li><li>bigram 標籤詞<ul><li>[unigram] :: [unigram], ex: v neck, high cut</li></ul></li><li>trigram 標籤詞<ul><li>[unigram label] :: [bigram label], ex: half high collar</li><li>[bigram label] :: [unigram], ex: deep v collar</li></ul></li></ul></li></ul></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>data mining basing on entropy<ul><li>Language Models – handling unseen sequences &amp; Information Theory<br><a href="https://www3.cs.stonybrook.edu/~ychoi/cse628/lecture/03-ngram.pdf">https://www3.cs.stonybrook.edu/~ychoi/cse628/lecture/03-ngram.pdf</a></li><li>反作弊基于左右信息熵和互信息的新词挖掘 <a href="https://zhuanlan.zhihu.com/p/25499358">https://zhuanlan.zhihu.com/p/25499358</a></li><li>基于互信息和左右信息熵的短语提取识别 <a href="http://www.hankcs.com/nlp/extraction-and-identification-of-mutual-information-about-the-phrase-based-on-information-entropy.html">http://www.hankcs.com/nlp/extraction-and-identification-of-mutual-information-about-the-phrase-based-on-information-entropy.html</a></li></ul></li><li>Normalization entropy<ul><li>Efficiency (normalized entropy) <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory">https://en.wikipedia.org/wiki/Entropy_(information_theory)#Efficiency_(normalized_entropy)</a>#Efficiency_(normalized_entropy))</li><li>Bouma, G. (2009). Normalized ( Pointwise ) Mutual Information in Collocation Extraction. Proceedings of German Society for Computational Linguistics (GSCL 2009), 31–40.</li><li>[<a href="https://math.stackexchange.com/questions/395121/how-entropy-scales-with-sample-size">https://math.stackexchange.com/questions/395121/how-entropy-scales-with-sample-size</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> recommendation system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation system </tag>
            
            <tag> NLP </tag>
            
            <tag> 推薦系統 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thompson Sampling 推薦系統中簡單實用的 Exploring Strategy</title>
      <link href="Implement-Thompson-Sampling-in-Recommendation-System/"/>
      <url>Implement-Thompson-Sampling-in-Recommendation-System/</url>
      
        <content type="html"><![CDATA[<h1 id="Exploring-and-Exploiting"><a href="#Exploring-and-Exploiting" class="headerlink" title="Exploring and Exploiting"></a>Exploring and Exploiting</h1><p><strong>Exploring and Exploiting (EE)</strong> 是推薦系統中歷久不衰的議題，如何幫助用戶發現更多感興趣的 entity 以及基於已有對用戶的認知推薦他感興趣的 entity，在推薦系統的實務上都得考慮。</p><p>具象化這個問題：在推薦系統中有$\text{}$ $\text{category A, category B, category C, category D, category E}$ 等五大類的 entity 集合，今天有個新用戶 $U$來了，我們要如何</p><ol><li>知道他對哪個種類的 entity 比較感興趣？</li><li>人的興趣可以分成長期興趣跟短期興趣，在電商場景中，用戶短期興趣指他有立即需求的商品，我們如何快速抓到他的意圖，調整推薦系統的響應？</li><li>推薦哪些類目能帶給他意料之外的驚喜 ? 那些他沒預期，但我們推薦給他，能讓他感到滿意的 category。</li></ol><p>Multi-armed bandit problem, K-armed bandit problem (MAP) 中的 Thompson Sampling，簡單又實用</p><a id="more"></a><p><a href="https://seed9d.github.io/categories/recommendation-system/">推薦系統相關文章</a></p><h1 id="Thompson-Sampling"><a href="#Thompson-Sampling" class="headerlink" title="Thompson Sampling"></a>Thompson Sampling</h1><blockquote><p>Thompson Sampling 利用了 beta distribution 是 bernoulli distribution 的 conjugacy prior， 來更新 entity 被選中的 posterior probability distribution</p></blockquote><h2 id="從-Beta-distribution-說起"><a href="#從-Beta-distribution-說起" class="headerlink" title="從 Beta distribution 說起"></a>從 Beta distribution 說起</h2><script type="math/tex; mode=display">Beta(p|\alpha, \beta) \triangleq  \cfrac{1}{B(\alpha, \beta)} \ p^{\alpha -1} \ (1-p)^{\beta - 1}</script><ul><li>beta function $B(\alpha, \beta)$ is a normalization term ，其作用是使 $\int^1_0 \cfrac{1}{B(\alpha, \beta)} \ p^{\alpha -1} \ (1-p)^{\beta - 1} dp = 1$<ul><li>$B(\alpha, \beta) = \int^1_0 p^{\alpha -1} (1-p)^{\beta -1} dp$</li></ul></li></ul><p>Beta distribution $beta(\alpha, \beta)$ 的期望值很簡潔</p><script type="math/tex; mode=display">E[p] = \cfrac{\alpha}{\alpha + \beta}</script><p>我們知道期望值本身是結果的加權平均，如果把 $\alpha$ 視為成功次數， $\beta$ 視為失敗次數，那不就是平均成功率了嗎？</p><p>更神奇的是，平均成功率還可以隨著試驗的失敗跟成功次數變動，依然還是 beta distribution</p><script type="math/tex; mode=display">\cfrac{\alpha + n^{(1)}}{\alpha + \beta + n^{(1)} + n^{(0)}}</script><ul><li>$n^{(1)}$：表新增成功次數</li><li>$n^{(0)}$: 表新增失敗次數</li></ul><p>也因為這個直覺的特性，Beta distribution 非常適合用在估計 打擊率, 點擊率, 命中率 …等等 binary problem</p><h3 id="以推薦系統-click-through-rate-CTR-當例子"><a href="#以推薦系統-click-through-rate-CTR-當例子" class="headerlink" title="以推薦系統 click through rate (CTR) 當例子"></a>以推薦系統 click through rate (CTR) 當例子</h3><p>在推薦系統中，category $A$ 在用戶的點擊率 (ctr) 統計中，所有用戶對 category $A$ :</p><ul><li>$\text{average ctr} = 0.33$</li><li>$\text{ctr variacne} = 0.00147$</li></ul><p>以 $\text{mean=0.33 variacne=0.00147}$  算出 $\alpha \approx50$ $\beta \approx100$， $\alpha =50$ $\beta=100$ 在本推薦系統中的意義是，$\text{category A}$  平均每 150 次 impression ($\alpha + \beta$) 能產生 50 次 click ($\alpha$)，100 次 看了不點 ($\beta$)。</p><p>畫出 PDF</p><p><img src="https://i.imgur.com/k4XcKD1.png" style="zoom: 33%;" /></p><ul><li><p>圖中 PDF curve 的意義是，有個人叫做 “平均用戶”，”平均用戶” 對 $\text{category A}$ 最有可能的點擊率是 $0.33$，但不一定是 0.33, 可能比 0.33 高，可能比 0.33 低，但產生 0.33 這個點擊率的 likelihood $L(\theta| X=0.33)$ 最高</p><ul><li>下圖是對  $beta(50, 100)$ sample 500 次，可以看出 $X=0.33$ 附近被 sample 到的次數的確較高</li></ul><p><img src="https://i.imgur.com/WpSNpVB.png" style="zoom: 33%;" /></p></li></ul><p>今天來了個新用戶 $U$，我們不知道他對 $\text{category A}$ 的喜好程度怎麼樣，但我們可以利用前面的 “平均用戶” 做為先驗： 150 impression 產生 50 次 click  ($\alpha=50 \ , \beta=100$ )，再透過他後續跟 $\text{category A}$ 的互動修正出 for  $\text{user U}$ 的 $\alpha_U \ \beta_U$。</p><p>假設我們給 $U$ 展示 $\text{category A}$  100 次後， 他 click了 60 次，看了不點 40 次，那他的 beta distribution 變成</p><p>$beta(50 + 60, 100 + 40 ) = beta(110, 140)$</p><p><img src="https://i.imgur.com/NKVZ91e.png" style="zoom: 33%;" /></p><p>可以發現橘線變得更尖，且往右移，此時 $mean =0.44$，表示 $user \ U$ 比＂平均用戶＂更加偏好 $\text{category A}$。</p><p>總結以上，一開始我們對於新用戶 $U$ 一無所知，不知道他對 $\text{category A}$ 的偏好，但我們透過已有的先驗，結合他跟推薦系統的互動，慢慢修正對他的認知：</p><script type="math/tex; mode=display">\cfrac{\alpha + n^{(1)}}{\alpha + \beta + n^{(1)} + n^{(0)}} = \cfrac{50 + 60}{50 + 100 + 60 + 40} = 0.44</script><ul><li>$n^{(1)}$：對 $\text{category A}$ 新的點擊行為</li><li>$n^{(0)}$: 對 $\text{category A}$ 新的＂看了未點＂的行為</li></ul><p>於是，ctr 從原本 “最有可能” 0.33 修正到 “最有可能” 0.44 。</p><ul><li>“最有可能”: 因爲一切都是 distribution 阿</li></ul><p>這個神奇又簡潔的現象背後的數學原理，正是 beta distribution 的 conjugacy 特性。</p><h2 id="Conjugate-prior-amp-Bayesian-inference"><a href="#Conjugate-prior-amp-Bayesian-inference" class="headerlink" title="Conjugate prior &amp; Bayesian inference"></a>Conjugate prior &amp; Bayesian inference</h2><blockquote><p>prior $p(\theta)$ is conjugate to the  likelihood function $p(X|\theta)$ when the posterior $p(\theta|X)$ has the same function form as the prior</p></blockquote><script type="math/tex; mode=display">p(\theta|X) = \cfrac{p(X|\theta)  p(\theta)}{p(X)}  \Leftrightarrow  \text{posterior} = \cfrac{\text{likelihood} \cdot \text{prior}}{\text{evidence}}</script><ul><li><p>$p(X)$ is the normalization term</p><p>$p(X) = \int_{\theta\in \Theta}p(X|\theta)p(\theta)d\theta$</p></li></ul><p>即是</p><ul><li>prior $p(\theta)$  為 beta distribution $Beta(\theta|\alpha, \beta) = \cfrac{1}{B(\alpha, \beta)} \ \theta^{\alpha -1} \ (1-\theta)^{\beta - 1}$</li><li>likelihood function $p(X|\theta)$ 為 bernoulli distribution   $Bern(c|\theta) = \theta^c(1-\theta)^{1-c}$</li></ul><p>beta distribution 與 bernoulli distribution 都有類似的 form: $\theta^m(1-\theta)^n$ ，同時 posterior distribution $p(\theta|X)$ 也是 beta distribution</p><p>posterior  $p(\theta|X)$ 也是 beta distribution 證明如下</p><h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>假設  </p><ul><li><p>推薦系統中，對 $category \ A$ 曝光 $N$ 次，用戶 $U$ 點擊次數 $n^{(1)}$，未點擊次數 $n^{(0)}$，本質上是個 $N \ bernoulli \ trail$ ， 所以其 likelihood function：</p><p>$p(C|p) =\prod_{i=1}^n p(C=c_i|p)= p^{n^{(1)}}(1-p)^{n^{(0)}}$ (忽略係數)</p><ul><li>$C$ 是 outcome, $c=1$ for positive ; $c=0$ for negative</li></ul></li><li><p>$prior$ $p(p)$ 為 beta distribution :</p><script type="math/tex; mode=display">p(p|\alpha, \beta) =  Beta(p|\alpha, \beta) = \cfrac{1}{B(\alpha, \beta)} \ p^{\alpha -1} \ (1-p)^{\beta - 1}</script></li></ul><p>則 $\text{posterior}$  $p(p|C,\alpha, \beta) = \cfrac{ p(C|p) \ p(p|\alpha, \beta)}{\int^1_0  p(C|p) \ p(p|\alpha, \beta) \ dp}$ </p><p>分母項 $\int^1_0  p(C|p) \ p(p|\alpha, \beta) \ dp$  作用為 normalize the distribution，通常用 $Z$ 代表：</p><script type="math/tex; mode=display">\begin{aligned}p(p|C,\alpha, \beta)  &= \cfrac{ p(C|p)  p(p|\alpha, \beta)}{\int^1_0  p(C|p)  p(p|\alpha, \beta) dp} \\&= \cfrac{p^{n^{(1)}} (1-p)^{n^{(0)}} \cfrac{1}{B(\alpha,\beta)} p^{\alpha -1} (1-p)^{\beta -1}}{Z} \\ &= \cfrac{p^{[n^{(1)} + \alpha] -1} (1 - p) ^{[n^{(0)} + \beta]-1}}{B(\alpha, \beta) Z} \end{aligned}</script><ul><li>$Z =\cfrac{1}{B(\alpha,\beta)} \int^1_0 p^{n^{(1)}} (1-p)^{n^{(0)}}  p^{\alpha -1} (1-p)^{\beta -1} dp$</li></ul><p>分母要 normalize 整個 probability distribution 使  $\int p(p|C,\alpha, \beta) dp= 1$</p><p>而新的 normalization 項為 </p><script type="math/tex; mode=display">B(\alpha,\beta)Z =  \int^1_0 p^{[n^{(1)} + \alpha] -1} (1 - p) ^{[n^{(0)} + \beta]-1}dp</script><p>這不正是另一個 Beta function:  $B(n^{(1)} + \alpha , n^{(0) } +\beta)$ ？？</p><p>所以  $p(p|C,\alpha, \beta)$  最終化簡成</p><script type="math/tex; mode=display">\begin{aligned} p(p|C,\alpha, \beta)  &= \cfrac{p^{[n^{(1)} + \alpha] -1} (1 - p) ^{[n^{(0)} + \beta]-1}}{B(n^{(1)} + \alpha , n^{(0) } +\beta)} \\ &= Beta (p|n^{(1)} +\alpha, n^{(0)} + \beta)\end{aligned}</script><p>故得證 $\text{posterior}$ $p(p|C,\alpha, \beta)$  也是 $\text{Beta distribution}$</p><h1 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h1><p>一個簡單的實作方式是</p><ol><li>先在線下計算好每個 category 的 ctr mean 跟 variance。</li><li>在實時推薦時，拿回某用戶近期對每個 category 交互數據 impression 與 click ，計算出新的  $\alpha  \ \beta$。</li><li>有了每個類目的 $\alpha \ \beta$ 後，對每個類目的 $beta(\alpha, \beta)$ sampling，接著取出 sample 後 top K 的類目即可。</li><li>C2I 召回 …..</li></ol><p>當然，你也可以不基於 category 維度計算 beta distribution，而是基於每一個 entity。不過如果 entity 數量上百萬，這顯然不切實際。</p><h2 id="線下"><a href="#線下" class="headerlink" title="線下"></a>線下</h2><h3 id="統計每個-category-CTR-的-variance-and-mean"><a href="#統計每個-category-CTR-的-variance-and-mean" class="headerlink" title="統計每個 category CTR 的 variance and mean"></a>統計每個 category CTR 的 variance and mean</h3><ul><li>Spark snippets</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calAvgAndVar</span></span>(input: <span class="type">Dataset</span>[<span class="type">Row</span>], categoryCol: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">Row</span>] =</span><br><span class="line">    input.select(categoryCol, ctrCol)</span><br><span class="line">      .groupBy(categoryCol).agg(</span><br><span class="line">      fn.avg(fn.col(ctrCol)).as(<span class="string">&quot;ctr_avg&quot;</span>),</span><br><span class="line">      fn.variance(ctrCol).alias(<span class="string">&quot;ctr_var&quot;</span>))</span><br><span class="line">      .na.drop</span><br><span class="line">      .withColumnRenamed(categoryCol, <span class="string">&quot;categoryId&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="線上"><a href="#線上" class="headerlink" title="線上"></a>線上</h2><ol><li><p>計算每個 category 的初始 $\alpha_0 \ \beta_0$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ImmutablePair&lt;Double, Double&gt; <span class="title">calAlphaAndBeta</span><span class="params">(<span class="keyword">double</span> ctrMean, <span class="keyword">double</span> ctrVar)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">double</span> alpha = (((<span class="number">1</span> - ctrMean) / (ctrMean)) - <span class="number">1</span> / ctrMean) * Math.pow(ctrMean, <span class="number">2</span>);</span><br><span class="line">            <span class="keyword">double</span> beta = alpha * ((<span class="number">1</span> / ctrMean) - <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span> ImmutablePair.of(alpha, beta);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}\alpha &=\left(\frac{1-\mu}{\sigma^2}-\frac{1}{\mu}\right)\mu^2 \\\beta &= \alpha\left(\frac{1}{\mu}-1\right)\end{aligned}</script></li><li><p>取回用戶的近期 category 交互行為 impression and click，並計算新的 $\alpha_t,\ \beta_t$</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* left: alpha, right: beta */</span></span><br><span class="line">ImmutablePair&lt;Double, Double&gt; prior = calAlphaAndBeta(<span class="keyword">double</span> ctrMean, <span class="keyword">double</span> ctrVar);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* left: impression, right: click */</span></span><br><span class="line">ImmutablePair&lt;Integer, Integer&gt; posteriorPair = posteriorData.getOrDefault(cateId, ImmutablePair.of(<span class="number">0</span>, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> clickCount = posteriorPair.getRight();</span><br><span class="line"><span class="keyword">int</span> impressionCount = posteriorPair.getLeft();</span><br><span class="line"><span class="keyword">int</span> impressionWithoutClick = (impressionCount - clickCount) &gt; <span class="number">0</span> ? (impressionCount - clickCount) : impressionCount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> newAlpha = prior.getLeft() + clickCount;</span><br><span class="line"><span class="keyword">double</span> newBeta =  prior.getRight() + impressionWithoutClick;</span><br></pre></td></tr></table></figure></li><li><p>對每個 category 的 beta distribution $beta(\alpha, \beta)$ sampling</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.commons.math3.distribution.BetaDistribution;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">calBetaProbability</span><span class="params">(<span class="keyword">double</span> alpha, <span class="keyword">double</span> beta)</span> </span>&#123;</span><br><span class="line">  BetaDistribution betaDistribution = <span class="keyword">new</span> BetaDistribution(alpha, beta);</span><br><span class="line">    <span class="keyword">double</span> rand = Math.random();</span><br><span class="line">    <span class="keyword">return</span> betaDistribution.inverseCumulativeProbability(rand);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>sampling 利用 beta distribution 的 inverse cumulative distribution function (inverse CDF) sampling 出 random variable<ul><li>參考 <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">https://en.wikipedia.org/wiki/Inverse_transform_sampling</a></li></ul></li></ul></li></ol><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Multi-Armed Bandit With Thompson Sampling <a href="https://predictivehacks.com/multi-armed-bandit-with-thompson-sampling/">https://predictivehacks.com/multi-armed-bandit-with-thompson-sampling/</a></li><li>Conjugacy in Bayesian Inference <a href="http://gregorygundersen.com/blog/2019/03/16/conjugacy/">http://gregorygundersen.com/blog/2019/03/16/conjugacy/</a></li><li>Understanding the beta distribution (using baseball statistics) <a href="http://varianceexplained.org/statistics/beta_distribution_and_baseball/">http://varianceexplained.org/statistics/beta_distribution_and_baseball/</a><ul><li>中文翻譯 : 如何通俗理解 beta 分布？ - 小杰的回答 - 知乎 <a href="https://www.zhihu.com/question/30269898/answer/123261564">https://www.zhihu.com/question/30269898/answer/123261564</a></li></ul></li><li><a href="https://en.wikipedia.org/wiki/Beta_distribution">https://en.wikipedia.org/wiki/Beta_distribution</a></li><li>Heinrich, G. (2005). Parameter estimation for text analysis<ul><li>雖然是講 LDA，但前面從 ML MAP 一路推導到 Bayesian inference ，很詳細</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> recommendation system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommendation system </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax</title>
      <link href="Pytorch-Implement-CBOW-with-Hierarchical-Softmax/"/>
      <url>Pytorch-Implement-CBOW-with-Hierarchical-Softmax/</url>
      
        <content type="html"><![CDATA[<h1 id="CBOW-with-Hierarchical-Softmax"><a href="#CBOW-with-Hierarchical-Softmax" class="headerlink" title="CBOW with Hierarchical Softmax"></a>CBOW with Hierarchical Softmax</h1><p>CBOW  的思想是用兩側 context words 去預測中間的 center word</p><script type="math/tex; mode=display">P(center|context;\theta)</script><a id="more"></a><p><img src="https://i.imgur.com/WukfJ8F.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>$\text{v}’_j$ 表 $W’$  中  j-th columns  vector</li><li>在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是 leaf node  ，跟 column vector $\text{v}’_j$ 與 word $w_i$  不是直接對應的關係</li></ul></li></ul><h2 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h2><p><img src="https://i.imgur.com/c40z44h.png" style="zoom:50%;" /></p><p>Huffman Tree</p><p>令 $w_{I,j}$  表 input 的 第 $j$ 個 context word; $w_O$ 表 target 的 center word</p><p> 則 Hierarchical Softmax 下的 objective function</p><script type="math/tex; mode=display">\begin{aligned} &-\log p(w_O| w_I) = -\log \dfrac{\text{exp}({h^\top \text{v}'_O})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})} \\& = - \sum^{L(w)-1}_{l=1}  \log\sigma([\cdot] h^\top \text{v}^{'}_l)\end{aligned}</script><ul><li>$L(w_i) -1$ 表 huffman tree 中從 root node  到 leaf node of $w_i$ 的 node number</li><li>$[\cdot]$表 huffman tree 的分岔判斷<ul><li>$[\cdot] = 1$ 表 turn left</li><li>$[\cdot ] = -1$ 表 turn right</li></ul></li><li>$h = \frac {1}{C} \sum^{C}_{j=1}\text{v}_{w_{I,j}}$ average of all context word vector $w_{I,j}$</li></ul><p>詳細推導請見 <a href="/hierarchical-softmax-in-word2vec/" title="Word2Vec (2):Hierarchical Softmax 背後的數學">Word2Vec (2):Hierarchical Softmax 背後的數學</a></p><p>透過 Hierarchical Softmax，因爲 huffman tree 為 full binary tree， time complexity 降成 $\log_2|V|$</p><h1 id="Pytorch-CBOW-with-Hierarchical-Softmax"><a href="#Pytorch-CBOW-with-Hierarchical-Softmax" class="headerlink" title="Pytorch CBOW with Hierarchical Softmax"></a>Pytorch CBOW with Hierarchical Softmax</h1><h2 id="Building-Huffman-Tree"><a href="#Building-Huffman-Tree" class="headerlink" title="Building Huffman Tree"></a>Building Huffman Tree</h2><p>Huffman Tree  建樹過程</p><figure class="highlight"><figcaption><span>HuffmanTree >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuffmanTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, fre_dict</span>):</span></span><br><span class="line">        self.root = <span class="literal">None</span></span><br><span class="line">        freq_dict = <span class="built_in">sorted</span>(fre_dict.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(freq_dict)</span><br><span class="line">        self.node_dict = &#123;&#125;</span><br><span class="line">        self._build_tree(freq_dict)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_tree</span>(<span class="params">self, freq_dict</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            freq_dict is in decent order</span></span><br><span class="line"><span class="string">            node_list: two part: [leaf node :: internal node]</span></span><br><span class="line"><span class="string">                leaf node is sorting by frequency in decent order; </span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">        node_list = [HuffmanNode(is_leaf=<span class="literal">True</span>, value=w, fre=fre) <span class="keyword">for</span> w, fre <span class="keyword">in</span> freq_dict]  <span class="comment"># create leaf node</span></span><br><span class="line">        node_list += [HuffmanNode(is_leaf=<span class="literal">False</span>, fre=<span class="number">1e10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size)]  <span class="comment"># create non-leaf node</span></span><br><span class="line"></span><br><span class="line">        parentNode = [<span class="number">0</span>] * (self.vocab_size * <span class="number">2</span>)  <span class="comment"># only 2 * vocab_size - 2 be used</span></span><br><span class="line">        binary = [<span class="number">0</span>] * (self.vocab_size * <span class="number">2</span>)  <span class="comment"># recording turning left or turning right</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">          pos1 points to currently processing leaf node at left side of node_list</span></span><br><span class="line"><span class="string">          pos2 points to currently processing non-leaf node at right side of node_list</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        pos1 = self.vocab_size - <span class="number">1</span></span><br><span class="line">        pos2 = self.vocab_size</span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            each iteration picks two node from node_list</span></span><br><span class="line"><span class="string">            the first pick assigns to min1i</span></span><br><span class="line"><span class="string">            the second pick assigns to min2i </span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            min2i&#x27;s frequency is always larger than min1i</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        min1i = <span class="number">0</span></span><br><span class="line">        min2i = <span class="number">0</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            the main process of building huffman tree</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size - <span class="number">1</span>):</span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                first pick assigns to min1i</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> pos1 &gt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> node_list[pos1].fre &lt; node_list[pos2].fre:</span><br><span class="line">                    min1i = pos1</span><br><span class="line">                    pos1 -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    min1i = pos2</span><br><span class="line">                    pos2 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                min1i = pos2</span><br><span class="line">                pos2 += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">               second pick assigns to min2i </span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> pos1 &gt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> node_list[pos1].fre &lt; node_list[pos2].fre:</span><br><span class="line">                    min2i = pos1</span><br><span class="line">                    pos1 -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    min2i = pos2</span><br><span class="line">                    pos2 += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                min2i = pos2</span><br><span class="line">                pos2 += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27; fill information of non leaf node &#x27;&#x27;&#x27;</span></span><br><span class="line">            node_list[self.vocab_size + a].fre = node_list[min1i].fre + node_list[min2i].fre</span><br><span class="line">            node_list[self.vocab_size + a].left = node_list[min1i]</span><br><span class="line">            node_list[self.vocab_size + a].right = node_list[min2i]</span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                the parent node always is non leaf node</span></span><br><span class="line"><span class="string">                assigen lead child (min2i) and right child (min1i) to parent node</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            parentNode[min1i] = self.vocab_size + a  <span class="comment"># max index = 2 * vocab_size - 2</span></span><br><span class="line">            parentNode[min2i] = self.vocab_size + a</span><br><span class="line">            binary[min2i] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;generate huffman code of each leaf node &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size):</span><br><span class="line">            b = a</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            code = []</span><br><span class="line">            point = []</span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                backtrace path from current node until root node. (bottom up)</span></span><br><span class="line"><span class="string">                &#x27;root node index&#x27; in node_list is  2 * vocab_size - 2 </span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">while</span> b != self.vocab_size * <span class="number">2</span> - <span class="number">2</span>:</span><br><span class="line">                code.append(binary[b])  </span><br><span class="line">                b = parentNode[b]</span><br><span class="line">                <span class="comment"># point recording the path index from leaf node to root, the length of point is less 1 than the length of code</span></span><br><span class="line">                point.append(b)</span><br><span class="line">            </span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                huffman code should be top down, so we reverse it.</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            node_list[a].code_len = <span class="built_in">len</span>(code)</span><br><span class="line">            node_list[a].code = <span class="built_in">list</span>(<span class="built_in">reversed</span>(code))</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">                1. Recording the path from root to leaf node (top down). </span></span><br><span class="line"><span class="string">                </span></span><br><span class="line"><span class="string">                2.The actual index value should be shifted by self.vocab_size,</span></span><br><span class="line"><span class="string">                  because we need the index starting from zero to mapping non-leaf node</span></span><br><span class="line"><span class="string">                </span></span><br><span class="line"><span class="string">                3. In case of full binary tree, the number of non leaf node always equals to vocab_size - 1.</span></span><br><span class="line"><span class="string">                  The index of BST root node in node_list is 2 * vocab_size - 2,</span></span><br><span class="line"><span class="string">                  and we shift vocab_size to get the actual index of root node: vocab_size - 2</span></span><br><span class="line"><span class="string">            &#x27;&#x27;&#x27;</span></span><br><span class="line">            node_list[a].node_path = <span class="built_in">list</span>(<span class="built_in">reversed</span>([p - self.vocab_size <span class="keyword">for</span> p <span class="keyword">in</span> point]))</span><br><span class="line">            </span><br><span class="line">            self.node_dict[node_list[a].value] = node_list[a]</span><br><span class="line">            </span><br><span class="line">        self.root = node_list[<span class="number">2</span> * vocab_size - <span class="number">2</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure><p>建樹過程參考 Word2Vec 作者 Tomas Mikolov 的 c code，思路如下：</p><ol><li>建一個  Array，左半邊放 leaf node ，右半邊放 non leaf node<ul><li>leaf node 按照 frequency 降序排列</li></ul></li><li>bottom up building tree<ul><li>從 Array 中間位置向右半邊填 non leaf node</li><li>each iteration 都從 leaf node 跟 已填完的 non leaf node 找兩個 frequency 最小的 node，做為  child node 填入當下 non leaf node</li></ul></li></ol><p><img src="https://i.imgur.com/H7do73N.png" style="zoom: 67%;" /></p><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>用 huffman tree 實作 Hierarchical Softmax</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HierarchicalSoftmaxLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, freq_dict</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">## in w2v c implement, syn1 initial with all zero</span></span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.syn1 = nn.Embedding(</span><br><span class="line">            num_embeddings=vocab_size + <span class="number">1</span>,</span><br><span class="line">            embedding_dim=embedding_dim,</span><br><span class="line">            padding_idx=vocab_size</span><br><span class="line">            </span><br><span class="line">        )</span><br><span class="line">        torch.nn.init.constant_(self.syn1.weight.data, val=<span class="number">0</span>)</span><br><span class="line">        self.huffman_tree = HuffmanTree(freq_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, neu1, target</span>):</span></span><br><span class="line">        <span class="comment"># neu1: [b_size, embedding_dim]</span></span><br><span class="line">        <span class="comment"># target: [b_size, 1]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># turns:[b_size, max_code_len_in_batch]</span></span><br><span class="line">        <span class="comment"># paths: [b_size, max_code_len_in_batch]</span></span><br><span class="line">        turns, paths = self._get_turns_and_paths(target)</span><br><span class="line">        paths_emb = self.syn1(paths) <span class="comment"># [b_size, max_code_len_in_batch, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">        loss = -F.logsigmoid(</span><br><span class="line">            (turns.unsqueeze(<span class="number">2</span>) * paths_emb * neu1.unsqueeze(<span class="number">1</span>)).<span class="built_in">sum</span>(<span class="number">2</span>)).<span class="built_in">sum</span>(<span class="number">1</span>).mean()</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_turns_and_paths</span>(<span class="params">self, target</span>):</span></span><br><span class="line">        turns = []  <span class="comment"># turn right(1) or turn left(-1) in huffman tree</span></span><br><span class="line">        paths = []</span><br><span class="line">        max_len = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> target:</span><br><span class="line">            n = n.item()</span><br><span class="line">            node = self.huffman_tree.node_dict[n]</span><br><span class="line">            </span><br><span class="line">            code = target.new_tensor(node.code).<span class="built_in">int</span>()  <span class="comment"># in code, left node is 0; right node is 1</span></span><br><span class="line">            turn = torch.where(code == <span class="number">1</span>, code, -torch.ones_like(code))</span><br><span class="line">            </span><br><span class="line">            turns.append(turn)</span><br><span class="line">            paths.append(target.new_tensor(node.node_path))</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> node.code_len &gt; max_len:</span><br><span class="line">                max_len = node.code_len</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        turns = [F.pad(t, pad=(<span class="number">0</span>, max_len - <span class="built_in">len</span>(t)), mode=<span class="string">&#x27;constant&#x27;</span>, value=<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> turns] </span><br><span class="line">        paths = [F.pad(p, pad=(<span class="number">0</span>, max_len - p.shape[<span class="number">0</span>]), mode=<span class="string">&#x27;constant&#x27;</span>, value=net.hs.vocab_size) <span class="keyword">for</span> p <span class="keyword">in</span> paths]</span><br><span class="line">        <span class="keyword">return</span> torch.stack(turns).<span class="built_in">int</span>(), torch.stack(paths).long()</span><br></pre></td></tr></table></figure><ul><li>syn1 表 $W’$ 裡面的 vector 對應到 huffman tree non leaf node 的 vector<ul><li>實作上 $W’$  row vector 才有意義</li></ul></li><li>neu1 即 $\text{h}$ 為 hidden layer 的輸出</li><li>target 為 center word $w_O$</li><li>function _get_turns_and_paths 中<ul><li>實作時 -1 表 turn left ; 1 表 turn right ，其實兩者只要相反就好，因爲對於 binary classification<ul><li>$p(\text{true}) = \sigma(x)$ ⇒ $p(\text{false}) = 1- \sigma(x) = \sigma(-x)$</li><li>只是  $\sigma$ 裡的正負號對換而已</li></ul></li></ul></li></ul><h2 id="CBOW-Hierarchical-Softmax"><a href="#CBOW-Hierarchical-Softmax" class="headerlink" title="CBOW + Hierarchical Softmax"></a>CBOW + Hierarchical Softmax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOWHierarchicalSoftmax</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, fre_dict</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.hs = HierarchicalSoftmaxLayer(vocab_size, embedding_dim, fre_dict)</span><br><span class="line">        torch.nn.init.xavier_uniform_(self.syn0.weight.data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, context, target</span>):</span></span><br><span class="line">        <span class="comment"># context: [b_size, 2 * window_size]</span></span><br><span class="line">        <span class="comment"># target: [b_size]</span></span><br><span class="line">        neu1 = self.syn0(context.long()).mean(dim=<span class="number">1</span>)  <span class="comment"># [b_size, embedding_dim]</span></span><br><span class="line">        loss = self.hs(neu1, target.long())</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>neu1 為 average of context words’ vector</li></ul><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>訓練過程省略，有興趣請見 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Embedding">seed9D/hands-on-machine-learning</a></p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>訓練語料是聖經，看看 jesus  跟  christ 的相近詞</p><p>In : </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)</span><br><span class="line">cosinSim.get_synonym(<span class="string">&#x27;christ&#x27;</span>)</span><br></pre></td></tr></table></figure><p>out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;christ&#x27;</span>, <span class="number">1.0</span>),</span><br><span class="line"> (<span class="string">&#x27;hope&#x27;</span>, <span class="number">0.78780156</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.7656436</span>),</span><br><span class="line"> (<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.74575657</span>),</span><br><span class="line"> (<span class="string">&#x27;faith&#x27;</span>, <span class="number">0.7190881</span>),</span><br><span class="line"> (<span class="string">&#x27;godliness&#x27;</span>, <span class="number">0.7005944</span>),</span><br><span class="line"> (<span class="string">&#x27;offences&#x27;</span>, <span class="number">0.70045626</span>),</span><br><span class="line"> (<span class="string">&#x27;grace&#x27;</span>, <span class="number">0.6946964</span>),</span><br><span class="line"> (<span class="string">&#x27;dear&#x27;</span>, <span class="number">0.666232</span>),</span><br><span class="line"> (<span class="string">&#x27;willing&#x27;</span>, <span class="number">0.66131693</span>)]</span><br></pre></td></tr></table></figure><p>In </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cosinSim.get_synonym(<span class="string">&#x27;jesus&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Out</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.9999999</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.8051339</span>),</span><br><span class="line"> (<span class="string">&#x27;grace&#x27;</span>, <span class="number">0.75879383</span>),</span><br><span class="line"> (<span class="string">&#x27;church&#x27;</span>, <span class="number">0.7542972</span>),</span><br><span class="line"> (<span class="string">&#x27;christ&#x27;</span>, <span class="number">0.74575657</span>),</span><br><span class="line"> (<span class="string">&#x27;manifest&#x27;</span>, <span class="number">0.7415799</span>),</span><br><span class="line"> (<span class="string">&#x27;believed&#x27;</span>, <span class="number">0.7215627</span>),</span><br><span class="line"> (<span class="string">&#x27;faith&#x27;</span>, <span class="number">0.7198993</span>),</span><br><span class="line"> (<span class="string">&#x27;godliness&#x27;</span>, <span class="number">0.7091305</span>),</span><br><span class="line"> (<span class="string">&#x27;john&#x27;</span>, <span class="number">0.7015951</span>)]</span><br></pre></td></tr></table></figure><p>In</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cosinSim.get_synonym(<span class="string">&#x27;god&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Out</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.9999999</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.8051339</span>),</span><br><span class="line"> (<span class="string">&#x27;grace&#x27;</span>, <span class="number">0.75879383</span>),</span><br><span class="line"> (<span class="string">&#x27;church&#x27;</span>, <span class="number">0.7542972</span>),</span><br><span class="line"> (<span class="string">&#x27;christ&#x27;</span>, <span class="number">0.74575657</span>),</span><br><span class="line"> (<span class="string">&#x27;manifest&#x27;</span>, <span class="number">0.7415799</span>),</span><br><span class="line"> (<span class="string">&#x27;believed&#x27;</span>, <span class="number">0.7215627</span>),</span><br><span class="line"> (<span class="string">&#x27;faith&#x27;</span>, <span class="number">0.7198993</span>),</span><br><span class="line"> (<span class="string">&#x27;godliness&#x27;</span>, <span class="number">0.7091305</span>),</span><br><span class="line"> (<span class="string">&#x27;john&#x27;</span>, <span class="number">0.7015951</span>)]</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://github.com/tmikolov/word2vec">https://github.com/tmikolov/word2vec</a><ul><li>c code</li></ul></li><li>基于Numpy实现Word2Vec Hierarchical Softmax CBOW and SkipGram模型 <a href="http://ziyangluo.tech/2020/02/29/W2VHierarchical/">http://ziyangluo.tech/2020/02/29/W2VHierarchical/</a></li><li><a href="https://github.com/ilyakhov/pytorch-word2vec">https://github.com/ilyakhov/pytorch-word2vec</a></li><li><a href="https://github.com/weberrr/pytorch_word2vec">https://github.com/weberrr/pytorch_word2vec</a></li><li>other<ul><li>Binary Tree: Intro(簡介) <a href="http://alrightchiu.github.io/SecondRound/binary-tree-introjian-jie.html#fullcomplete">http://alrightchiu.github.io/SecondRound/binary-tree-introjian-jie.html#fullcomplete</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling</title>
      <link href="Pytorch-Implement-Skipgram-with-Negative-Sampling/"/>
      <url>Pytorch-Implement-Skipgram-with-Negative-Sampling/</url>
      
        <content type="html"><![CDATA[<h1 id="Skipgram-with-Negative-Sampling"><a href="#Skipgram-with-Negative-Sampling" class="headerlink" title="Skipgram with Negative Sampling"></a>Skipgram with Negative Sampling</h1><p>skipgram  的思想是用中心詞 center word 去預測兩側的 context words </p><script type="math/tex; mode=display">P(context|center; \theta)</script><a id="more"></a><p><img src="https://i.imgur.com/snzt3hg.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h2 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h2><p> 令 $w_I$ 表 input 的 center word ; $w_{O,j}$ 表 target 的 第 $j$ 個 context word。</p><p> 則 Negative Sampling 下的 objective function</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \sigma(\text{v}^\top_{w_I} \text{v}'_{w_{O,j}}) +  \sum^M_{\substack{i=1 \\ \tilde{w}_i \sim Q}}\sigma(-\text{v}^\top_{w_I} \text{v}'_{\tilde{w}_i})]</script><ul><li>$\tilde{w}_i$ 為從 distribution $Q$ sample 出的 word</li><li>M 為 從  $Q$  sample 出的 $\tilde{w}$ 數量</li></ul><p>第一項為 input center word  $w_I$ 與 target  context word $w_{O,j}$ 產生的 loss</p><p>第二項為 negative sample 產生的 loss ，共 sample 出 $M$  個  word </p><p>有興趣看從 softmax 推導到 NEG的，參閱 <a href="/negative-sampling-in-word2vec/" title="Word2Vec (3):Negative Sampling 背後的數學">Word2Vec (3):Negative Sampling 背後的數學</a></p><h2 id="Negative-Sample-NEG"><a href="#Negative-Sample-NEG" class="headerlink" title="Negative Sample (NEG)"></a>Negative Sample (NEG)</h2><p>目標是從一個分佈 $Q$ sample 出 word $\tilde{w}$</p><p> 實作上從 vocabulary $V$ sample  出 ${w}_i$ 的 probability $P(w_i)$ 為</p><script type="math/tex; mode=display">P(w_i) = \cfrac{f(w_i)^{\alpha}}{\sum^M_{j=0}(f(w_j)^\alpha)}</script><ul><li>$f(w_i)$  為 $w_i$ 在 corpus 的 frequency count</li><li>$\alpha$  為 factor, 通常設為  $0.75$，其作用是 increase  the probability for less frequency words and decrease the probability for more frequent words</li></ul><p>每個 word  $w_i$ 都有個被 sample 出的 probability $P(w_i)$， 目的是從 $P(w)$ sample 出 $M$ 個  word 做為 negative 項</p><p>網路上常見的實現方法是調用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.multinomial(sample_size, pvals)</span><br></pre></td></tr></table></figure><p>此法應該是透過 inverse CDF 來 sample word，每筆 training data 都調用一次的話運算效率不高</p><p>Word2Vec 作者 Tomas Mikolov 在他的 c code  中，採用了一種近似方式，其思想是在極大的抽樣次數下 $M = 1e8$，word  的 probability 越高代表其 frequency 越大，也就是在 M 中所占份額 shares 越多。</p><p>例如 yellow 的 probability 最大，理應在 M=30 中佔據較多的份額。</p><p><img src="https://i.imgur.com/lCdJO7Y.png" alt=""></p><ul><li>$P(\text{blue}) = \frac{2}{30}$</li><li>$P(\text{green}) = \frac{6}{30}$</li><li>$P(\text{yellow}) = \frac{10}{30}$</li><li>$P(\text{red}) = \frac{5}{30}$</li><li>$P(\text{gray}) = \frac{7}{30}$</li></ul><p>所以事先準備好一張 size 夠大的 table ($M = 1e8$)，根據 word frequency 給予相應的 shares ，真正要 sample word 的時候，只要從 $M$ 中 uniform random 出一個 index $m$ ， index $m$ 對應到的 word 就是被 sample 出的  word  $\tilde{w}$，是個以空間換取時間的做法。</p><h3 id="Seeing-is-Believing"><a href="#Seeing-is-Believing" class="headerlink" title="Seeing is Believing"></a>Seeing is Believing</h3><p>做了一下測試 ，10000 次迭代，每次取  6 個  negatvie sample 的情景下，Tomas Mikolov 的近似思路比較有效率，而且是碾壓性的</p><p><img src="https://i.imgur.com/YLbFdt4.png" style="zoom: 50%;" /></p><p>但在 一次 sample 較多 word 的時候，multinomial 較有效率，可能 numpy 內部有做平行化的關係</p><p><img src="https://i.imgur.com/MXK06wG.png" style="zoom:50%;" /></p><h1 id="Pytorch-Skipgram-with-Negative-Sampling"><a href="#Pytorch-Skipgram-with-Negative-Sampling" class="headerlink" title="Pytorch Skipgram with Negative Sampling"></a>Pytorch Skipgram with Negative Sampling</h1><h2 id="Negative-Sample"><a href="#Negative-Sample" class="headerlink" title="Negative Sample"></a>Negative Sample</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NegativeSampler</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, corpus, sample_ratio=<span class="number">0.75</span></span>):</span></span><br><span class="line">        self.sample_ratio = sample_ratio</span><br><span class="line">        self.sample_table =  self.__build_sample_table(corpus)</span><br><span class="line">        self.table_size = <span class="built_in">len</span>(self.sample_table)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__build_sample_table</span>(<span class="params">self, corpus</span>):</span></span><br><span class="line">        counter = <span class="built_in">dict</span>(Counter(<span class="built_in">list</span>(itertools.chain.from_iterable(corpus))))</span><br><span class="line">        words = np.array(<span class="built_in">list</span>(counter.keys()))</span><br><span class="line">        probs = np.power(np.array(<span class="built_in">list</span>(counter.values())), self.sample_ratio)</span><br><span class="line">        normalizing_factor = probs.<span class="built_in">sum</span>()</span><br><span class="line">        probs = np.divide(probs, normalizing_factor)</span><br><span class="line">        </span><br><span class="line">        sample_table = []</span><br><span class="line"></span><br><span class="line">        table_size = <span class="number">1e8</span></span><br><span class="line">        word_share_list = np.<span class="built_in">round</span>(probs * table_size)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">         the higher prob, the more shares in  sample_table</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">for</span> w_idx, w_fre <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_share_list):</span><br><span class="line">            sample_table += [words[w_idx]] * <span class="built_in">int</span>(w_fre)</span><br><span class="line"></span><br><span class="line"><span class="comment">#         sample_table = np.array(sample_table) // too slow</span></span><br><span class="line">        <span class="keyword">return</span> sample_table</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, sample_size=<span class="number">6</span></span>):</span></span><br><span class="line">        negatvie_samples = [self.sample_table[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> np.random.randint(<span class="number">0</span>, self.table_size, sample_size)]</span><br><span class="line">        <span class="keyword">return</span> np.array(negatvie_samples)</span><br></pre></td></tr></table></figure><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sampler = NegativeSampler(corpus)</span><br><span class="line">sampler.generate()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([&#39;visiting&#39;, &#39;defiled&#39;, &#39;thieves&#39;, &#39;beyond&#39;, &#39;lord&#39;, &#39;fill&#39;],</span><br><span class="line">      dtype&#x3D;&#39;&lt;U18&#39;)</span><br></pre></td></tr></table></figure><h2 id="Skipgram-NEG"><a href="#Skipgram-NEG" class="headerlink" title="Skipgram + NEG"></a>Skipgram + NEG</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipGramNEG</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim) <span class="comment"># |V| x |K|</span></span><br><span class="line">        self.neg_syn1 = nn.Embedding(vocab_size, embedding_dim) <span class="comment"># |V| x |K|</span></span><br><span class="line">        torch.nn.init.constant_(self.neg_syn1.weight.data, val=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, center: torch.Tensor, context: torch.Tensor, negative_samples: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># center : [b_size, 1]</span></span><br><span class="line">        <span class="comment"># context: [b_size, 1]</span></span><br><span class="line">        <span class="comment"># negative_sample: [b_size, negative_sample_num]</span></span><br><span class="line">        embd_center = self.syn0(center)  <span class="comment"># [b_size, 1, embedding_dim]</span></span><br><span class="line">        embd_context = self.neg_syn1(context) <span class="comment"># [b_size, 1, embedding_dim]</span></span><br><span class="line">        embd_negative_sample = self.neg_syn1(negative_samples) <span class="comment"># [b_size, negative_sample_num, embedding_dim]</span></span><br><span class="line">        </span><br><span class="line">        prod_p =  (embd_center * embd_context).<span class="built_in">sum</span>(dim=<span class="number">1</span>).squeeze()  <span class="comment"># [b_size]</span></span><br><span class="line">        loss_p =  F.logsigmoid(prod_p).mean() <span class="comment"># 1</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        prod_n = (embd_center * embd_negative_sample).<span class="built_in">sum</span>(dim=<span class="number">2</span>) <span class="comment"># [b_size, negative_sample_num]</span></span><br><span class="line">        loss_n = F.logsigmoid(-prod_n).<span class="built_in">sum</span>(dim=<span class="number">1</span>).mean() <span class="comment"># 1</span></span><br><span class="line">        <span class="keyword">return</span> -(loss_p + loss_n)</span><br></pre></td></tr></table></figure><ul><li>syn0 對應到 input side 的  matrix $W$</li><li>neg_syn1 對應到 output side 的  matrix $W’$<ul><li>Tomas Mikolov 在 WordVec c code 初始化為 0</li></ul></li><li>loss function<ul><li>loss_p 對應到 $\log \sigma(\text{v}^\top_{w_I} \text{v}’_{w_{O,j}})$</li><li>loos_n 對應到 $\sum^M_{\substack{i=1 \\ \tilde{w}_i \sim Q}}\exp(\text{v}^\top_{w_I} \text{v}’_{\tilde{w}_i})$</li></ul></li></ul><h2 id="Training-Skipgram-Negative-Sampling"><a href="#Training-Skipgram-Negative-Sampling" class="headerlink" title="Training Skipgram + Negative Sampling"></a>Training Skipgram + Negative Sampling</h2><p>訓練過程省略，參閱 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Embedding">seed9D/hands-on-machine-learning</a></p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><h3 id="取回-embedding"><a href="#取回-embedding" class="headerlink" title="取回 embedding"></a>取回 embedding</h3><p>簡單的把 syn0 跟 neg_syn1 平均</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">syn0 = model.syn0.weight.data</span><br><span class="line">neg_syn1 = model.neg_syn1.weight.data</span><br><span class="line"></span><br><span class="line">w2v_embedding = (syn0 + neg_syn1) / <span class="number">2</span></span><br><span class="line">w2v_embedding = w2v_embedding.numpy()</span><br><span class="line">l2norm = np.linalg.norm(w2v_embedding, <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">w2v_embedding = w2v_embedding / l2norm</span><br></pre></td></tr></table></figure><h3 id="Cosine-similarity"><a href="#Cosine-similarity" class="headerlink" title="Cosine similarity"></a>Cosine similarity</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineSimilarity</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word_embedding, idx_to_word_dict, word_to_idx_dict</span>):</span></span><br><span class="line">        self.word_embedding = word_embedding <span class="comment"># normed already</span></span><br><span class="line">        self.idx_to_word_dict = idx_to_word_dict</span><br><span class="line">        self.word_to_idx_dict = word_to_idx_dict</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_synonym</span>(<span class="params">self, word, topK=<span class="number">10</span></span>):</span></span><br><span class="line">        idx = self.word_to_idx_dict[word]</span><br><span class="line">        embed = self.word_embedding[idx]</span><br><span class="line">        </span><br><span class="line">        cos_similairty = w2v_embedding @ embed</span><br><span class="line">        </span><br><span class="line">        topK_index = np.argsort(-cos_similairty)[:topK]</span><br><span class="line">        pairs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> topK_index:</span><br><span class="line">            w = self.idx_to_word_dict[i]</span><br><span class="line">            pairs.append((w, cos_similairty[i]))</span><br><span class="line">        <span class="keyword">return</span> pairs</span><br></pre></td></tr></table></figure><p>訓練語料是聖經，看看 jesus  跟  christ 的相近詞</p><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosinSim = CosineSimilarity(w2v_embedding, idx_to_word, word_to_idx)</span><br><span class="line">cosinSim.get_synonym(<span class="string">&#x27;christ&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;christ&#x27;</span>, <span class="number">1.0</span>),</span><br><span class="line"> (<span class="string">&#x27;jesus&#x27;</span>, <span class="number">0.7170907</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.4621805</span>),</span><br><span class="line"> (<span class="string">&#x27;peter&#x27;</span>, <span class="number">0.39412546</span>),</span><br><span class="line"> (<span class="string">&#x27;disciples&#x27;</span>, <span class="number">0.3873747</span>),</span><br><span class="line"> (<span class="string">&#x27;noise&#x27;</span>, <span class="number">0.28152165</span>),</span><br><span class="line"> (<span class="string">&#x27;asleep&#x27;</span>, <span class="number">0.26372147</span>),</span><br><span class="line"> (<span class="string">&#x27;taught&#x27;</span>, <span class="number">0.2422184</span>),</span><br><span class="line"> (<span class="string">&#x27;zarhites&#x27;</span>, <span class="number">0.24168596</span>),</span><br><span class="line"> (<span class="string">&#x27;nobles&#x27;</span>, <span class="number">0.23950878</span>)]</span><br></pre></td></tr></table></figure><p>In:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cosinSim.get_synonym(<span class="string">&#x27;jesus&#x27;</span>)</span><br></pre></td></tr></table></figure><p>out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;jesus&#x27;</span>, <span class="number">1.0</span>),</span><br><span class="line"> (<span class="string">&#x27;christ&#x27;</span>, <span class="number">0.7170907</span>),</span><br><span class="line"> (<span class="string">&#x27;gospel&#x27;</span>, <span class="number">0.5360588</span>),</span><br><span class="line"> (<span class="string">&#x27;peter&#x27;</span>, <span class="number">0.3603956</span>),</span><br><span class="line"> (<span class="string">&#x27;disciples&#x27;</span>, <span class="number">0.3460646</span>),</span><br><span class="line"> (<span class="string">&#x27;church&#x27;</span>, <span class="number">0.2755898</span>),</span><br><span class="line"> (<span class="string">&#x27;passed&#x27;</span>, <span class="number">0.24744174</span>),</span><br><span class="line"> (<span class="string">&#x27;noise&#x27;</span>, <span class="number">0.23768528</span>),</span><br><span class="line"> (<span class="string">&#x27;preach&#x27;</span>, <span class="number">0.23454829</span>),</span><br><span class="line"> (<span class="string">&#x27;send&#x27;</span>, <span class="number">0.2337867</span>)]</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://github.com/tmikolov/word2vec">https://github.com/tmikolov/word2vec</a><ul><li>c code</li></ul></li><li>word2vec的PyTorch实现 <a href="https://samaelchen.github.io/word2vec_pytorch/">https://samaelchen.github.io/word2vec_pytorch/</a><ul><li>CBOW + NEG</li></ul></li><li><a href="https://rguigoures.github.io/word2vec_pytorch/">https://rguigoures.github.io/word2vec_pytorch/</a><ul><li>CBOW + NEG</li></ul></li><li><a href="https://github.com/ilyakhov/pytorch-word2vec">https://github.com/ilyakhov/pytorch-word2vec</a></li><li>Word2Vec Tutorial Part 2 - Negative Sampling <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li><li>基于PyTorch实现word2vec模型 <a href="https://lonepatient.top/2019/01/18/Pytorch-word2vec.html#pytorch%E5%AE%9E%E7%8E%B0">https://lonepatient.top/2019/01/18/Pytorch-word2vec.html</a></li><li>other<ul><li><a href="https://github.com/Adoni/word2vec_pytorch/blob/master/model.py">https://github.com/Adoni/word2vec_pytorch/blob/master/model.py</a></li><li><a href="http://medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4">medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec (4):Pytorch 實作 Word2Vec with Softmax</title>
      <link href="Pytorch-Implement-Naive-Word2Vec-with-Softmax/"/>
      <url>Pytorch-Implement-Naive-Word2Vec-with-Softmax/</url>
      
        <content type="html"><![CDATA[<p>用  pytorch  實現最簡單版本的  CBOW 與 skipgram，objective function 採用 minimize negative log likelihood with softmax</p><a id="more"></a><h1 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h1><p>CBOW  的思想是用兩側 context 詞預測中間 center 詞，context 詞有數個，視 window size 大小而定</p><script type="math/tex; mode=display">P(center|context;\theta)</script><p><img src="https://i.imgur.com/N0SXV8P.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><p>Condition probability $P(center | context; \theta)$ 中 variable $\textit{center word}$  有限，所以是個 descrete probability，可以轉化成多分類問題來解</p><p>令 $w_O$ 表 center word, $w_I$ 表 input 的 context word，則</p><script type="math/tex; mode=display">P(center|context;\theta) = P(w_O|w_I; \theta) =  \cfrac{\exp(h^\top \text{v}^{'}_{w_{O}})}{\sum_{w_ \in V}\exp(h^\top \text{v}'_{w_i})}</script><ul><li>$h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\cfrac{1}{C}(\text{v}_{w_1} + \text{v}_{w_2}+ …+ \text{v}_{w_C})^T$</li></ul><p>訓練過程 $\text{maximize log of condition probability } P(w_O|w_I; \theta$</p><script type="math/tex; mode=display">\begin{aligned} & \text{maxmize}_\theta  \ \log P(w_O|w_I; \theta)\\&  = \text{minimize}_\theta \ -\log \ P(w_O|w_I; \theta)\\& = \text{minimize}_\theta  \ - \log \cfrac{\exp(h^\top \text{v}^{'}_{w_{O}})}{\sum_{w_i  \in V} \exp(h^\top \text{v}^{'}_{w_i})} \end{aligned}</script><h2 id="Pytorch-CBOW-softmax"><a href="#Pytorch-CBOW-softmax" class="headerlink" title="Pytorch  CBOW + softmax"></a>Pytorch  CBOW + softmax</h2><h3 id="CBOW-softmax-模型定義"><a href="#CBOW-softmax-模型定義" class="headerlink" title="CBOW + softmax 模型定義"></a>CBOW + softmax 模型定義</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOWSoftmax</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.syn1 = nn.Linear(embedding_dim, vocab_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, context, center</span>):</span></span><br><span class="line">        <span class="comment">#  context: [b_size, windows_size]</span></span><br><span class="line">        <span class="comment">#  center: [b_size, 1]</span></span><br><span class="line">        embds = self.syn0(context).mean(dim=<span class="number">1</span>) <span class="comment"># [b_size, embedding_dim]</span></span><br><span class="line">        out = self.syn1(embds)</span><br><span class="line">        </span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        loss = F.nll_loss(log_probs, center.view(-<span class="number">1</span>), reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li><p>syn0 對應到 input 側的 embedding matrix $W$</p></li><li><p>syn1 對應到 output 側的 embedding matrix  $W’$</p></li><li><p>loss 的計算</p><p>$- log \cfrac{\exp(h^\top \text{v}^{‘}_{w_{O}})}{\sum_{w_i  \in V} \exp(h^\top \text{v}^{‘}_{w_i})}$</p></li><li><p>input: context  跟 center 內容都是將 word index  化</p><p><img src="https://i.imgur.com/1PILCdT.png" style="zoom: 50%;" /></p></li><li><p>因爲 context 是由 windows size N 個 words 組成，所以總共有 N 個 word embedding ，常規操作是 sum or  mean</p></li></ul><h3 id="Training-Stage"><a href="#Training-Stage" class="headerlink" title="Training Stage"></a>Training Stage</h3><p>訓練過程省略，有興趣的可以去 github 看 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/blob/main/Embedding/CBOW_softmax.ipynb">seed9D/hands-on-machine-learning</a></p><h3 id="取出-Embedding"><a href="#取出-Embedding" class="headerlink" title="取出 Embedding"></a>取出 Embedding</h3><p>創建一個衡量 cosine similarity的 class</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineSimilarity</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word_embedding, idx_to_word_dict, word_to_idx_dict</span>):</span></span><br><span class="line">        self.word_embedding = word_embedding <span class="comment"># normed already</span></span><br><span class="line">        self.idx_to_word_dict = idx_to_word_dict</span><br><span class="line">        self.word_to_idx_dict = word_to_idx_dict</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_synonym</span>(<span class="params">self, word, topK=<span class="number">10</span></span>):</span></span><br><span class="line">        idx = self.word_to_idx_dict[word]</span><br><span class="line">        embed = self.word_embedding[idx]</span><br><span class="line">        </span><br><span class="line">        cos_similairty = w2v_embedding @ embed</span><br><span class="line">        </span><br><span class="line">        topK_index = np.argsort(-cos_similairty)[:topK]</span><br><span class="line">        pairs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> topK_index:</span><br><span class="line">            w = self.idx_to_word_dict[i]</span><br><span class="line"><span class="comment">#             pairs[w] = cos_similairty[i]</span></span><br><span class="line">            pairs.append((w, cos_similairty[i]))</span><br><span class="line">        <span class="keyword">return</span> pairs</span><br></pre></td></tr></table></figure><p>僅使用 syn0 做為 embedding，記得 L2 norm </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">syn0 = model.syn0.weight.data</span><br><span class="line"></span><br><span class="line">w2v_embedding = syn0 </span><br><span class="line">w2v_embedding = w2v_embedding.numpy()</span><br><span class="line">l2norm = np.linalg.norm(w2v_embedding, <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">w2v_embedding = w2v_embedding / l2norm</span><br></pre></td></tr></table></figure><p>訓練的 corpus 是聖經，所以簡單看下 jesus 與 christ 兩個 word 的相似詞，效果不予置評</p><p><img src="https://i.imgur.com/W2w6p65.png" alt="Pytorch%20Implement%20Naive%20Word2Vec%20with%20Softmax%20ae605d15ce0e403694f9d8049c1f2354/Untitled%202.png"></p><h1 id="Skipgram"><a href="#Skipgram" class="headerlink" title="Skipgram"></a>Skipgram</h1><p>skipgram  的思想是用中心詞 center word 去預測兩側的 context words </p><script type="math/tex; mode=display">P(context|center; \theta)</script><p><img src="https://i.imgur.com/AoD1UHk.png" style="zoom:50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><p>令 $w_I$ 表 input 的 center word， $w_{O,j}$ 表 target 的 第  $j$  個 context word ，則 condition  probability</p><script type="math/tex; mode=display">P(context|center;\theta) = P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) = \prod^C_{c=1 }\cfrac{\exp(h^\top \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(h^\top \text{v}'_{w_i})}</script><ul><li>$h$ 表 hidden layer 的輸出，在 skipgram 實際上就是 $\text{v}_{w_I}$</li></ul><p>Skipgram  的  objective function</p><script type="math/tex; mode=display">\begin{aligned} & -\log P(w_{O,1}, w_{O,2},...,w_{O,C}|w_I) \\ & = -\log \prod^C_{c=1}\cfrac{\exp(h^\top \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(h^{\top} \text{v}'_{w_i})}\\ & = -\log \prod^C_{c=1}\cfrac{\exp(\text{v}^\top_{w_I} \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(\text{v}^\top_{w_I} \text{v}'_{w_i})}\\& = -\sum^C_{c=1}\log \cfrac{\exp(\text{v}^\top_{w_I} \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(\text{v}^\top_{w_I} \text{v}'_{w_i})}\end{aligned}</script><h2 id="Pytorch-skipgram-softmax"><a href="#Pytorch-skipgram-softmax" class="headerlink" title="Pytorch  skipgram + softmax"></a>Pytorch  skipgram + softmax</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SkipgramSoftmax</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embedding_dim = embedding_dim</span><br><span class="line">        self.syn0 = nn.Embedding(vocab_size, embedding_dim)  <span class="comment"># |V| x |K|</span></span><br><span class="line">        self.syn1 = nn.Linear(embedding_dim, vocab_size)  <span class="comment"># |K| x |V|</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, center, context</span>):</span></span><br><span class="line">        <span class="comment"># center: [b_size, 1]</span></span><br><span class="line">        <span class="comment"># context: [b_size, 1]</span></span><br><span class="line">        embds = self.syn0(center.view(-<span class="number">1</span>))</span><br><span class="line">        out = self.syn1(embds)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        loss = F.nll_loss(log_probs, context.view(-<span class="number">1</span>), reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><ul><li>syn0 對應到 input 側的 embedding matrix $W$</li><li>syn1 對應到 output 側的 embedding matrix  $W’$</li></ul><p>實際上，skipgram 每筆 training data 只需要 (a center word, a context word) 的 pair 即可</p><p><img src="https://i.imgur.com/dsXqMKo.png" style="zoom:50%;" /></p><p> 所以  loss function 實現上非常簡單</p><script type="math/tex; mode=display">-\log \cfrac{\exp(\text{v}^\top_{w_I} \text{v}'_{w_{O,c}})}{\sum_{w_i \in V} \exp(\text{v}^\top_{w_I} \text{v}'_{w_i})}</script><h3 id="Training-Stage-1"><a href="#Training-Stage-1" class="headerlink" title="Training Stage"></a>Training Stage</h3><p>訓練過程省略，有興趣的可以去 github 看 notebook</p><p><a href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Embedding">seed9D/hands-on-machine-learning</a></p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>取出 embedding，這次 embedding  嘗試 $(W + W’)/2$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">syn0 = model.syn0.weight.data</span><br><span class="line">syn1 = model.syn1.weight.data</span><br><span class="line"></span><br><span class="line">w2v_embedding = (syn0 + syn1) / <span class="number">2</span></span><br><span class="line">w2v_embedding = w2v_embedding.numpy()</span><br><span class="line">l2norm = np.linalg.norm(w2v_embedding, <span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">w2v_embedding = w2v_embedding / l2norm</span><br></pre></td></tr></table></figure><p>一樣看 jesus 跟 christ 的相似詞，感覺似乎比 CBOW 好一點</p><p><img src="https://i.imgur.com/2pS6zCo.png" style="zoom:50%;" /></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html</a></li><li><a href="https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb">https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb</a></li><li>基于PyTorch实现word2vec模型 <a href="https://lonepatient.top/2019/01/18/Pytorch-word2vec.htm">https://lonepatient.top/2019/01/18/Pytorch-word2vec.htm</a></li><li>Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from <a href="http://arxiv.org/abs/1411.2738">http://arxiv.org/abs/1411.2738</a></li><li><a href="https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py">https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec (3):Negative Sampling 背後的數學</title>
      <link href="negative-sampling-in-word2vec/"/>
      <url>negative-sampling-in-word2vec/</url>
      
        <content type="html"><![CDATA[<p>以下用 Skip-gram 為例</p><p><img src="https://i.imgur.com/YeDgnQ9.png" style="zoom: 50%;" /></p><a id="more"></a><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h2 id="Noise-Contrastive-Estimation-NCE"><a href="#Noise-Contrastive-Estimation-NCE" class="headerlink" title="Noise Contrastive Estimation (NCE)"></a>Noise Contrastive Estimation (NCE)</h2><blockquote><p>NCE attempts to approximately maximize the log probability of the softmax output</p></blockquote><ul><li>The Noise Contractive Estimation metric intends to differentiate the target word from noise sample using a logistic regression classifier</li></ul><p><img src="https://i.imgur.com/m2gAqLM.png" style="zoom: 50%;" /></p><h3 id="從-cross-entropy-說起"><a href="#從-cross-entropy-說起" class="headerlink" title="從 cross entropy 說起"></a>從 cross entropy 說起</h3><p> True label $y_i$ is 1 only when $w_i$ is the output word:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - \sum_{i=1}^V y_i \log p(w_i | w_I) = - \log p(w_O \vert w_I)</script><p>又</p><p>$p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ </p><ul><li>$\text{v}^{‘}_{w_i}$ is  vector of word $w_i$ in $W^{‘}$</li><li>$w_O$ is the output word in $V$</li><li>$w_I$ is the input word in $V$</li></ul><p>代入後</p><script type="math/tex; mode=display">\mathcal{L}_{\theta} = - \log \frac{\exp({\text{v}'_{w_O}}^{\top}{\text{v}_{w_I}})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top}{\text{v}_{w_I} })}= - {\text{v}'_{w_O}}^{\top}{\text{v}_{w_I} } + \log \sum_{i=1}^V \exp({\text{v}'_{w_i} }^{\top}{\text{v}_{w_I}})</script><p>Compute gradient of loss function w.s.t mode’s parameter $\theta$，令 $z_{IO} = {\text{v}’_{w_O}}^{\top}{\text{v}_{w_I}}$ ; $z_{Ii} = {\text{v}’_{w_i}}^{\top}{\text{v}_{w_I}}$</p><script type="math/tex; mode=display">\begin{aligned}\nabla_\theta \mathcal{L}_{\theta}&= \nabla_\theta\big( - z_{IO} + \log \sum_{i=1}^V e^{z_{Ii}} \big) \\ &= - \nabla_\theta z_{IO} + \nabla_\theta \big( \log \sum_{i=1}^V e^{z_{Ii}} \big) \\&= - \nabla_\theta z_{IO} + \frac{1}{\sum_{i=1}^V e^{z_{Ii}}} \sum_{i=1}^V e^{z_{Ii}} \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \sum_{i=1}^V \frac{e^{z_{Ii}}}{\sum_{i=1}^V e^{z_{Ii}}} \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta z_{Ii} \\&= - \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta z_{Ii}\end{aligned}</script><p>可以看出，gradient $\nabla_{\theta}\mathcal{L}_{\theta}$是由兩部分組成 :</p><ol><li>a positive reinforcement for the target word $w_O$,  $\nabla_{\theta}z_{O}$</li><li>a negative reinforcement for all other words $w_i$, which weighted by their probability, $\sum_{i=1}^V p(w_i \vert w_I) \nabla_\theta z_{Ii}$</li></ol><p><strong>Second term actually is just the expectation  of the gradient $\nabla_{\theta}z_{Ii}$ for all words $w_i$ in $V$。</strong></p><p>And probability distribution $Q(\tilde{w})$ could see as the distribution of noise samples</p><h3 id="NCE-sample-原理"><a href="#NCE-sample-原理" class="headerlink" title="NCE sample 原理"></a>NCE sample 原理</h3><p>According to gradient of loss function $\nabla_{\theta}\mathcal{L}_{\theta}$</p><script type="math/tex; mode=display">\nabla_{\theta}\mathcal{L}_{\theta} =- \nabla_\theta z_{IO} + \mathbb{E}_{w_i \sim Q(\tilde{w})} \nabla_\theta z_{Ii}</script><p>Given an input word $w_I$, the correct output word is known as $w$，我們同時可以從 noise sample distribution $Q$  sample 出 $M$ 個 samples $\tilde{w}_1<br>, \tilde{w}_2, \dots, \tilde{w}_M \sim Q$ 來近似 cross entropy gradient 的後半部分</p><p>現在我們手上有個 correct output word $w$ 和 $M$ 個從 $Q$ sample 出來的 word $\tilde{w}$ ， 假設我們有一個  binary classifier 負責告訴我們，手上的 word $w$，哪個是 correct  $p(d=1 | w, w_I)$，哪個是 negative $p(d=0 | \tilde{w}, w_I)$</p><p>於是 loss function  改寫成：</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) + \sum_{i=1, \tilde{w}_i \sim Q}^M \log p(d=0|\tilde{w}_i, w_I) ]</script><p>According to the law of large numbers $E_{p(x)} [ f(x)] \approx \frac{1}{n} \sum^{n}_{i=1}f(x_i)$，we could simplify:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log p(d=1 \vert w, w_I) +  M \mathbb{E}_{\tilde{w}_i \sim Q} \log p(d=0|\tilde{w}_i, w_I)]</script><p>$p(d |w, w_I)$ 可以從 joint probability $p(d, w|w_I)$ 求</p><ol><li><p>$p(d, w | w_I) =<br>\begin{cases}<br>  \frac{1}{M+1} p(w \vert w_I) &amp; \text{if } d=1 \\<br>  \frac{M}{M+1} q(\tilde{w}) &amp; \text{if } d=0<br>  \end{cases}$</p><ul><li>$d$  is binary value</li><li>$M$ is number of samples，we have a correct output word $w$ and sample M word from distribution $Q$</li></ul></li><li><p>因爲 $p(d| w, w_I) = \frac{p(d, w, w_I)}{p(w, w_I)} = \frac{p(d, w | w_I) p(w_I)}{p(w|w_I)p(w_I)} = \frac{p(d, w| w_I)}{\sum_dp(d,w| w_I)}$</p><p>可以得出</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{Mq(\tilde{w})}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script></li></ol><ul><li>$q(\tilde{w})$ 表從 distribution $Q$ sample 出 word $\tilde{w}$ 的 probability</li></ul><p>最終 loss function of NCE</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_\theta & = - [ \log p(d=1 \vert w, w_I) +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log p(d=0|\tilde{w}_i, w_I)] \\& = - [ \log \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})} +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{p(w \vert w_I) + Mq(\tilde{w}_i)}]\end{aligned}</script><p>$p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 代入 $\mathcal{L}_{\theta}$</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\theta} &= -[log\frac{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})}}{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})+ Mq(\tilde{w})}} + \sum_{\substack{i=1 \\ \tilde{w}_i\sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{\frac{\exp({\text{v}'_{w}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}'_{w_i}}^{\top} \text{v}_{w_I})}+ Mq(\tilde{w}_i)}]\end{aligned}</script><p>可以看到 normalizer $Z(w) = {\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 依然要 sum up the entire vocabulary，實務上通常會假設 $Z(w)\approx1$，所以 $\mathcal{L}_\theta$ 簡化成:</p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \frac{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}})}{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}}) + Mq(\tilde{w})} +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \frac{Mq(\tilde{w}_i)}{\exp({\text{v}'_w}^{\top}{\text{v}_{w_I}}) + Mq(\tilde{w}_i)}]</script><h3 id="關於-Noise-distribution-Q"><a href="#關於-Noise-distribution-Q" class="headerlink" title="關於 Noise distribution $Q$"></a>關於 Noise distribution $Q$</h3><p>關於 noise distribution $Q$，在設計的時候通常會考慮</p><ul><li>it should intuitively be very similar to the real data distribution.</li><li>it should be easy to sample from.</li></ul><h2 id="Negative-Sampling-NEG"><a href="#Negative-Sampling-NEG" class="headerlink" title="Negative Sampling (NEG)"></a>Negative Sampling (NEG)</h2><p>Negative sampling can be seen as an approximation to NCE</p><ul><li>Noise Contrastive Estimation attempts to approximately maximize the log probability of the softmax output</li><li>The objective of NEG is to <strong>learn high-quality word representations</strong> rather than achieving low perplexity on a test set, as is the goal in language modeling</li></ul><h3 id="從-NCE-說起"><a href="#從-NCE-說起" class="headerlink" title="從 NCE 說起"></a>從 NCE 說起</h3><p>NCE  $p(d|w, w_I)$ equation，$p(d=1 |w, w_I)$ 代表 word $w$ 是 output word 的機率; $p(d=0|w, w_I)$ 表 sample 出 noise word 的機率</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(d=1, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}p(d=0 \vert w, w_I) &= \frac{p(d=0, w \vert w_I)}{p(d=1, w \vert w_I) + p(d=0, w \vert w_I)}&= \frac{Mq(\tilde{w})}{p(w \vert w_I) + Mq(\tilde{w})}\end{aligned}</script><p>NCE 假設  $p(w_O \vert w_I) = \frac{\exp({\text{v}’_{w_O}}^{\top} \text{v}_{w_I})}{\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})}$ 中的分母 $Z(w) = {\sum_{i=1}^V \exp({\text{v}’_{w_i}}^{\top} \text{v}_{w_I})} = 1$，所以簡化成</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &= \frac{p(w \vert w_I)}{p(w \vert w_I) + Mq(\tilde{w})}&= \frac{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i})}{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) + Mq(\tilde{w})}\end{aligned}</script><h3 id="NEG-繼續化簡"><a href="#NEG-繼續化簡" class="headerlink" title="NEG 繼續化簡"></a>NEG 繼續化簡</h3><p><strong>NEG 繼續假設 $Nq(\tilde{w}) = 1$</strong> 式子變成</p><script type="math/tex; mode=display">\begin{aligned}p(d=1 \vert w, w_I) &=\frac{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i})}{\exp({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) + 1}&=\frac{1}{1  +\exp(-{\text{v}^{'}_w}^{\top}\text{v}_{w_i})} = \sigma({\text{v}^{'}_w}^{\top}\text{v}_{w_i})\end{aligned}</script><script type="math/tex; mode=display">p(d=0|w, w_I) = 1 - \sigma({\text{v}^{'}_w}^{\top}\text{v}_{w_i}) = \sigma(-{\text{v}^{'}_w}^{\top}\text{v}_{w_i})</script><p>最終得到 loss function </p><script type="math/tex; mode=display">\mathcal{L}_\theta = - [ \log \sigma({\text{v}'_{w}}^\top \text{v}_{w_I}) +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^M \log \sigma(-{\text{v}'_{\tilde{w}_i}}^\top \text{v}_{w_I})]</script><p>前項是 positive sample $p(d=1 \vert w, w_I)$，後項是 M 個 negative samples $p(d=0|w, w_I) $</p><p>在 skipgram with negative sampling 上</p><ul><li>$\text{v}_{w_I}$ 表 input 的 center word  $w_I$ 的 vector，來自 $W$</li><li>$\text{v}’_{w}$ 表 output side  的一個 context word  $w$ 的 vector， 來自 $W’$</li></ul><p>實作上  skipgram 一個單位的 data sample 只會包含 一個 center word 與 一個 context word  的 pair 對 $(w_I, w_{C,j})$</p><p>參閱  <a href="/Pytorch-Implement-Skipgram-with-Negative-Sampling/" title="Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling">Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling</a></p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><ul><li>NEG 實際上目的跟 Hierarchical Softmax 不一樣，並不直接學 likelihood of correct word，而是直接學 words 的 embedding，也就是更好的 word distribution representation</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Learning Word Embedding <a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#loss-functions</a><ul><li>此篇從 skip gram 講解 negative sampling</li></ul></li><li>On word embeddings - Part 2: Approximating the Softmax <a href="https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches">https://ruder.io/word-embeddings-softmax/index.html#samplingbasedapproaches</a><ul><li>此篇從 CBOW 講解 negative sampling</li></ul></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722">http://arxiv.org/abs/1402.3722</a></li><li>Word2Vec Tutorial Part 2 - Negative Sampling [<a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec (2):Hierarchical Softmax 背後的數學</title>
      <link href="hierarchical-softmax-in-word2vec/"/>
      <url>hierarchical-softmax-in-word2vec/</url>
      
        <content type="html"><![CDATA[<h1 id="以-CBOW-為例"><a href="#以-CBOW-為例" class="headerlink" title="以 CBOW 為例"></a>以 CBOW 為例</h1><p><img src="https://i.imgur.com/Pbdqtx9.png" style="zoom:50%;" /></p><a id="more"></a><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>$\text{v}’_j$ 表 $W’$  中  j-th columns  vector</li><li>在 Hierarchical softmax 中， $W’$ each column 表 huffman tree 的 non leaf node 的 vector 而不是  leaf node  ，跟 column vector $\text{v}’_j$ 與 word $w_i$  不是直接對應的關係</li></ul></li></ul><p>Hierarchical Softmax 優化了原始 softmax 分母 normalization 項需要對整個 vocabulary set 內的所有 word 內積，原始 softmax 如下</p><script type="math/tex; mode=display">p(w | c) = \dfrac{\text{exp}({h^\top \text{v}'_w})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><ul><li>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$，is average of input context words’ vector representation in $W$</li></ul><p>Hierarchical Softmax build a full binary tree to avoid computation over all vocabulary，示意圖：</p><p><img src="https://i.imgur.com/WnaPO5L.png" style="zoom:50%;" /></p><ol><li>每個 leaf node 代表一個 word $w_i$</li><li>Matrix $W^{‘}$  就是所有 non-leaf node $n$ 代表的 vector $\text{v}^{‘}_n$ 集合，與 word $w_i$ 無一對一對應關係</li></ol><p>Binary tree 中每個 node 分岔的 probability 是個 binary classification problem</p><p>$p(n, \text{left}) = \sigma({\text{v}’_n}^{\top} h)$</p><p>$p(n, \text{righ}) = 1 - p(\text{left},n) = \sigma(-{\text{v}’_n}^{\top} h)$</p><ul><li>$\text{v}^{‘}_{n}$  代表 node $n$ 的 vector</li></ul><p>則每個 word $w_O = w_i$ 的 generate probability $p(w_i)$ 為其從 root node 分岔到 lead node 的 probability</p><p>$p(w_i = w_O) = \prod^{L(w_O)-1}_{j=1} \sigma(\mathbb{I}_{\text{turn}}(n(w_O, j), n(w_O, j + 1) \cdot {v^{‘}_{n(w_O, j)}}^{\top}h)$</p><ul><li><p>$w_O$ 表 output word 的意思</p></li><li><p>$L(w_O)$ is the depth of the path leading to the output word $w_O$</p></li><li><p>$\mathbb{I}_{turn}$  is a specially indicator function</p><p>1 if $n(w_O, k+1)$ is the <strong>left</strong> child of $n(w_O, k)$</p><p>-1 if $n(w_O, k+1)$ is the <strong>right</strong> child of $n(w_O, k)$</p></li><li><p>$n(w, j)$ means the $j$ th unit on the path from root to the word $w$</p></li><li><p>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$ average of all context word vector</p></li></ul><h2 id="簡單的例子"><a href="#簡單的例子" class="headerlink" title="簡單的例子"></a>簡單的例子</h2><p>Ex 1: Following the path from the root to leaf node of $w_2$, we can compute the probability of $w_2$ $p(w_2)$:</p><p><img src="https://i.imgur.com/swoUyHO.png" style="zoom:50%;" /></p><p>Ex 2:</p><p><img src="https://i.imgur.com/oomh3Bv.png" alt="" style="zoom: 33%;" /></p><ul><li>$\sum^{V}_{i=1} p(w_i = w_O) = 1$</li></ul><p>probability $p(\text{cat}| context)$,  是由 $ node1  \stackrel{\text{left}}{\to} node \stackrel{\text{right}}{\to}  node 5 \stackrel{\text{right}}{\to}  cat  $  這條路徑組成</p><p>其中 context words  經過  hidden layer 後的輸出為 $h(\text{context words})$</p><h2 id="為什麼-Hierarchical-Softmax-可以減少-Time-Complexity"><a href="#為什麼-Hierarchical-Softmax-可以減少-Time-Complexity" class="headerlink" title="為什麼 Hierarchical Softmax 可以減少 Time Complexity?"></a>為什麼 Hierarchical Softmax 可以減少 Time Complexity?</h2><p>透過  Hierarchical Softmax ， 原本計算 $p(w|c)$  需要求所有 word  $w_i$ 的 vector $\text{v}_{w_i}$對 $h$ 的內積，現在只需要計算路徑上的節點數 $L(w_i) -1$，又 HS 是 full binary tree ，其最大深度為 $\log_2|V|$</p><p><strong>So we only need to evaluate at most $log_2|V|$</strong></p><h1 id="Hierarchical-Softmax-如何-update-參數"><a href="#Hierarchical-Softmax-如何-update-參數" class="headerlink" title="Hierarchical Softmax 如何 update 參數"></a>Hierarchical Softmax 如何 update 參數</h1><h2 id="Error-Funtion-of-Hierarchical-Softmax"><a href="#Error-Funtion-of-Hierarchical-Softmax" class="headerlink" title="Error Funtion of Hierarchical Softmax"></a>Error Funtion of Hierarchical Softmax</h2><p>Error function $E$ is negative log likelihood</p><p><img src="https://i.imgur.com/vYCY3GR.png" style="zoom:50%;" /></p><p><img src="https://i.imgur.com/lmxXtuJ.png" style="zoom:50%;" /></p><ul><li>$L(w_i) -1$ 表 從 root node  到 leaf node of $w_i$ 的 node number</li><li>$[ \cdot ]$表分岔判斷</li><li>$h = \frac {1}{C} \sum^{C}_{c=1}\text{v}_{w_c}$ average of all context word vector</li></ul><p>And we use <strong>gradient decent</strong> to update $\text{v}^{‘}_j$  and $h$ in $W’$ and $W’$</p><h2 id="Calculate-the-Derivate-E-with-Regard-to-text-v-‘-top-jh"><a href="#Calculate-the-Derivate-E-with-Regard-to-text-v-‘-top-jh" class="headerlink" title="Calculate the Derivate $E$ with Regard to  $\text{v}^{‘\top}_jh$"></a>Calculate the Derivate $E$ with Regard to  $\text{v}^{‘\top}_jh$</h2><p>先求 total loss 對 $\text{v}^{‘\top}_jh$ 的 gradient</p><p><img src="https://i.imgur.com/JejjNsl.png" style="zoom:50%;" /></p><ul><li>$\sigma^{‘}(x) = \sigma(x)[1 - \sigma(x)]$</li><li>$[\log\sigma(x)]^{‘} = 1 - \sigma(x)$   ⇒    $[log(1 - \sigma(x)]^{‘} = -\sigma(x)$</li></ul><h2 id="Calculate-the-Derivate-E-with-Regard-to-text-v-‘-j"><a href="#Calculate-the-Derivate-E-with-Regard-to-text-v-‘-j" class="headerlink" title="Calculate the Derivate $E$ with Regard to  $\text{v}^{‘}_j$"></a>Calculate the Derivate $E$ with Regard to  $\text{v}^{‘}_j$</h2><p>根據 chain rule 可以求出 total loss 對 huffman tree node vector $\text{v}^{‘}_j$ 的 gradient</p><p><img src="https://i.imgur.com/RbiLvNW.png" alt="" style="zoom:50%;" /></p><h3 id="Update-Equation"><a href="#Update-Equation" class="headerlink" title="Update Equation"></a>Update Equation</h3><p><img src="https://i.imgur.com/hLzWOpf.png" style="zoom:50%;" /></p><h2 id="Calculate-the-Derivate-E-with-Regard-to-h"><a href="#Calculate-the-Derivate-E-with-Regard-to-h" class="headerlink" title="Calculate the Derivate $E$ with Regard to $h$"></a>Calculate the Derivate $E$ with Regard to $h$</h2><p> 最後求 total loss 對 hidden layer outpot $h$ 的 gradient</p><p><img src="https://i.imgur.com/BLGopRf.png" style="zoom:50%;" /></p><ul><li>$EH$: an N-dim vector, is the sum of the output vector of all words in the vocabulary, weighted by their prediction error</li></ul><h3 id="Update-Equation-1"><a href="#Update-Equation-1" class="headerlink" title="Update Equation"></a>Update Equation</h3><p>Because hidden vector $h$ is composed with all the context word $w_{I,c}$</p><p><img src="https://i.imgur.com/W4aEVVm.png" style="zoom:50%;" /></p><ul><li>$\text{v}_{w_{I,c}}$ is the input vector of c-th word in input context</li></ul><h1 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h1><p>CBOW + HS 實現  <a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax">Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Language Models, Word2Vec, and Efficient Softmax Approximations <a href="https://rohanvarma.me/Word2Vec/">https://rohanvarma.me/Word2Vec/</a></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722">http://arxiv.org/abs/1402.3722</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec (1):NLP Language Model</title>
      <link href="NLP-language-model/"/>
      <url>NLP-language-model/</url>
      
        <content type="html"><![CDATA[<h1 id="General-Form"><a href="#General-Form" class="headerlink" title="General Form"></a>General Form</h1><script type="math/tex; mode=display">p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_1)</script><p>展開來後 $P(w_1, w_2, w_3,…,w_T) = P(w_1)P(x_2|w_1)P(w_3|w_2, w_1)…P(w_T|w_1,…w_{T-1})$</p><a id="more"></a><ul><li>EX</li></ul><p><img src="https://i.imgur.com/4JCLrKg.png" alt="" style="zoom:50%;" /></p><h1 id="Ngram-Model"><a href="#Ngram-Model" class="headerlink" title="Ngram Model"></a>Ngram Model</h1><p>根據 Markov assumption， word $i$ 只跟其包含 $i$ 的連續 $n$ 個 word 有關, 即前面 $n -1$ 個 word</p><script type="math/tex; mode=display">p(w_1 , \cdots , w_T) = \prod\limits_i p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1})</script><p>其中 conditional probability 統計 corpus 內的 word frequency，可以看到 ngram 只跟前面 $n-1$ 個 word 有關</p><script type="math/tex; mode=display">p(w_i \: | \: w_{i-1} , \cdots , w_{i-n+1}) = \dfrac{count(w_{i-n+1}, \cdots , w_{i-1},w_o)}{count({w_{i-n+1}, \cdots , w_{i-1}})}</script><p>如果 $k=2$ 則稱為 bigram model :</p><script type="math/tex; mode=display">p(w_i|w_1, w_2, ... w_{i-1}) \approx p(w_i|w_{i-1})</script><p>最簡單的實作直接統計 corpus 內的 word frequency 得到 conditional probability:</p><p><img src="https://i.imgur.com/C3DgaYi.png" alt="2 gram model" style="zoom:50%;" /></p><p>但通常泛化性不足，所以用 neural network 與 softmax 來泛化 n gram conditional probability  $p(w_i \: | \: w_{i-1} , \cdots , w_{i-i+1})$</p><h1 id="Neural-Network-Implementation"><a href="#Neural-Network-Implementation" class="headerlink" title="Neural Network Implementation"></a>Neural Network Implementation</h1><p>In neural network, we achieve the same objective using the softmax layer</p><p><img src="https://i.imgur.com/evkVSOl.png" style="zoom: 67%;" /></p><p>$p(w_t \: | \: w_{t-1} , \cdots , w_{t-n+1}) = \dfrac{\text{exp}({h^\top v’_{w_t}})}{\sum_{w_i \in V} \text{exp}({h^\top v’_{w_i}})}$</p><ul><li>$h$ is the output vector of the penultimate network layer</li><li>$v^{‘}_{w}$ is the output embedding of word $w$</li><li>the inner product $h^\top v’_{w_t}$ computes the unnormalized log probability of word $w_t$</li><li>the denominator normalizes log probability by sum of the log-probabilities of all word in $V$</li></ul><h1 id="Implement-Ngram-model-with-Pytorch"><a href="#Implement-Ngram-model-with-Pytorch" class="headerlink" title="Implement Ngram model with Pytorch"></a>Implement Ngram model with Pytorch</h1><h3 id="Creating-Corpus-and-Training-Pairs"><a href="#Creating-Corpus-and-Training-Pairs" class="headerlink" title="Creating Corpus and Training Pairs"></a>Creating Corpus and Training Pairs</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">test_sentence = <span class="string">&quot;&quot;&quot;When forty winters shall besiege thy brow,</span></span><br><span class="line"><span class="string">And dig deep trenches in thy beauty&#x27;s field,</span></span><br><span class="line"><span class="string">Thy youth&#x27;s proud livery so gazed on now,</span></span><br><span class="line"><span class="string">Will be a totter&#x27;d weed of small worth held:</span></span><br><span class="line"><span class="string">Then being asked, where all thy beauty lies,</span></span><br><span class="line"><span class="string">Where all the treasure of thy lusty days;</span></span><br><span class="line"><span class="string">To say, within thine own deep sunken eyes,</span></span><br><span class="line"><span class="string">Were an all-eating shame, and thriftless praise.</span></span><br><span class="line"><span class="string">How much more praise deserv&#x27;d thy beauty&#x27;s use,</span></span><br><span class="line"><span class="string">If thou couldst answer &#x27;This fair child of mine</span></span><br><span class="line"><span class="string">Shall sum my count, and make my old excuse,&#x27;</span></span><br><span class="line"><span class="string">Proving his beauty by succession thine!</span></span><br><span class="line"><span class="string">This were to be new made when thou art old,</span></span><br><span class="line"><span class="string">And see thy blood warm when thou feel&#x27;st it cold.&quot;&quot;&quot;</span>.split()</span><br><span class="line"></span><br><span class="line">trigrams = [([test_sentence[i], test_sentence[i + <span class="number">1</span>]], test_sentence[i + <span class="number">2</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_sentence) - <span class="number">2</span>)]</span><br><span class="line">vocab = <span class="built_in">set</span>(test_sentence)</span><br><span class="line">word_to_idx = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br></pre></td></tr></table></figure><h3 id="Define-N-Gram-Model"><a href="#Define-N-Gram-Model" class="headerlink" title="Define N Gram Model"></a>Define N Gram Model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NGramLanguageModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NGramLanguageModel, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line"></span><br><span class="line">        out = self.linear2(out)</span><br><span class="line"></span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br></pre></td></tr></table></figure><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">10</span></span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">net = NGramLanguageModel(<span class="built_in">len</span>(vocab), EMBEDDING_DIM, CONTEXT_SIZE)</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> trigrams:</span><br><span class="line">        context_idxs = torch.tensor([word_to_idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> context], dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        </span><br><span class="line">        log_probs = net(context_idxs)</span><br><span class="line">        loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.data</span><br><span class="line">    print(<span class="string">&quot;epcoh &#123;&#125; loss &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch, total_loss))</span><br><span class="line">    losses.append(total_loss)</span><br></pre></td></tr></table></figure><h3 id="Fetch-Embedding"><a href="#Fetch-Embedding" class="headerlink" title="Fetch Embedding"></a>Fetch Embedding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">emb = net.embeddings(torch.tensor([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vocab))])).detach().numpy()</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>on word embeddings <a href="https://ruder.io/word-embeddings-1/">https://ruder.io/word-embeddings-1/</a></li><li><a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html">https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec (0):從原理到實現</title>
      <link href="word2vec-from-theory-2-implement/"/>
      <url>word2vec-from-theory-2-implement/</url>
      
        <content type="html"><![CDATA[<p>這篇是在 notion 整理的筆記大綱，只提供綱要性的說明</p><h1 id="預備知識"><a href="#預備知識" class="headerlink" title="預備知識"></a>預備知識</h1><ul><li><p>language model： NLP 語言模型</p><p>參閱 <a href="/NLP-language-model/" title="Word2Vec (1):NLP Language Model">Word2Vec (1):NLP Language Model</a></p></li><li><p>huffman tree</p></li></ul><h1 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h1><h2 id="兩種網路結構"><a href="#兩種網路結構" class="headerlink" title="兩種網路結構"></a>兩種網路結構</h2><p><img src="https://i.imgur.com/395NfQN.png" alt="CBOW and skipgram" style="zoom:67%;" /></p><a id="more"></a><h3 id="Continuous-bag-of-words-CBOW-amp-Softmax"><a href="#Continuous-bag-of-words-CBOW-amp-Softmax" class="headerlink" title="Continuous bag of words (CBOW) &amp; Softmax"></a>Continuous bag of words (CBOW) &amp; Softmax</h3><blockquote><p>CBOW feeds $n$ words around the target word $w_t$ at each step</p></blockquote><script type="math/tex; mode=display">P(center|context;\theta)</script><p><img src="https://i.imgur.com/JTnC7Ko.png" alt="CBOW" style="zoom: 50%;" /></p><ul><li>$V$： the vocabulary size</li><li>$N$ : the embedding dimension</li><li>$W$： the input side matrix which is  $V \times N$<ul><li>each row is the $N$ dimension vector</li><li>$\text{v}_{w_i}$ is the representation of the input word $w_i$</li></ul></li><li>$W’$: the output side matrix  which is $N \times V$<ul><li>each column is the $N$ dimension vector</li><li>$\text{v}^{‘}_{w_j}$ is the j-th column of the matrix $W’$ representing $w_j$</li></ul></li></ul><h4 id="CBOW-的-Objective-Function"><a href="#CBOW-的-Objective-Function" class="headerlink" title="CBOW 的 Objective Function"></a>CBOW 的 Objective Function</h4><p>$J_\theta = \frac{1}{V}\sum\limits_{t=1}^V\ \text{log} \space p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})$</p><p>其中</p><script type="math/tex; mode=display">p(w_t \: | \: w_{t-n} , \cdots , w_{t-1}, w_{t+1}, \cdots , w_{t+n})  = \cfrac{\exp(h^\top \text{v}^{'}_{w_{t}})}{\sum_{w_i \in V}\exp(h^\top \text{v}'_{w_i})}</script><ul><li>$n$ 表 window size</li><li>$w_t$ 表 CBOW target center word </li><li>$w_i$ 表 word $i$ in vocabulary $V$</li><li>$\text{v}_{w_i}$ 表 matrix $W$ 中，代表 $w_i$ 的那個  row vector</li><li>$\text{v}’_{w_i}$ 表 matrix $W’$ 中，代表 $w_i$ 的那個  column vector</li><li>$h$ 表 hidden layer 的輸出，其值為 input context word vector 的平均 $\cfrac{1}{C}(\text{v}_{w_1} + \text{v}_{w_2}+ …+ \text{v}_{w_C})^T$</li></ul><p>Because there are multiple contextual words, we construct the input vector by averaging each words’s distribution representation</p><h3 id="Skipgram-amp-Softmax"><a href="#Skipgram-amp-Softmax" class="headerlink" title="Skipgram &amp; Softmax"></a>Skipgram &amp; Softmax</h3><blockquote><p>skipgram uses the centre word to predict the surrounding words instead of using the surrounding words to predict the centre word</p></blockquote><script type="math/tex; mode=display">P(context|center; \theta)</script><p><img src="https://i.imgur.com/aYoLDWN.png" alt="Skipgram" style="zoom: 50%;" /></p><h4 id="Skipgram-的-Objective-Function"><a href="#Skipgram-的-Objective-Function" class="headerlink" title="Skipgram 的 Objective Function"></a>Skipgram 的 Objective Function</h4><script type="math/tex; mode=display">J_\theta = \frac{1}{V}\sum\limits_{t=1}^V\ \sum\limits_{-n \leq j \leq n , \neq 0} \text{log} \space p(w_{t+j} \: | \: w_t)</script><p>其中</p><script type="math/tex; mode=display">p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({h^\top \text{v}'_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><ul><li>$n$ 為 window size</li><li>$w_{t+j}$ 表 skipgram target 第 j 個 context word</li><li>$w_t$ 為 skipgram input 的 center word</li><li>skip-gram 有兩個獨立的 embedding matrix $W$ and $W^{‘}$<ul><li>$W$ : $V \times  N$ , $V$ is vocabulary size; N is vector dimension</li><li>output matrix $W^{‘}$: $N \times V$, encoding the meaning of context</li></ul></li><li>$\text{v}^{‘}_{w_i}$ is  column vector of word $w_i$ in $Ｗ^{‘}$</li><li>$h$ is the hidden layer’s output</li></ul><p>事實上，skip-gram 沒有 hidden layer, 因爲 input 只有一個 word,  $h$  就是 word embedding  $\text{v}_{w_t}$of the word $w_t$ in $W$。</p><p>所以  </p><p> $p(w_{t+j} \: | \: w_t ) = \dfrac{\text{exp}({\text{v}^\top_{w_t} \text{v}’_{w_{t+j}}})}{\sum_{w_i \in V} \text{exp}({\text{v}^\top_{w_t} \text{v}’_{w_i}})}$</p><h2 id="兩種-loss-function-優化"><a href="#兩種-loss-function-優化" class="headerlink" title="兩種 loss function 優化"></a>兩種 loss function 優化</h2><p>原始 softmax 作為 objective function， 分母必須對所有 word $w_i$ in vocabulary 與 vector $ h$ 內積，造成運算瓶頸</p><script type="math/tex; mode=display">p(w_O | w_I) = \dfrac{\text{exp}({h^\top \text{v}'_{w_{O}}})}{\sum_{w_i \in V} \text{exp}({h^\top \text{v}'_{w_i}})}</script><p>所以 word2Vec 論文中採用兩種 objective function 的優化方案，Hierarchical Softmax  與 Negatvie Sampling</p><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>原理推導請參閱  <a href="/hierarchical-softmax-in-word2vec/" title="Word2Vec (2):Hierarchical Softmax 背後的數學">Word2Vec (2):Hierarchical Softmax 背後的數學</a></p><blockquote><p>Hierarchical softmax build a full binary tree to avoid computation over all vocabulary</p></blockquote><p><img src="https://i.imgur.com/Kqj88Gk.png" alt=""></p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>原理推導請參閱  <a href="/negative-sampling-in-word2vec/" title="Word2Vec (3):Negative Sampling 背後的數學">Word2Vec (3):Negative Sampling 背後的數學</a></p><blockquote><p>negative sampling did further simplification because it focuses on learning high-quality word embedding rather than modeling the likelihood of correct word in natural language.</p></blockquote><ul><li>In implement，negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them</li></ul><h1 id="實現-WordVec"><a href="#實現-WordVec" class="headerlink" title="實現 WordVec"></a>實現 WordVec</h1><ul><li>skip gram + softmax  <a href="/Pytorch-Implement-Naive-Word2Vec-with-Softmax/" title="Word2Vec (4):Pytorch 實作 Word2Vec with Softmax">Word2Vec (4):Pytorch 實作 Word2Vec with Softmax</a></li><li>CBOW + softmax  <a href="/Pytorch-Implement-Naive-Word2Vec-with-Softmax/" title="Word2Vec (4):Pytorch 實作 Word2Vec with Softmax">Word2Vec (4):Pytorch 實作 Word2Vec with Softmax</a></li><li>CBOW + hierarchical softmax <a href="/Pytorch-Implement-CBOW-with-Hierarchical-Softmax/" title="Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax">Word2Vec (5):Pytorch 實作 CBOW with Hierarchical Softmax</a></li><li>CBOW + negatove sampling</li><li>skip gram + hierarchical softmax</li><li>skip gram + negative sampling <a href="/Pytorch-Implement-Skipgram-with-Negative-Sampling/" title="Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling">Word2Vec (6):Pytorch 實作 Skipgram with Negative Sampling</a></li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Skip gram 與 CBOW 實際上都 train 了兩個  embedding matrix $W$ and $W’$ </p><ul><li>$W:$ 在 C implement  稱作 $syn0$。</li><li>$W’$:<ul><li>若採用 hierarchical softmax 稱為  $syn1$</li><li>若採用 negative sampling 叫 $syn1neg$ </li></ul></li></ul><p>根據性質，$syn0$ 與 $syn1neg$ 是一體兩面的，本身都可以代表 word embedding，皆可使用。</p><p>而代表 $W’$ 的 $syn1$ 因爲連結的是 huffman tree 的 non leaf node，所以其本身對 word $w_i$ 沒直接意義。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p>On word embeddings - Part 1 <a href="https://ruder.io/word-embeddings-1/">https://ruder.io/word-embeddings-1/</a></p></li><li><p>On word embeddings - Part 2: Approximating the Softmax <a href="https://ruder.io/word-embeddings-softmax/">https://ruder.io/word-embeddings-softmax/</a></p></li><li><p>Learning Word Embedding <a href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy">https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html#cross-entropy</a></p></li><li><p>other</p><ul><li>Rong, X. (2014). word2vec Parameter Learning Explained, 1–21. Retrieved from <a href="http://arxiv.org/abs/1411.2738">http://arxiv.org/abs/1411.2738</a></li><li>Language Models, Word2Vec, and Efficient Softmax Approximations <a href="https://rohanvarma.me/Word2Vec/">https://rohanvarma.me/Word2Vec/</a></li></ul><ul><li><p><a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a></p></li><li><p><a href="https://www.cnblogs.com/pinard/p/7160330.html">word2vec原理(一) CBOW与Skip-Gram模型基础</a></p></li><li><p><a href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a></p></li><li><p><a href="http://shomy.top/2017/07/28/word2vec-all/">Word2vec数学原理全家桶</a></p></li></ul><ul><li><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></li><li>Word2Vec-知其然知其所以然 <a href="https://www.zybuluo.com/Dounm/note/591752#word2vec-%E7%9F%A5%E5%85%B6%E7%84%B6%E7%9F%A5%E5%85%B6%E6%89%80%E4%BB%A5%E7%84%B6">https://www.zybuluo.com/Dounm/note/591752#word2vec-知其然知其所以然</a></li></ul></li><li><p>C source code</p><ul><li>Word2Vec源码解析 <a href="https://www.cnblogs.com/neopenx/p/4571996.html">https://www.cnblogs.com/neopenx/p/4571996.html</a></li></ul></li><li><p>應用</p><ul><li>小白看Word2Vec的正确打开姿势|全部理解和应用 <a href="https://zhuanlan.zhihu.com/p/120148300">https://zhuanlan.zhihu.com/p/120148300</a></li><li>推荐系统从零单排系列(六)—Word2Vec优化策略层次Softmax与负采样 [<a href="https://zhuanlan.zhihu.com/p/66417229">https://zhuanlan.zhihu.com/p/66417229</a></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> word embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一步步透視 GBDT Classifier</title>
      <link href="GBDT-Classifier-step-by-step/"/>
      <url>GBDT-Classifier-step-by-step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a><strong>TL;DR</strong></h1><ul><li>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$<ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x)$</li></ul></li><li>訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}$ 的方向<ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li><li>GBDT classifier 常用的 loss function 為 cross entropy</li><li>classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$  跟 $probability$  有關，得將 $F(x)$ 通過 $\textit{sigmoid function }$ 獲得  probability<ul><li>$p = \sigma(F(x))$</li></ul></li></ul><p>GBDT 簡介在 <a href="https://seed9d.github.io/GBDT-Rregression-Tree-Step-by-Step/#GBDT-%E7%B0%A1%E4%BB%8B">一步步透視 GBDT Regression Tree</a></p><p>直接進入正題吧</p><a id="more"></a><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>GBDT classification tree algorithm 跟 regression  tree 並無不同</p><p><img src="https://i.imgur.com/bfBpmPD.png" style="zoom:50%;" /></p><h2 id="Input-Dat-and-Loss-Function"><a href="#Input-Dat-and-Loss-Function" class="headerlink" title="Input Dat and Loss Function"></a>Input Dat and Loss Function</h2><blockquote><p>Input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/xUdzKge.png" alt="Data"></p><ul><li>target $y_i$: who loves Troll2</li><li>features of $x_i$: “likes popcorn”, “Age”,  “favorite”</li></ul><p>Our goal is using $x_i$ to predict someone like Trolls 2 or not</p><p>loss function 為  cross entropy</p><script type="math/tex; mode=display">\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​</script><p><strong>值得注意的是，GBDT - classifier $F(x)$  輸出的是  $log(odds)$ 而不是 $probability$</strong></p><p>要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\textit{sigmoide function}$</p><script type="math/tex; mode=display">\textit{The probability of Loving Troll 2 } = \sigma(F(x)) = p</script><ul><li><p>$log(odds)$ 轉換成 $probability$ 公式</p><script type="math/tex; mode=display">  p = \cfrac{e^{\log(odds)}}{1 + e^{\log(odds)}}</script></li></ul><h2 id="Step-1-Initial-model-with-a-constant-value-F-0-X"><a href="#Step-1-Initial-model-with-a-constant-value-F-0-X" class="headerlink" title="Step 1 Initial model with a constant value $F_0(X)$"></a>Step 1 Initial model with a constant value $F_0(X)$</h2><p><img src="https://i.imgur.com/vZnfhjM.png" alt="初始 data samples" style="zoom:67%;" /></p><p>初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可</p><p><img src="https://i.imgur.com/0g9VDEd.png" alt=""></p><p>計算完，得到 $F_0(x) = 0.69$，每個  data point 的初始 prediction 都一樣就是 $F_0(x)$。</p><p>$F_0(x)$ 是 $\log(odds)$ 若要計算 probability of loving Troll 2 呢？</p><p><img src="https://i.imgur.com/L6ilbXq.png" alt=""></p><p>$\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。</p><p><img src="https://i.imgur.com/TvWxTvg.png" alt=""></p><ul><li>ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$</li><li>ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving  Troll 2 </li></ul><h2 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  <strong>with new tree $f_m(x)$</strong></li></ol><hr><h3 id="At-Epoch-m-1"><a href="#At-Epoch-m-1" class="headerlink" title="At Epoch m = 1"></a>At Epoch m = 1</h3><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>classification 問題中  residual 為 predicted probability  與 observed label $y$ 之間的差距</p><p>$residual = observed - \textit{predicted probability}$</p><p><img src="https://i.imgur.com/wr9RAF2.png" alt="residual" style="zoom:67%;" /></p><ul><li>true label 為 1</li><li>false label 為 0</li></ul><p><strong>注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$  輸出的是 $log(odds)$</strong></p><p>計算各 data sample 的 residual  後：</p><p><img src="https://i.imgur.com/Xrwhgxy.png" style="zoom:67%;" /></p><ul><li>ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$</li><li>ep_0_prob 表 $F_0(x)$ predicted probability，$\sigma(F_0(x))$</li><li>ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/MywnkEq.png" style="zoom:67%;" /></p><p>建樹為一般 fit regression tree  的過程，criterion 為 mean square error，假設找到的樹結構為</p><p><img src="https://i.imgur.com/FRJtKz0.png" alt=""></p><p>可以看到綠色為 leaf node，所有的 data  sample $x$ 都被歸到特定 leaf node 下</p><p><img src="https://i.imgur.com/obH8T1T.png" alt="" style="zoom:67%;" /></p><ul><li>ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下</p><script type="math/tex; mode=display">\cfrac{\sum residual_i}{\sum [\textit{previous probability} \times \textit{(1 - previous probability)}]}</script><ul><li>分子是 each leaf node 下的 data sample $x$ 的 residual 和</li><li>分母的 previous probability 為 $m -1$  步 GBDT 輸出的 probability $p = \sigma(F(x))$ 。<br>在這個 epoch 是指 $F_0(x)$</li></ul><p>經過計算後，每個 leaf node 輸出</p><p><img src="https://i.imgur.com/j7I1oVk.png" alt=""></p><p><img src="https://i.imgur.com/Sasd4Ei.png" style="zoom:67%;" /></p><ul><li>ep_0_prob  表 $\sigma(F_0(x))$ 計算出的 probability of loving Troll2</li><li>ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob</li><li>ep_1_leaf_output 表 data sample $x$ 在  tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值</li></ul><h4 id="D-update-F-1-x-with-new-tree-f-1-x"><a href="#D-update-F-1-x-with-new-tree-f-1-x" class="headerlink" title="(D) update $F_1(x)$  with new tree $f_1(x)$"></a>(D) update $F_1(x)$  <strong>with new tree $f_1(x)$</strong></h4><p>現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 </p><script type="math/tex; mode=display">F_1(x) = F_0(x) + \textit{learning rate} \times  f_1(x)</script><p>輸出的 probability 為 $\sigma(F_1(x))$</p><p>令 $\textit{learnign rate = 0.8}$，得到 epoch 2 每個  data sample 的 $\log(odds)$  prediction 與 probability prediction</p><p><img src="https://i.imgur.com/xt6rxMA.png" style="zoom:67%;" /></p><ul><li>ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$</li><li>ep_1_prob 為 $F_1(x)$ 輸出的 probability $\sigma(F_1(x))$</li></ul><hr><h3 id="At-Epoch-m-2"><a href="#At-Epoch-m-2" class="headerlink" title="At Epoch m = 2"></a>At Epoch m = 2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_1(x)$"></a>(A) Calculate Residuals of $F_1(x)$</h4><p>計算上一步 $\textit{residual of } F_1(X)$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p><img src="https://i.imgur.com/mIWXSGC.png" style="zoom:67%;" /></p><ul><li>ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\sigma(F_1(x))$</li><li>ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>用 data sample x 的 columns “like_popcor”, “age”, “favorite_color”  擬合 ep_2_residual   build a new tree $f_2(x)$</p><p><img src="https://i.imgur.com/levLwV4.png" style="zoom:67%;" /></p><p> 假設得到 $f_2(x)$ 的樹結構：</p><p><img src="https://i.imgur.com/XMBr1KE.png" alt=""></p><p> 每個 data sample 對應的 leaf index<img src="https://i.imgur.com/qM6crwo.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_index 表 data sample  對應到 $f_2(x)$  上的 leaf node index</li></ul><h4 id="D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$"></a>(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$</h4><p>計算 $f_2(x)$ 下每個 leaf node 的輸出:</p><p><img src="https://i.imgur.com/MOaIis1.png" alt=""></p><p>對應到 data sample 上:</p><p><img src="https://i.imgur.com/yKumKHj.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node  下會有一樣的值</li></ul><h3 id="Update-F-2-x-with-New-Tree-f-2-x"><a href="#Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="Update $F_2(x)$  with New Tree $f_2(x)$"></a>Update $F_2(x)$  with New Tree $f_2(x)$</h3><p>到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為</p><p>$F_2(x) = F_0(x) + \nu(f_1(x) + f_2(x))$</p><ul><li>$\nu$ 為 learning rate，假設為 0.8</li></ul><p>GBDT 輸出的 probability 為 $\sigma(F_2(x))$，計算 epoch 2 的 prediction of  probability of loving troll2:</p><p><img src="https://i.imgur.com/6Lxi8mZ.png" style="zoom:67%;" /></p><ul><li>love_toll2: our target</li><li>ep_0_pre 表 $F_0(x)$</li><li>ep_1_leaf_output 表 data sample x​  在第一顆樹 $f_1(x)$ 的輸出值</li><li>ep_2_leaf_output 表 data sample x 在第二顆樹  $f_2(x)$  的輸出值</li><li>ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$</li><li>ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\sigma(F_2(x))$</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p>Output GBDT fitted model $F_M(x)$</p></blockquote><p>把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/MHQLmVE.png" style="zoom:67%;" /></p><p>$F_M(x)$ 的每棵樹 $f_m(x)$  都是去 fit  $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p>所以 $F_m(x)$ 又可以寫成</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><p>這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住：</p><ul><li>$F_m(x)$ 輸出 $log(odds)$</li><li>$residual$  的計算與 probability 有關</li></ul><h1 id="GBDT-Classifier-背後的數學"><a href="#GBDT-Classifier-背後的數學" class="headerlink" title="GBDT Classifier 背後的數學"></a>GBDT Classifier 背後的數學</h1><h3 id="Q-為什麼用-cross-entropy-做為-loss-function"><a href="#Q-為什麼用-cross-entropy-做為-loss-function" class="headerlink" title="Q: 為什麼用 cross entropy 做為 loss function ?"></a>Q: 為什麼用 cross entropy 做為 loss function ?</h3><p>在分類問題上，我們預測的是 $\textit{The probability of loving Troll 2}$  $P(Y|x)$，$\textit{}$ 以 $maximize$ $\textit{log likelihood}$ 來解 $P(Y|x)$。</p><p>令 GBDT - classification tree 的  probability prediction 為 $P(Y| x) = \sigma(F(x))$，則 objective function 為 </p><script type="math/tex; mode=display">\text{log (likelihood of the obersved data given the prediction) }  \\= \sum_{i=1}^N [y_i \log(p) + (1-y_i)\log(1-p)]</script><ul><li>$p = P(Y=1|x)$，表 the probability of loving movie Troll 2</li><li>$y_i$ : observation  of data sample $x_i$ loving Troll 2 or not<ul><li>$y \in \{1, 0\}$</li></ul></li></ul><p>而 $\text{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成</p><script type="math/tex; mode=display">\text{objective function} = - \sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)</script><p>所以 $\text{loss function} = -[y \log(p) + (1-y)\log(1-p)]$</p><p>把 loss function 用 $odds$ 表示：</p><script type="math/tex; mode=display">\begin{aligned} -[y \log(p) + (1-y)\log(1-p)] & = -y\log(p)-(1-y)\log(1-p) \\ &= -y\log(p)-\log(1-p) + y\log(1-p) \\ &= -y[\log(p) - \log(1-p)] - \log(1-p)  \\ & = -y\log(odds) - \log(1-p) \\ &= -y\log(odds) + \log(1 + \exp(\log(odds))) \end{aligned}</script><ul><li>第三個等號 到 第四個等號用到 $odds=\cfrac{p}{1-p}$</li><li>第四個等號 到 第五個等號用到 $p = \cfrac{\exp(\log(odds))}{1 + \exp(\log(odds))}$ 這個結論<ul><li>$\log(1-p) = \log(1- \cfrac{\exp(\log(odds))}{1 + \exp(\log(odds))}) = \log(\cfrac{1}{1 + \exp(\log(odds))}) = -\log(1 + \exp(\log(odds)))$</li></ul></li></ul><p>把 loss  function 表示成 odds 的好處是， $ -y\log(odds) + \log(1 + \exp(\log(odds)))$ 對 $log(odds)$  微分形式很簡潔</p><script type="math/tex; mode=display">\cfrac{d}{d \ log(odds)} -ylog(odds) + log(1 + \exp^{log(odds)}) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}} = -y + p</script><p>loss function 對 $log(odds)$ 的微分，既可以以 $\log(odds)$ 表示，也可以以 probability $p$ 表示</p><ul><li>以 $log(odds)$ 表示：  $\cfrac{d}{d \log(odds)}L(y_i, p) = -y -\cfrac{\exp(\log(odds))}{1 + \exp(\log(odds))}$</li><li>以 $p$ 表示：$\cfrac{d}{d \log(odds)}L(y_i, p) = -y + p$</li></ul><p>用 $p$ 表示時，loss function 對 $log(odds)$ 的微分</p><script type="math/tex; mode=display">-y + p = \text{ -(observed  - predicted) = negative residual}</script><h3 id="Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false"><a href="#Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false" class="headerlink" title="Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?"></a>Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?</h3><blockquote><p>來自 Step 1 的問題</p></blockquote><p>根據選定的  loss function </p><script type="math/tex; mode=display">\text{loss function} = -[y \log(p) + (1-y)\log(1-p)]</script><ul><li>$P(Y=1|x) = p$ 為出現正類的 probability</li><li>$y \in \{1, 0\}$</li></ul><p>將 loss  function 以 $\log(odds)$ 表示</p><script type="math/tex; mode=display">-[y \log(p) + (1-y)\log(1-p)] = -[y\log(odds) + \log(1 + \exp(log(odds)))]</script><p>$F_0(x)$ 為能使 $\textit{cost function}$ 最小的 $\log(odds): \gamma$</p><script type="math/tex; mode=display">F_0(x) = argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma) = argmin_\gamma \sum^n_{i=1} -[y_i\log(odds) + \log(1 + \exp(\log(odds)))]</script><ul><li>$n$ 為 number of data sample $x$</li></ul><p>令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$</p><p>cost  function 對 $log(odds)$  微分取極值：</p><script type="math/tex; mode=display">\begin{aligned}& \cfrac{d}{d \log(odds)}\sum^n_{i=1} -[y_i\log(odds) + \log(1 + \exp(log(odds)))]   \\ & = \cfrac{d}{d \log(odds)}\sum^{n^{(1)}}_i -(\log(odds) + \log(1 + exp(log(odds)))) - \sum^{n^{(0)}}_j (0 * \log(odds) + \log(1 + \exp(\log(odds))))  \\& = \cfrac{d}{d\log(odds)} -n^{(1)} \times (\log(odds) + \log(1 + \exp(\log(odds)))) - n^{(0)} \times \log(1 + \exp(\log(odds))) \\ & =0\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} & n^{(1)} \times(-1 + \cfrac{e^{\log(odds)}}{1 + e^{\log(odds)}})   + n^{(0)} \times(\cfrac{e^{\log(odds)}}{1 + e^{\log(odds)}}) \\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\ & = - n^{(1)} + n p \\ &= 0 \end{aligned}</script><p>移項得到  $p$</p><script type="math/tex; mode=display">p = \cfrac{n^{(1)}}{n}</script><script type="math/tex; mode=display">\log(odds) = \cfrac{p}{1-p} = \cfrac{n^{(1)}}{n^{(0)}}</script><p>故得證，給定 $\text{loss function }  = -[y \log(p) + (1-y)\log(1-p)]$， 能使 $argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma)$  的 $\gamma$ 為 </p><script type="math/tex; mode=display">log(odds)= \cfrac{n^{(1)}}{n^{(0)}}</script><script type="math/tex; mode=display">\therefore F_0(x) = \cfrac{n^{(1)}}{n^{(0)}}</script><h3 id="Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h3><blockquote><p>問題來自 Step 2 - (A)</p></blockquote><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum -[y_i \log(p_i) + (1-y_i)\log(1-p_i)]</script><ul><li>$y$ 為 target label</li><li>$p = P(Y=1|x)$ 表正類的 probability</li></ul><p>注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy </p><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x) \hAar   F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \  L(y, F_{m-1}(x)) \\ &= -(-(y - p)) \\ &= -(-(\text{observed} - \text{predict probability})) \\ &= - \text{negative residual} \\ & = \text{residual} \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual</strong></p><h3 id="Q-leaf-node-的輸出公式怎麼來的？"><a href="#Q-leaf-node-的輸出公式怎麼來的？" class="headerlink" title="Q: leaf node 的輸出公式怎麼來的？"></a>Q: leaf node 的輸出公式怎麼來的？</h3><blockquote><p>問題來自 Step 2-(C)</p></blockquote><p>在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\gamma_{jm}$，使的 cost function 最小</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &= argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma) \\ &= argmin_\gamma \sum_{x_i \in R_{j, m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + e^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$j$ 表 leaf node index</li><li>$m$ 表第 $m$ 步</li><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合</li></ul><p>將 loss function 以 $\log(odds)$ 表示後的 objective function</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + e^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$-[y \log(p) + (1-y)\log(1-p)] = -[y\log(odds) + \log(1 + e^{\log(odds)})]$<ul><li>$F_{m-1}(x)$ 輸出為 $\log(odds)$</li></ul></li></ul><p>cost function  對 $\gamma$  微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近  loss function 處理</p><script type="math/tex; mode=display">f(x) \approx f(a) + f'(a)(x-a) + \cfrac{1}{2}f''(a)(x-a)^2</script><p>讓  2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \gamma)$ 在 $F_{m-1}(x)$ 處展開</p><script type="math/tex; mode=display">L(y_i, F_{m-1}(x_i) + \gamma) \approx L(y_i, F_{m-1}(x_i) ) + \cfrac{d}{d (F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))\gamma   + \cfrac{1}{2} \cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\gamma^2</script><p>將 cost function 對 $\gamma$ 微分取極值，求  $\gamma_{j,m}$</p><script type="math/tex; mode=display">\sum_{x_i \in R_{jm}} \cfrac{d}{d\gamma}  L(y_i, F_{m-1}(x_i), \gamma) \approx \sum_{x_i \in R_{jm}} (\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\gamma) = 0</script><p>移項得到 $\gamma$</p><script type="math/tex; mode=display">\gamma = \cfrac{\sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))} = \cfrac{ \sum_{x_i \in R_{jm} } g_i}{ \sum_{x_i \in R_{jm} } h_i}</script><p>分子是  derivative of Loss function ;   分母是  second derivative of loss function</p><ul><li><p>分子部分:</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\& = \sum \cfrac{d}{d(F_{m-1}(x_i))}  \ y_i \times [F_{m-1}(x_i)  ] - \log(1 + \exp(F_{m-1}(x_i)  )) \\ &=  \sum (y_i - \cfrac{\exp(F_{m-1}(x_i) )}{1 + \exp(F_{m-1}(x_i) )} ）\\& = \sum_{x_i \in R_{jm}} (y_i -p_i) \end{aligned}</script><ul><li>$F_{m-1}(x_i)$  是 $m-1$  步時 $classifier$  輸出的 $\log(odds)$</li></ul><ul><li><strong>分子部分為 $\text{summation of residual}$</strong></li></ul></li><li><p>分母部分</p></li></ul><script type="math/tex; mode=display">\begin{aligned}& \sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\ & = \sum_{x_i \in R_{jm} } \cfrac{d^2}{d \, \ (F_{m-1}(x_i))^2} \, -[y_i  \times F_{m-1}(x_i) - \log(1 + \exp(F_{m-1}(x_i)))] \\ &= \sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)}-[y_i  - \cfrac{\exp(F_{m-1}(x_i) )}{1 + \exp(F_{m-1}(x_i) )}] \\ & =\sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \exp(F_{m-1}(x_i)))^{-1} \times \exp(F_{m-1}(x_i))]  \\ & = \sum_{x_i \in R_{jm} }-[(1 + e^{F_{m-1}(x_i)})^{-2} \exp(F_{m-1}(x_i))\times \exp(F_{m-1}(x_i)) - (1+ \exp(F_{m-1}(x_i)))^{-1}  \times \exp(F_{m-1}(x_i))  ]\\&=  \sum_{x_i \in R_{jm} } \cfrac{-\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} \quad + \quad \cfrac{\exp(F_{m-1}(x_i))}{1+ \exp(F_{m-1}(x_i))} = \sum_{x_i \in R_{jm} }\cfrac{-\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2}   \ + \ \cfrac{-\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} \times \cfrac{(1 + \exp(F_{m-1}(x_i)))}{1 + \exp(F_{m-1}(x_i))}\\&= \sum_{x_i \in R_{jm} } \cfrac{-\exp(2 * F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} + \cfrac{\exp(F_{m-1}(x_i)) + \exp(2 * F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} = \sum_{x_i \in R_{jm} }\cfrac{\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))^2} \\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp(F_{m-1}(x_i))}{(1 + \exp(F_{m-1}(x_i)))} \times \cfrac{1}{(1 + \exp(F_{m-1}(x_i)))}\\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp(\log(odds)_i)}{1 + \exp(\log(odds)_i)} \times \cfrac{1}{1 + \exp(\log(odds)_i)}\\ &= \sum_{x_i \in R_{jm} } p_i \times (1-p_i)\end{aligned}</script><p>綜合分子分母，能使 $F_m(x)$  cost function 最小化的  tree  $f_m(x)$   第 $j$ 個  leaf node 輸出為</p><script type="math/tex; mode=display">\gamma_{jm}= \cfrac{\sum_{x_i \in R_{jm})} (y_i - p_i)}{\sum_{x_i \in R_{jm} }(p_i \times (1- p_i))} = \cfrac{\text{summation of  residuals }}{\text{summantion of (previous probability $\times$ (1 - previoous probability))}}</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><h2 id="Data-Sample"><a href="#Data-Sample" class="headerlink" title="Data Sample"></a>Data Sample</h2><blockquote><p>learning by doing it</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">12</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">87</span>, <span class="string">&#x27;gree&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">44</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">19</span>, <span class="string">&#x27;red&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">32</span>, <span class="string">&#x27;green&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">14</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;like_popcorn&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;love_troll2&#x27;</span>]</span><br><span class="line">target = <span class="string">&#x27;love_troll2&#x27;</span></span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p><a href="https://www.youtube.com/watch?v=jxuNLH5dXCs&amp;">Gradient Boost Part 3 (of 4): Classification</a></p></li><li><p><a href="https://www.youtube.com/watch?v=StWY5QWMXCw&amp;t">Gradient Boost Part 4 (of 4): Classification Details</a></p></li></ul><ul><li>Gradient Boosting In Classification: Not a Black Box Anymore! <a href="https://blog.paperspace.com/gradient-boosting-for-classification/">https://blog.paperspace.com/gradient-boosting-for-classification/</a><ul><li>statquest 整理</li></ul></li><li>StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! <a href="https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s">https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s</a></li><li>The Logit and Sigmoid Functions <a href="https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/">https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/</a></li><li>Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends <a href="https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979">https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一步步透視 GBDT Regression Tree</title>
      <link href="GBDT-Rregression-Tree-Step-by-Step/"/>
      <url>GBDT-Rregression-Tree-Step-by-Step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><ul><li><p>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$</p><ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x) $</li></ul></li><li><p>訓練中的 GBDT，每棵新樹  $f_m(x)$ 都去擬合  target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient  decent}$  的方向 </p><ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li></ul><p><img src="https://i.loli.net/2021/01/23/ZWgsyMOw5LoQxFz.png" alt="Golf Boosting"></p><a id="more"></a><p>結論說完了，想看數學原理請移駕 <a href="#math">背後的數學</a>，想看例子從無到有生出 GBDT 的請到 <a href="#alg">Algorithm - step by step</a> ，想離開得請按上一頁。</p><h1 id="GBDT-簡介"><a href="#GBDT-簡介" class="headerlink" title="GBDT 簡介"></a>GBDT 簡介</h1><p>GBDT-regression tree 簡單來說，訓練時依序建立 trees $\{ f_1(x), f_2(x), …. , f_M(x)\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。</p><p>所以 GBDT - regression  tree 的訓練是 sequentially ，無法以並行訓練加速。</p><p>我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下:</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu\sum^m_{i=1}f_i(x) = F_{m-1}(x)  + \nu f_m(x)</script><ul><li>$\nu$ 為 learning rate</li></ul><p>GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。</p><p>差多遠即 residual 的概念：</p><script type="math/tex; mode=display">\textit{residual = observed - predicted}</script><p>因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual</p><p><a name="alg"><a> </p><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>Algorithm 參考了 statQuest 對 GBDT 的講解，連結放在  reference，必看！</p><p>GBDT-regression tree 擬合 algorithm：</p><h2 id="Input-Data-and-Loss-Function"><a href="#Input-Data-and-Loss-Function" class="headerlink" title="Input Data and Loss Function"></a><img src="https://i.loli.net/2021/01/23/6BuRsPyvaM7GULc.png" alt="Algorithm of GBDT">Input Data and Loss Function</h2><blockquote><p>input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/yYJ9IpX.png" alt=""></p><p>接下來都用這組簡單的數據，其中</p><ul><li>Target $y_i$ : weight which is what we want to predict</li><li>$x_i$: features 的組成有 身高，喜歡的顏色，性別</li></ul><p>目標是用 $x_i$ 的 height, favorite color, gender  來預測  $y_i$ 的 weight</p><p>loss function 為 square error </p><script type="math/tex; mode=display">L(y_i, F(x)) = \cfrac{1}{2}(\textit{observed - predicted}) ^2 = \cfrac{1}{2}(y_i^2 - F(x))^2</script><ul><li><p>square error commonly use in Regression with Gradient Boost</p></li><li><p>$\textit{observed - predicted}$  is called  $residual$</p></li><li><p>$y_i$ are observed value</p></li><li><p>$F(x)$: the function which give us the predicted value</p><ul><li><p>也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><script type="math/tex; mode=display">F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)</script></li></ul></li></ul><p><a name="step1"></a></p><h2 id="Step-1-Initialize-Model-with-a-Constant-Value"><a href="#Step-1-Initialize-Model-with-a-Constant-Value" class="headerlink" title="Step 1 Initialize Model with a Constant Value"></a>Step 1 Initialize Model with a Constant Value</h2><p>初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可</p><p><img src="https://i.imgur.com/uamGKg1.png" alt=""></p><p>取  weight 平均得到 $F_0(x) = 71.2 = \textit{average weight}$</p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  with new tree $f_m(x)$</li></ol><hr><h3 id="At-epoch-m-1"><a href="#At-epoch-m-1" class="headerlink" title="At epoch m=1"></a>At epoch m=1</h3><p><a name="step2A"></a></p><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals </p><p>​    <script type="math/tex">residuals = \textit{observed weight - predicted weight}</script></p><p>而 $F_0(x) = \textit{average weight = 71.17}$</p><p>計算 residual 後: </p><p><img src="https://i.imgur.com/FmhobDH.png" alt=""></p><ul><li>epoch_0_prediction 表 $F_0(x)$ 輸出</li><li>epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的  residual </p><p>用 columns $\textit{height, favorite, color, gender}$  預測 $residuals$ 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/Vg5cKjw.png" alt=""></p><p>建樹的過程為一般的 regression tree building 過程，target 就是 residuals。</p><p>假設我們找到分支結構是</p><p><img src="https://i.imgur.com/qmuUdME.png" alt=""></p><p>綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$  特定的 leaf node 下。</p><p><img src="https://i.imgur.com/C0P4ukP.png" alt=""></p><ul><li>epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index</li></ul><p><a name="step2C"></a></p><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值</p><p>直覺地對每個 leaf node 內的 data sample $x$  weight 值取平均，得到輸出值</p><p><img src="https://i.imgur.com/wruHARq.png" alt=""></p><p><img src="https://i.imgur.com/ausJcrf.png" alt="Untitled 9"></p><ul><li>epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值</li></ul><p><a name="step2D"></a></p><h4 id="D-Update-F-1-x-with-New-Tree-f-1-x"><a href="#D-Update-F-1-x-with-New-Tree-f-1-x" class="headerlink" title="(D) Update $F_1(x)$  with New Tree $f_1(x)$"></a>(D) Update $F_1(x)$  with New Tree $f_1(x)$</h4><p>此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$</p><p>現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \textit{learning rate } \times f_1(x)$</p><p>假設 $\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下</p><p><img src="https://i.imgur.com/1fNc1dF.png" alt=""></p><ul><li>epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值</li></ul><hr><h3 id="At-epoch-m-2"><a href="#At-epoch-m-2" class="headerlink" title="At epoch m=2"></a>At epoch m=2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_{1}(x)$"></a>(A) Calculate Residuals of $F_{1}(x)$</h4><p>m = 2 新的 residual between observed weight and $F_1(x)$如下</p><p><img src="https://i.imgur.com/NMS5YJ0.png" alt=""></p><ul><li>epoch_1_prediction 為 $F_1(x)$  的輸出</li><li>epoch_2_residual 為 observed weight  與 predicted weight $F_1(x)$ 的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>建一顆新樹擬合 epoch 2 (A)  得出的 residual</p><p><img src="https://i.imgur.com/bcUyFaW.png" alt=""></p><ul><li>epoch_2_residual 為 $f_2(x)$ 要擬合的 target</li></ul><p>假設 $f_2(x)$ 擬合後樹結構長這樣</p><p><img src="https://i.imgur.com/pJJZRC6.png" alt=""></p><p><img src="https://i.imgur.com/eNDlBn4.png" alt=""></p><ul><li>epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$</h4><p>決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個  leaf node 下的 data sample $x$ 取平均</p><p><img src="https://i.imgur.com/SM21atv.png" alt=""></p><ul><li>epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是  residual epoch_2_residual</li></ul><h4 id="D-Update-F-2-x-with-New-Tree-f-2-x"><a href="#D-Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="(D) Update $F_2(x)$  with New Tree $f_2(x)$"></a>(D) Update $F_2(x)$  with New Tree $f_2(x)$</h4><p>到目前為止我們建立了兩顆 $tree$  $f_1(x), f_2(x)$，假設  $\textit{learning rate = 0.1}$，則 $F_2(x)$ 為</p><script type="math/tex; mode=display">F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))</script><p>每個  data sample  在 $F_2(x)$ 的 predict 值如下圖：</p><p><img src="https://i.imgur.com/wbbpfNc.png" alt=""> </p><ul><li>weight: out target value</li><li>epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值</li><li>epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual</li><li>epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual</li><li>epoch_2_prediction:  $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p> Output GBDT fitted model $F_M(x)$</p></blockquote><p>Step 3 輸出模型 $F_M(x)$，把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/ZPUv6Cr.png" alt=""></p><p>$F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$  與上一步 predicted value $\hat{y}$ $F_{m-1}(x)$ 的  $residual$</p><p><a name="math"></a></p><h1 id="GBDT-Regression-背後的數學"><a href="#GBDT-Regression-背後的數學" class="headerlink" title="GBDT Regression 背後的數學"></a>GBDT Regression 背後的數學</h1><h2 id="Q-Loss-function-為什麼用-mean-square-error"><a href="#Q-Loss-function-為什麼用-mean-square-error" class="headerlink" title="Q:  Loss function 為什麼用 mean square error ?"></a>Q:  Loss function 為什麼用 mean square error ?</h2><p>​    </p><p>選擇 $\cfrac{1}{2}(\textit{observed - predicted}) ^2$  當作 loss function  的好處是對 $F(X)$ 微分的形式簡潔</p><script type="math/tex; mode=display">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script><p>其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\textit{negative  residual}$</p><p>​    $-(y_i - F(X)) = \textit{-(observed - predicted) = negative residual }  $</p><p>而我們知道 $F(X)$ 在  loss function $L(y_i, F(X))$ 的 gradient 就是 $\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$</p><h2 id="Q：為什麼初始-F-0-X-直接對-targets-取平均？"><a href="#Q：為什麼初始-F-0-X-直接對-targets-取平均？" class="headerlink" title="Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？"></a>Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？</h2><p><a href="#step1">問題來自 step 1</a></p><p>我們要找的是能使 cost function $\sum^n_{i=1}L(y_i, \gamma)$ 最小的那個輸出值 $\gamma$ 做為  $F_0(X)$。</p><p>$F_0(x) = argmin_r \sum^n_{i=1} L(y_i,\gamma)$</p><ul><li><p>$F_0(x)$ 初始化的  function，其值是常數</p></li><li><p>$\gamma$  refers to the predicted values</p></li><li><p>$n$ 是 data sample 數</p></li></ul><p><em>Proof:</em></p><p>已知</p><p>​    <script type="math/tex">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script></p><p>所以 cost function 對 $\gamma $ 微分後，是所有 sample data 跟 $\gamma$ 的 negative residual 和 為 0</p><p>​    <script type="math/tex">\sum^n_{i=1}L(y_i, \gamma) = -(y_1 - \gamma) - (y_2 - \gamma)-  ... -(y_n - \gamma) = 0</script></p><p>移項得到 $\gamma$</p><p>​    <script type="math/tex">\gamma = \cfrac{y_1 + y_2 + .... + y_n}{n}</script></p><p>正是所有 target value 的 mean，故得證</p><p>​    <script type="math/tex">F_0(x) = \gamma = \textit{the average of targets value}</script> </p><h2 id="Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h2><p><a href="#step2A">問題來自 step 2A</a></p><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum \cfrac{1}{2}(y_i^2 - F(x))^2</script><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)\hAar F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} \ L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \ L(y, F_{m-1}(x)) \\ &= -(-(y - F_{m-1}(x))) \\ &= -(-(observed - predicted )) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent  的方向和值， 也就是 residual</strong> </p><p>​    $f_m(x)$  = $\textit{gradient decent}$   = $\textit{negative gradient}$  = $residual$</p><p><strong>GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\textit{gradient  decent}$  的方向得到新的 $F(x)$</strong></p><p><strong>這也正是為什麼叫做 gradient boost</strong> 。</p><p>by the way，step 2-(A) compute residuals:</p><script type="math/tex; mode=display">r_{im} = -[\cfrac{\partial L(y_i, F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \ \textit{for i = 1,....,n}</script><ul><li>$i$: sample number</li><li>$m$: the tree we are trying to build</li></ul><h2 id="Q：個別-leaf-node-的輸出為什麼是取平均-？"><a href="#Q：個別-leaf-node-的輸出為什麼是取平均-？" class="headerlink" title="Q：個別 leaf node 的輸出為什麼是取平均 ？"></a>Q：個別 leaf node 的輸出為什麼是取平均 ？</h2><p><a href="#step2C">問題來自 step 2C </a></p><p>在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\gamma$ 使 cost function  最小</p><p>$\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma)$</p><ul><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合</li><li>$F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$  上的輸出值為 $\gamma_{j,m}$</li></ul><script type="math/tex; mode=display">\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2</script><p>直接對  $\gamma$ 微分</p><script type="math/tex; mode=display">\begin{aligned}\cfrac{d}{d \gamma} \ \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2 =  \ \sum_{x_i \in R_{j,m}}-(y_i - F_{m-1}(x_i) - \gamma) = 0\end{aligned}</script><p>移項</p><script type="math/tex; mode=display">\gamma = \cfrac{1}{n_{jm}} \sum_{x_i \in R_{j,m}}y_i - F_{m-1}(x_i)</script><ul><li>$n_{jm}$ is the number of data sample in leaf node $j$ at step $m$</li></ul><p>白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均</p><p>事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $  都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。</p><h2 id="Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？"><a href="#Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？" class="headerlink" title="Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？"></a>Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？</h2><p><a href="#step2D">問題來自 step 2D </a></p><script type="math/tex; mode=display">\begin{aligned}F_m(x) & = F_{m-1}(x) + \nu f_m(x) \\ &= F_{m-1}(x) + \nu \sum^{J_m}_{j=1}\gamma_{jm}I(x \in R_{jm})\end{aligned}</script><ul><li>$F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)$</li><li>$\nu$ learning rate</li><li>$J_m$ m 步 的 leaf node 總數</li><li>$\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出</li></ul><p>$F_m(x)$ 展開來就是</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu(f_1(x) + f_2(x)+ ...+f_m(t))</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><blockquote><p>seeing is believing</p></blockquote><p>太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙</p><p>人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶</p><blockquote><p>learning by doing it</p></blockquote><p>所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">88</span>),</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Greem&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">76</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">56</span>),</span><br><span class="line">    (<span class="number">1.8</span>, <span class="string">&#x27;Red&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">73</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Green&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">77</span>),</span><br><span class="line">    (<span class="number">1.4</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">57</span>)   </span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>]</span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><blockquote><p>always get your hands dirty</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h2 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h2><ul><li><p><a href="https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=75s">Gradient Boost Part 1 (of 4): Regression Main Ideas</a></p></li><li><p><a href="https://www.youtube.com/watch?v=2xudPOBz-vs">Gradient Boost Part 2 (of 4): Regression Details</a></p></li></ul><p>ccd comment: 上面兩個必看</p><ul><li>A Step by Step Gradient Boosting Decision Tree Example <a href="https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/">https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/</a><ul><li>真 step by step</li></ul></li><li>Gradient Boosting Decision Tree Algorithm Explained <a href="https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4">https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4</a><ul><li>including sklearn 實作</li></ul></li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li>Gradient Boosting Decision Tree <a href="http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95">http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法</a></li><li>提升树算法理论详解 <a href="https://hexinlin.top/2020/03/01/GBDT/">https://hexinlin.top/2020/03/01/GBDT/</a></li><li>梯度提升树(GBDT)原理小结 <a href="https://www.cnblogs.com/pinard/p/6140514.html">https://www.cnblogs.com/pinard/p/6140514.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="ckl9ms4df000glse29j6j5hzi/"/>
      <url>ckl9ms4df000glse29j6j5hzi/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
