<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>一步步透視 GBDT Classifier</title>
      <link href="GBDT-Classifier-step-by-step/"/>
      <url>GBDT-Classifier-step-by-step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a><strong>TL;DR</strong></h1><ul><li>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$<ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x)$</li></ul></li><li>訓練中的 GBDT，每棵新樹 $f_m(x)$ 都去擬合 target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient decent}$ 的方向<ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li><li>GBDT classifier 常用的 loss function 為 cross entropy</li><li>classifier $F(x)$ 輸出的是 $log(odds)$，但衡量 $residual$  跟 $probability$  有關，得將 $F(x)$ 通過 $\textit{sigmoid function }$ 獲得  probability<ul><li>$p = \sigma(F(x))$</li></ul></li></ul><p>GBDT 簡介在 <a href="https://seed9d.github.io/GBDT-Rregression-Tree-Step-by-Step/#GBDT-%E7%B0%A1%E4%BB%8B">一步步透視 GBDT Regression Tree</a></p><p>直接進入正題吧</p><a id="more"></a><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>GBDT classification tree algorithm 跟 regression  tree 並無不同</p><p><img src="https://i.imgur.com/bfBpmPD.png" style="zoom:50%;" /></p><h2 id="Input-Dat-and-Loss-Function"><a href="#Input-Dat-and-Loss-Function" class="headerlink" title="Input Dat and Loss Function"></a>Input Dat and Loss Function</h2><blockquote><p>Input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/xUdzKge.png" alt="Data"></p><ul><li>target $y_i$: who loves Troll2</li><li>features of $x_i$: “likes popcorn”, “Age”,  “favorite”</li></ul><p>Our goal is using $x_i$ to predict someone like Trolls 2 or not</p><p>loss function 為  cross entropy</p><script type="math/tex; mode=display">\textit{loss function} = -[y log(F(x)) + (1-y)log(1-F(x))]​</script><p><strong>值得注意的是，GBDT - classifier $F(x)$  輸出的是  $log(odds)$ 而不是 $probability$</strong></p><p>要將 $F(x)$ 輸出轉換成 $probability$，得將他通過 $\textit{sigmoide function}$</p><script type="math/tex; mode=display">\textit{The probability of Loving Troll 2 } = \sigma(F(x)) = p</script><ul><li><p>$log(odds)$ 轉換成 $probability$ 公式</p><script type="math/tex; mode=display">  p = \cfrac{\exp^{log(odds)}}{1 + exp^{log(odds)}}</script></li></ul><h2 id="Step-1-Initial-model-with-a-constant-value-F-0-X"><a href="#Step-1-Initial-model-with-a-constant-value-F-0-X" class="headerlink" title="Step 1 Initial model with a constant value $F_0(X)$"></a>Step 1 Initial model with a constant value $F_0(X)$</h2><p><img src="https://i.imgur.com/vZnfhjM.png" alt="初始 data samples" style="zoom:67%;" /></p><p>初始 GBDT 模型的 $F_0(x)$ 直接計算 loves_troll 的 $log(odds)$ 即可</p><p><img src="https://i.imgur.com/0g9VDEd.png" alt=""></p><p>計算完，得到 $F_0(x) = 0.69$，每個  data point 的初始 prediction 都一樣就是 $F_0(x)$。</p><p>$F_0(x)$ 是 $\log(odds)$ 若要計算 probability of loving Troll 2 呢？</p><p><img src="https://i.imgur.com/L6ilbXq.png" alt=""></p><p>$\textit{Probability of loving Troll 2} = 0.67$ ， 初始值每個 data sample 的 probability 都一樣。</p><p><img src="https://i.imgur.com/TvWxTvg.png" alt=""></p><ul><li>ep_0_pre 表 epoch 0 時 data sample $x$ 的 prediction， $F_0(x)$ 的輸出是 $log(odds)$</li><li>ep_0_prob 表 epoch 0 時 data sample $x$ 的 probability loving  Troll 2 </li></ul><h2 id="Step2"><a href="#Step2" class="headerlink" title="Step2"></a>Step2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  <strong>with new tree $f_m(x)$</strong></li></ol><hr><h3 id="At-Epoch-m-1"><a href="#At-Epoch-m-1" class="headerlink" title="At Epoch m = 1"></a>At Epoch m = 1</h3><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>classification 問題中  residual 為 predicted probability  與 observed label $y$ 之間的差距</p><p>$residual = observed - \textit{predicted probability}$</p><p><img src="https://i.imgur.com/wr9RAF2.png" alt="residual" style="zoom:67%;" /></p><ul><li>true label 為 1</li><li>false label 為 0</li></ul><p><strong>注意!! 當我們再說 residual 時，是 derived from probability，而 $F(x)$  輸出的是 $log(odds)$</strong></p><p>計算各 data sample 的 residual  後：</p><p><img src="https://i.imgur.com/Xrwhgxy.png" style="zoom:67%;" /></p><ul><li>ep_0_pre 表 $F_0(x)$ 的輸出 $log(odds)$</li><li>ep_0_prob 表 $F_0(x)$ predicted probability，$\sigma(F_0(x))$</li><li>ep_1_residual 表 target label lovel_trolls2 與 ep_0_prob 間的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的 residual，用 columns: “like_popcorn”, “age”, “favorite_color” 擬合 ep_1_residual 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/MywnkEq.png" style="zoom:67%;" /></p><p>建樹為一般 fit regression tree  的過程，criterion 為 mean square error，假設找到的樹結構為</p><p><img src="https://i.imgur.com/FRJtKz0.png" alt=""></p><p>可以看到綠色為 leaf node，所有的 data  sample $x$ 都被歸到特定 leaf node 下</p><p><img src="https://i.imgur.com/obH8T1T.png" alt="" style="zoom:67%;" /></p><ul><li>ep_1_leaf_index 表 data sample $x_i$ 所屬的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>現在每個 leaf node 下有數個 data sample $x$ ，但每個 leaf node 只能有一個值作為輸出， 決定每個 leaf node 的輸出公式如下</p><script type="math/tex; mode=display">\cfrac{\sum residual_i}{\sum [\textit{previous probability} \times \textit{(1 - previous probability)}]}</script><ul><li>分子是 each leaf node 下的 data sample $x$ 的 residual 和</li><li>分母的 previous probability 為 $m -1$  步 GBDT 輸出的 probability $p = \sigma(F(x))$ 。<br>在這個 epoch 是指 $F_0(x)$</li></ul><p>經過計算後，每個 leaf node 輸出</p><p><img src="https://i.imgur.com/j7I1oVk.png" alt=""></p><p><img src="https://i.imgur.com/Sasd4Ei.png" style="zoom:67%;" /></p><ul><li>ep_0_prob  表 $\sigma(F_0(x))$ 計算出的 probability of loving Troll2</li><li>ep_1_residual 表 new tree $f_1(x)$ 要擬合的 residual ，其值來自 love_troll2 - ep_0_prob</li><li>ep_1_leaf_output 表 data sample $x$ 在  tree $f_1(x)$ 中的輸出值，相同的 leaf node 下會有一樣的值</li></ul><h4 id="D-update-F-1-x-with-new-tree-f-1-x"><a href="#D-update-F-1-x-with-new-tree-f-1-x" class="headerlink" title="(D) update $F_1(x)$  with new tree $f_1(x)$"></a>(D) update $F_1(x)$  <strong>with new tree $f_1(x)$</strong></h4><p>現在 epoch $m=1$，只建成一顆樹 $f_1(x)$ ， 所以 GBDT classifier 輸出 $log(odds)$ 為 </p><script type="math/tex; mode=display">F_1(x) = F_0(x) + \textit{learning rate} \times  f_1(x)</script><p>輸出的 probability 為 $\sigma(F_1(x))$</p><p>令 $\textit{learnign rate = 0.8}$，得到 epoch 2 每個  data sample 的 $\log(odds)$  prediction 與 probability prediction</p><p><img src="https://i.imgur.com/xt6rxMA.png" style="zoom:67%;" /></p><ul><li>ep_1_pre 為 $F_1(x)$ 輸出的 $log(odds)$</li><li>ep_1_prob 為 $F_1(x)$ 輸出的 probability $\sigma(F_1(x))$</li></ul><hr><h3 id="At-Epoch-m-2"><a href="#At-Epoch-m-2" class="headerlink" title="At Epoch m = 2"></a>At Epoch m = 2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_1(x)$"></a>(A) Calculate Residuals of $F_1(x)$</h4><p>計算上一步 $\textit{residual of } F_1(X)$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p><img src="https://i.imgur.com/mIWXSGC.png" style="zoom:67%;" /></p><ul><li>ep_1_prob 表 $F_1(x)$ 輸出的 $probability$ $\sigma(F_1(x))$</li><li>ep_2_residual 表 target love_troll2 與 ep_1_prob 間的 $residual$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>用 data sample x 的 columns “like_popcor”, “age”, “favorite_color”  擬合 ep_2_residual   build a new tree $f_2(x)$</p><p><img src="https://i.imgur.com/levLwV4.png" style="zoom:67%;" /></p><p> 假設得到 $f_2(x)$ 的樹結構：</p><p><img src="https://i.imgur.com/XMBr1KE.png" alt=""></p><p> 每個 data sample 對應的 leaf index<img src="https://i.imgur.com/qM6crwo.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_index 表 data sample  對應到 $f_2(x)$  上的 leaf node index</li></ul><h4 id="D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#D-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$"></a>(D) Compute Each Leaf Node’s Output of New Tree $f_2(x)$</h4><p>計算 $f_2(x)$ 下每個 leaf node 的輸出:</p><p><img src="https://i.imgur.com/MOaIis1.png" alt=""></p><p>對應到 data sample 上:</p><p><img src="https://i.imgur.com/yKumKHj.png" style="zoom:67%;" /></p><ul><li>ep_2_leaf_output 表 data sample $x$ 在 $f_2(x)$ 的輸出值，相同 leaf node  下會有一樣的值</li></ul><h3 id="Update-F-2-x-with-New-Tree-f-2-x"><a href="#Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="Update $F_2(x)$  with New Tree $f_2(x)$"></a>Update $F_2(x)$  with New Tree $f_2(x)$</h3><p>到目前為止，總共建立了兩顆樹 $f_1(x), f_2(x)$，所以 GBDT 輸出的 $log(odds)$為</p><p>$F_2(x) = F_0(x) + \nu(f_1(x) + f_2(x))$</p><ul><li>$\nu$ 為 learning rate，假設為 0.8</li></ul><p>GBDT 輸出的 probability 為 $\sigma(F_2(x))$，計算 epoch 2 的 prediction of  probability of loving troll2:</p><p><img src="https://i.imgur.com/6Lxi8mZ.png" style="zoom:67%;" /></p><ul><li>love_toll2: our target</li><li>ep_0_pre 表 $F_0(x)$</li><li>ep_1_leaf_output 表 data sample x​  在第一顆樹 $f_1(x)$ 的輸出值</li><li>ep_2_leaf_output 表 data sample x 在第二顆樹  $f_2(x)$  的輸出值</li><li>ep_2_pre 表 $F_2(x)$ 對 data sample x​ 輸出的 $log(odds)$</li><li>ep_2_prob 表 $F_2(x)$ 對 data sample x​ 的 probability: $\sigma(F_2(x))$</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p>Output GBDT fitted model $F_M(x)$</p></blockquote><p>把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/MHQLmVE.png" style="zoom:67%;" /></p><p>$F_M(x)$ 的每棵樹 $f_m(x)$  都是去 fit  $F_{m-1}(x)$ 與 target label love_troll2 之間的 $residual$</p><script type="math/tex; mode=display">residual = observed - \textit{predicted probability}</script><p>所以 $F_m(x)$ 又可以寫成</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><p>這邊很容易搞混 $log(odds)$ 與 probability，實際上只要記住：</p><ul><li>$F_m(x)$ 輸出 $log(odds)$</li><li>$residual$  的計算與 probability 有關</li></ul><h1 id="GBDT-Classifier-背後的數學"><a href="#GBDT-Classifier-背後的數學" class="headerlink" title="GBDT Classifier 背後的數學"></a>GBDT Classifier 背後的數學</h1><h3 id="Q-為什麼用-cross-entropy-做為-loss-function"><a href="#Q-為什麼用-cross-entropy-做為-loss-function" class="headerlink" title="Q: 為什麼用 cross entropy 做為 loss function ?"></a>Q: 為什麼用 cross entropy 做為 loss function ?</h3><p>在分類問題上，我們預測的是 $\textit{The probability of loving Troll 2}$  $P(Y|x)$，$\textit{}$ 以 $maximize$ $\textit{log likelihood}$ 來解 $P(Y|x)$。</p><p>令 GBDT - classification tree 的  probability prediction 為 $P(Y| x) = \sigma(F(x))$，則 objective function 為 </p><script type="math/tex; mode=display">\textit{log (likelihood of the obersved data given the prediction) }  \\= \sum_{i=1}^N [y_i log(p) + (1-y_i)log(1-p)]</script><ul><li>$p = P(Y=1|x)$，表 the probability of loving movie Troll 2</li><li>$y_i$ : observation  of data sample $x_i$ loving Troll 2 or not<ul><li>$y \in \{1, 0\}$</li></ul></li></ul><p>而 $\textit{maximize log likelihood = minimize negative log likelihood}$，objective funtion 改寫成</p><script type="math/tex; mode=display">\textit{objective function} = - \sum^N_{i=1}y_i log(p) + (1-y_i) log(1 - p)</script><p>所以 $\textit{loss function} = -[y log(p) + (1-y)log(1-p)]$</p><p>把 loss function 用 $odds$ 表示：</p><script type="math/tex; mode=display">\begin{aligned} -[y log(p) + (1-y)log(1-p)] & = -ylog(p)-(1-y)log(1-p) \\ &= -ylog(p)-log(1-p) + ylog(1-p) \\ &= -y[log(p) - log(1-p)] - log(1-p)  \\ & = -ylog(odds) - log(1-p) \\ &= -ylog(odds) + log(1 + \exp^{log(odds)}) \end{aligned}</script><ul><li>第三個等號 到 第四個等號用到 $odds=\cfrac{p}{1-p}$</li><li>第四個等號 到 第五個等號用到 $p = \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}$ 這個結論<ul><li>$log(1-p) = log(1- \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}) = log(\cfrac{1}{1 + \exp^{log(odds)}}) = -log(1 + \exp^{log(odds)})$</li></ul></li></ul><p>把 loss  function 表示成 odds 的好處是， $-ylog(odds) + log(1 + \exp^{log(odds)})$ 對 $log(odds)$  微分形式很簡潔</p><script type="math/tex; mode=display">\cfrac{d}{d \ log(odds)} -ylog(odds) + log(1 + \exp^{log(odds)}) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}} = -y + p</script><p>loss function 對 $log(odds)$ 的微分，既可以以 $log(odds)$ 表示，也可以以 probability $p$ 表示</p><ul><li>以 $log(odds)$ 表示：  $\cfrac{d}{d log(odds)}L(y_i, p) = -y -\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}$</li><li>以 $p$ 表示：$\cfrac{d}{d log(odds)}L(y_i, p) = -y + p$</li></ul><p>用 $p$ 表示時，loss function 對 $log(odds)$ 的微分</p><script type="math/tex; mode=display">-y + p = \textit{ -(observed  - predicted) = negative residual}</script><h3 id="Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false"><a href="#Q-為什麼-F-0-x-可以直接計算-log-cfrac-count-true-count-false" class="headerlink" title="Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?"></a>Q: 為什麼 $F_0(x)$ 可以直接計算 $log(\cfrac{count(true)}{count(false)})$ ?</h3><blockquote><p>來自 Step 1 的問題</p></blockquote><p>根據選定的  loss function </p><script type="math/tex; mode=display">\textit{loss function} = -[y log(p) + (1-y)log(1-p)]</script><ul><li>$P(Y=1|x) = p$ 為出現正類的 probability</li><li>$y \in \{1, 0\}$</li></ul><p>將 loss  function 以 $log(odds)$ 表示</p><script type="math/tex; mode=display">-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \exp^{log(odds)})]</script><p>$F_0(x)$ 為能使 $\textit{cost function}$ 最小的 $log(odds): \gamma$</p><script type="math/tex; mode=display">F_0(x) = argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma) = argmin_\gamma \sum^n_{i=1} -[y_ilog(odds) + log(1 + \exp^{log(odds)})]</script><ul><li>$n$ 為 number of data sample $x$</li></ul><p>令 $n$中，正樣本 $n^{(1)}$; 負樣本 $n^{(0)}$，$n = n^{(1)} + n^{(0)}$</p><p>cost  function 對 $log(odds)$  微分取極值：</p><script type="math/tex; mode=display">\begin{aligned}& \cfrac{d}{d log(odds)}\sum^n_{i=1} -[y_ilog(odds) + log(1 + \exp^{log(odds)})]   \\ & = \cfrac{d}{d log(odds)}\sum^{n^{(1)}}_i -(log(odds) + log(1 + exp^{log(odds)})) - \sum^{n^{(0)}}_j (0 * log(odds) + log(1 + \exp^{log(odds)}))  \\& = \cfrac{d}{dlog(odds)} -n^{(1)} \times (log(odds) + log(1 + exp^{log(odds)})) - n^{(0)} \times log(1 + \exp^{log(odds)}) \\ & =0\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} & n^{(1)} \times(-1 + \cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}})   + n^{(0)} \times(\cfrac{\exp^{log(odds)}}{1 + \exp^{log(odds)}}) \\ &= - n^{(1)} + n^{(1)}p + n^{(0)}p \\ & = - n^{(1)} + n p \\ &= 0 \end{aligned}</script><p>移項得到  $p$</p><script type="math/tex; mode=display">p = \cfrac{n^{(1)}}{n}</script><script type="math/tex; mode=display">log(odds) = \cfrac{p}{1-p} = \cfrac{n^{(1)}}{n^{(0)}}</script><p>故得證，給定 $\textit{loss function }  = -[y log(p) + (1-y)log(1-p)]$， 能使 $argmin_{\gamma}\sum^n_{i=1} L(y_i, \gamma)$  的 $\gamma$ 為 </p><script type="math/tex; mode=display">log(odds)= \cfrac{n^{(1)}}{n^{(0)}}</script><script type="math/tex; mode=display">\therefore F_0(x) = \cfrac{n^{(1)}}{n^{(0)}}</script><h3 id="Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q-為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q: 為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h3><blockquote><p>問題來自 Step 2 - (A)</p></blockquote><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，classifier 是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum -[y log(p) + (1-y)log(1-p)]</script><ul><li>$y$ 為 target label</li><li>$p = P(Y=1|x)$ 表正類的 probability</li></ul><p>注意，$F(x)$ 輸出的是 log(odds)，恆量 loss 是 probability $p$ 與 target label $y$ 的 cross entropy </p><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x) \hAar   F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \  L(y, F_{m-1}(x)) \\ &= -(-(y - p)) \\ &= -(-(observed - \textit{predict probability})) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent 的方向和值， 也就是 residual</strong></p><h3 id="Q-leaf-node-的輸出公式怎麼來的？"><a href="#Q-leaf-node-的輸出公式怎麼來的？" class="headerlink" title="Q: leaf node 的輸出公式怎麼來的？"></a>Q: leaf node 的輸出公式怎麼來的？</h3><blockquote><p>問題來自 Step 2-(C)</p></blockquote><p>在 new tree $f_m(x)$ 結構確定後，我們要計算每個 leaf node $j$ 的唯一輸出 $\gamma_{jm}$，使的 cost function 最小</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &= argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma) \\ &= argmin_\gamma \sum_{x_i \in R_{j, m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$j$ 表 leaf node index</li><li>$m$ 表第 $m$ 步</li><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample $x$ 集合</li></ul><p>將 loss function 以 $log(odds)$ 表示後的 objective function</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><ul><li>$-[y log(p) + (1-y)log(1-p)] = -[ylog(odds) + log(1 + \exp^{log(odds)})]$<ul><li>$F_{m-1}(x)$ 輸出為 $log(odds)$</li></ul></li></ul><p>cost function  對 $\gamma$  微分求極值不好處理，換個思路，利用 second order Tylor Polynomoal 逼近  loss function 處理</p><script type="math/tex; mode=display">\begin{aligned}\gamma_{j,m} &=  argmin_\gamma \sum_{x_i \in R_{j,m}} -y_i \times [F_{m-1}(x_i) + \gamma] + log(1 + \exp^{F_{m-1}(x_i) + \gamma }) \end{aligned}</script><p>讓  2nd Tyler approximation of $L(y_i, F_{m-1}(x_i) + \gamma)$ 在 $F_{m-1}(x)$ 處展開</p><script type="math/tex; mode=display">L(y_i, F_{m-1}(x_i) + \gamma) \approx L(y_i, F_{m-1}(x_i) ) + \cfrac{d}{d (F_{m-1}(x))} L(y_i, F_{m-1}(x_i))\gamma   + \cfrac{1}{2} \cfrac{d^2}{d (F_{m-1}(x) )^2}L(y_i, F_{m-1}(x_i))\gamma^2</script><p>將 cost function 對 $\gamma$ 微分取極值，求  $\gamma_{j,m}$</p><script type="math/tex; mode=display">\sum_{x_i \in R_{jm}} \cfrac{d}{d\gamma}  L(y_i, F_{m-1}(x_i), \gamma) \approx \sum_{x_i \in R_{jm}} (\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) + \cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))\gamma) = 0</script><p>移項得到 $\gamma$</p><script type="math/tex; mode=display">\gamma = \cfrac{\sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i))} L(y_i, F_{m-1}(x_i))}{\sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i) )^2}L(y_i, F_{m-1}(x_i))}</script><p>分子是  derivative of Loss function ;   分母是  second derivative of loss function</p><ul><li><p>分子部分:</p><script type="math/tex; mode=display">\begin{aligned} & \sum_{x_i \in R_{jm} }-\cfrac{d}{d(F_{m-1}(x_i) )} L(y_i, F_{m-1}(x_i)) \\& = \sum \cfrac{d}{d(F_{m-1}(x_i))}  \ y_i \times [F_{m-1}(x_i)  ] - log(1 + \exp^{F_{m-1}(x_i)  }) \\ &=  \sum (y_i - \cfrac{\exp^{^{F_{m-1}(x_i) }}}{1 + \exp^{^{F_{m-1}(x_i) }}} ）\\& = \sum_{x_i \in R_{jm}} (y_i -p_i) \end{aligned}</script><ul><li><p>$F_{m-1}(x_i)$  是 $m-1$  步時 $classifier$  輸出的 $log(odds)$</p><p><strong>分子部分為 $\textit{summation of residual}$</strong></p></li></ul></li><li><p>分母部分</p></li></ul><script type="math/tex; mode=display">\begin{aligned}& \sum_{x_i \in R_{jm} }\cfrac{d^2}{d (F_{m-1}(x_i))^2} L(y_i, F_{m-1}(x_i)) \\ & = \sum_{x_i \in R_{jm} } \cfrac{d^2}{d \, \ (F_{m-1}(x_i))^2} \, -[y_i  \times F_{m-1}(x_i) - log(1 + \exp^{F_{m-1}(x_i)})] \\ &= \sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)}-[y_i  - \cfrac{\exp^{^{F_{m-1}(x_i) }}}{1 + \exp^{^{F_{m-1}(x_i) }}}] \\ & =\sum_{x_i \in R_{jm} } \cfrac{d}{d F_{m-1}(x_i)} -[y_i - (1 + \exp^{F_{m-1}(x_i)})^{-1} \times \exp^{F_{m-1}(x_i)}]  \\ & = \sum_{x_i \in R_{jm} }-[(1 + \exp^{F_{m-1}(x_i)})^{-2} \exp^{F_{m-1}(x_i)}\times \exp^{F_{m-1}(x_i)} - (1+ \exp^{F_{m-1}(x_i)})^{-1}  \times \exp^{F_{m-1}(x_i)}  ]\\&=  \sum_{x_i \in R_{jm} } \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \quad + \quad \cfrac{\exp^{F_{m-1}(x_i)}}{1+ exp^{F_{m-1}(x_i)}} = \sum_{x_i \in R_{jm} }\cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2}   \ + \ \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \times \cfrac{(1 + \exp^{F_{m-1}(x_i)})}{1 + \exp^{F_{m-1}(x_i)}}\\&= \sum_{x_i \in R_{jm} } \cfrac{-\exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} + \cfrac{\exp^{F_{m-1}(x_i)} + \exp^{2 * F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} = \sum_{x_i \in R_{jm} }\cfrac{\exp^{F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})^2} \\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp^{F_{m-1}(x_i)}}{(1 + \exp^{F_{m-1}(x_i)})} \times \cfrac{1}{(1 + \exp^{F_{m-1}(x_i)})}\\ &= \sum_{x_i \in R_{jm} } \cfrac{\exp^{log(odds)_i}}{1 + \exp^{log(odds)_i}} \times \cfrac{1}{1 + \exp^{log(odds)_i}}\\ &= \sum_{x_i \in R_{jm} } p_i \times (1-p_i)\end{aligned}</script><p>綜合分子分母，能使 $F_m(x)$  cost function 最小化的  tree  $f_m(x)$   第 $j$ 個  leaf node 輸出為</p><script type="math/tex; mode=display">\gamma_{jm}= \cfrac{\sum_{x_i \in R_{jm})} (y_i - p_i)}{\sum_{x_i \in R_{jm} }(p_i \times (1- p_i))} = \cfrac{\textit{summation of  residuals }}{\textit{summantion of (previous probability $\times$ (1 - previoous probability))}}</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><h2 id="Data-Sample"><a href="#Data-Sample" class="headerlink" title="Data Sample"></a>Data Sample</h2><blockquote><p>learning by doing it</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">12</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">87</span>, <span class="string">&#x27;gree&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">44</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">True</span>, <span class="number">19</span>, <span class="string">&#x27;red&#x27;</span>, <span class="literal">False</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">32</span>, <span class="string">&#x27;green&#x27;</span>, <span class="literal">True</span>),</span><br><span class="line">    (<span class="literal">False</span>, <span class="number">14</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="literal">True</span>)</span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;like_popcorn&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;love_troll2&#x27;</span>]</span><br><span class="line">target = <span class="string">&#x27;love_troll2&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.youtube.com/watch?v=jxuNLH5dXCs&amp;">Gradient Boost Part 3 (of 4): Classification</a></p><p><a href="https://www.youtube.com/watch?v=StWY5QWMXCw&amp;t">Gradient Boost Part 4 (of 4): Classification Details</a></p><ul><li>Gradient Boosting In Classification: Not a Black Box Anymore! <a href="https://blog.paperspace.com/gradient-boosting-for-classification/">https://blog.paperspace.com/gradient-boosting-for-classification/</a><ul><li>statquest 整理</li></ul></li><li>StatQuest: Odds Ratios and Log(Odds Ratios), Clearly Explained!!! <a href="https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s">https://www.youtube.com/watch?v=8nm0G-1uJzA&amp;t=925s</a></li><li>The Logit and Sigmoid Functions <a href="https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/">https://nathanbrixius.wordpress.com/2016/06/04/functions-i-have-known-logit-and-sigmoid/</a></li><li>Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends <a href="https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979">https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一步步透視 GBDT Regression Tree</title>
      <link href="GBDT-Rregression-Tree-Step-by-Step/"/>
      <url>GBDT-Rregression-Tree-Step-by-Step/</url>
      
        <content type="html"><![CDATA[<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h1><ul><li><p>訓練完的 GBDT 是由多棵樹組成的 function set $f_1(x), …,f_{M}(x)$</p><ul><li>$F_M(x) = F_0(x) + \nu\sum^M_{i=1}f_i(x) $</li></ul></li><li><p>訓練中的 GBDT，每棵新樹  $f_m(x)$ 都去擬合  target $y$ 與 $F_{m-1}(x)$ 的 $residual$，也就是 $\textit{gradient  decent}$  的方向 </p><ul><li>$F_m(x) = F_{m-1}(x) + \nu f_m(x)$</li></ul></li></ul><p><img src="https://i.loli.net/2021/01/23/ZWgsyMOw5LoQxFz.png" alt="Golf Boosting"></p><a id="more"></a><p>結論說完了，想看數學原理請移駕 <a href="#math">背後的數學</a>，想看例子從無到有生出 GBDT 的請到 <a href="#alg">Algorithm - step by step</a> ，想離開得請按上一頁。</p><h1 id="GBDT-簡介"><a href="#GBDT-簡介" class="headerlink" title="GBDT 簡介"></a>GBDT 簡介</h1><p>GBDT-regression tree 簡單來說，訓練時依序建立 trees $\{ f_1(x), f_2(x), …. , f_M(x)\}$，每顆 tree $f_m(x)$ 都站在 $m$ 步前已經建立好的 trees $f_1(x), …f_{m-1}(x)$ 的上做改進。</p><p>所以 GBDT - regression  tree 的訓練是 sequentially ，無法以並行訓練加速。</p><p>我們把 GBDT-regression tree 訓練時第 $m$ 步的輸出定義為 $F_m(x)$，則其迭代公式如下:</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu\sum^m_{i=1}f_i(x) = F_{m-1}(x)  + \nu f_m(x)</script><ul><li>$\nu$ 為 learning rate</li></ul><p>GBDT 訓練迭代的過程可以生動的比喻成 playing golf，揮這一杆 $F_m(x)$ 前都去看 “上一杆 $F_{m-1}(x)$ 跟 flag y 還差多遠” ，再揮杆。</p><p>差多遠即 residual 的概念：</p><script type="math/tex; mode=display">\textit{residual = observed - predicted}</script><p>因此可以很直觀的理解，GBDT 每建一顆新樹，都是基於上一步的 residual</p><p><a name="alg"><a> </p><h1 id="GBDT-Algorithm-step-by-step"><a href="#GBDT-Algorithm-step-by-step" class="headerlink" title="GBDT Algorithm - step by step"></a>GBDT Algorithm - step by step</h1><p>Algorithm 參考了 statQuest 對 GBDT 的講解，連結放在  reference，必看！</p><p>GBDT-regression tree 擬合 algorithm：</p><h2 id="Input-Data-and-Loss-Function"><a href="#Input-Data-and-Loss-Function" class="headerlink" title="Input Data and Loss Function"></a><img src="https://i.loli.net/2021/01/23/6BuRsPyvaM7GULc.png" alt="Algorithm of GBDT">Input Data and Loss Function</h2><blockquote><p>input Data $\{(x_i, y_i)\}^n_{i=1}$ and a differentiable <strong>Loss Function</strong>  $L(y_i, F(x))$</p></blockquote><h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p><img src="https://i.imgur.com/yYJ9IpX.png" alt=""></p><p>接下來都用這組簡單的數據，其中</p><ul><li>Target $y_i$ : weight which is what we want to predict</li><li>$x_i$: features 的組成有 身高，喜歡的顏色，性別</li></ul><p>目標是用 $x_i$ 的 height, favorite color, gender  來預測  $y_i$ 的 weight</p><p>loss function 為 square error </p><script type="math/tex; mode=display">L(y_i, F(x)) = \cfrac{1}{2}(\textit{observed - predicted}) ^2 = \cfrac{1}{2}(y_i^2 - F(x))^2</script><ul><li><p>square error commonly use in Regression with Gradient Boost</p></li><li><p>$\textit{observed - predicted}$  is called  $residual$</p></li><li><p>$y_i$ are observed value</p></li><li><p>$F(x)$: the function which give us the predicted value</p><ul><li><p>也就是所有 regression tree set $(f_1, f_2, f_3, ….)$ 的整合輸出</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)</script><script type="math/tex; mode=display">F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)</script></li></ul></li></ul><p><a name="step1"></a></p><h2 id="Step-1-Initialize-Model-with-a-Constant-Value"><a href="#Step-1-Initialize-Model-with-a-Constant-Value" class="headerlink" title="Step 1 Initialize Model with a Constant Value"></a>Step 1 Initialize Model with a Constant Value</h2><p>初始 GBDT 模型 $F_0(x)$ 直接把所有 Weight 值加起來取平均即可</p><p><img src="https://i.imgur.com/uamGKg1.png" alt=""></p><p>取  weight 平均得到 $F_0(x) = 71.2 = \textit{average weight}$</p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>For $m=1$ to $M$，重複做以下的事</p><ol><li>(A) calculate residuals of $F_{m-1}(x)$</li><li>(B) construct new regression tree $f_m(x)$</li><li>(C) compute each leaf node’s output of  new tree $f_m(x)$</li><li>(D) update $F_m(x)$  with new tree $f_m(x)$</li></ol><hr><h3 id="At-epoch-m-1"><a href="#At-epoch-m-1" class="headerlink" title="At epoch m=1"></a>At epoch m=1</h3><p><a name="step2A"></a></p><h4 id="A-Calculate-Residuals-of-F-0-x"><a href="#A-Calculate-Residuals-of-F-0-x" class="headerlink" title="(A) Calculate Residuals of $F_{0}(x)$"></a>(A) Calculate Residuals of $F_{0}(x)$</h4><p>epoch $m =1$，對每筆 data sample $x$ 計算與 $F_{0}(x)$ 的 residuals </p><p>​    <script type="math/tex">residuals = \textit{observed weight - predicted weight}</script></p><p>而 $F_0(x) = \textit{average weight = 71.17}$</p><p>計算 residual 後: </p><p><img src="https://i.imgur.com/FmhobDH.png" alt=""></p><ul><li>epoch_0_prediction 表 $F_0(x)$ 輸出</li><li>epoch_1_residual 表 residual between observed weight and predicted weight $F_0(x)$</li></ul><h4 id="B-Construct-New-Regression-Tree-f-1-x"><a href="#B-Construct-New-Regression-Tree-f-1-x" class="headerlink" title="(B) Construct New Regression Tree $f_1(x)$"></a>(B) Construct New Regression Tree $f_1(x)$</h4><p>此階段要建立新樹擬合 (A) 算出的  residual </p><p>用 columns $\textit{height, favorite, color, gender}$  預測 $residuals$ 來建新樹 $f_1(x)$</p><p><img src="https://i.imgur.com/Vg5cKjw.png" alt=""></p><p>建樹的過程為一般的 regression tree building 過程，target 就是 residuals。</p><p>假設我們找到分支結構是</p><p><img src="https://i.imgur.com/qmuUdME.png" alt=""></p><p>綠色部分為 leaf node，data sample $x$ 都被歸到 tree $f_1(x)$  特定的 leaf node 下。</p><p><img src="https://i.imgur.com/C0P4ukP.png" alt=""></p><ul><li>epoch_1_leaf_index，表 data sample $x$ 歸屬的 leaf node index</li></ul><p><a name="step2C"></a></p><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-1-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_1(x)$</h4><p>此階段藥決定 new tree $f_1(x)$ 每個 leaf node 的輸出值</p><p>直覺地對每個 leaf node 內的 data sample $x$  weight 值取平均，得到輸出值</p><p><img src="https://i.imgur.com/wruHARq.png" alt=""></p><p><img src="https://i.imgur.com/ausJcrf.png" alt="Untitled 9"></p><ul><li>epoch_1_leaf_output 表 data sample $x$ 在 tree $f_1(x)$ 中的輸出值</li></ul><p><a name="step2D"></a></p><h4 id="D-Update-F-1-x-with-New-Tree-f-1-x"><a href="#D-Update-F-1-x-with-New-Tree-f-1-x" class="headerlink" title="(D) Update $F_1(x)$  with New Tree $f_1(x)$"></a>(D) Update $F_1(x)$  with New Tree $f_1(x)$</h4><p>此階段要將新建的 tree $f_m(x)$ 合併到 $F_{m-1}(x)$ 迭代出 $F_m(x)$</p><p>現在 $m=1$，所以只有一顆 tree $f_1(x)$，所以 $F_1(x) = F_0(x) + \textit{learning rate } \times f_1(x)$</p><p>假設 $\textit{learning rate = 0.1}$ ，此時 $F_1(x)$ 對所有 data sample $x$ 的 weight 預測值如下</p><p><img src="https://i.imgur.com/1fNc1dF.png" alt=""></p><ul><li>epoch_1_prediction 表 data sample $x$ 在 $F_1(x)$ 的輸出，也就是擬合出的 weight 的值</li></ul><hr><h3 id="At-epoch-m-2"><a href="#At-epoch-m-2" class="headerlink" title="At epoch m=2"></a>At epoch m=2</h3><h4 id="A-Calculate-Residuals-of-F-1-x"><a href="#A-Calculate-Residuals-of-F-1-x" class="headerlink" title="(A) Calculate Residuals of $F_{1}(x)$"></a>(A) Calculate Residuals of $F_{1}(x)$</h4><p>m = 2 新的 residual between observed weight and $F_1(x)$如下</p><p><img src="https://i.imgur.com/NMS5YJ0.png" alt=""></p><ul><li>epoch_1_prediction 為 $F_1(x)$  的輸出</li><li>epoch_2_residual 為 observed weight  與 predicted weight $F_1(x)$ 的 residual</li></ul><h4 id="B-Construct-New-Regression-Tree-f-2-x"><a href="#B-Construct-New-Regression-Tree-f-2-x" class="headerlink" title="(B) Construct New Regression Tree $f_2(x)$"></a>(B) Construct New Regression Tree $f_2(x)$</h4><p>建一顆新樹擬合 epoch 2 (A)  得出的 residual</p><p><img src="https://i.imgur.com/bcUyFaW.png" alt=""></p><ul><li>epoch_2_residual 為 $f_2(x)$ 要擬合的 target</li></ul><p>假設 $f_2(x)$ 擬合後樹結構長這樣</p><p><img src="https://i.imgur.com/pJJZRC6.png" alt=""></p><p><img src="https://i.imgur.com/eNDlBn4.png" alt=""></p><ul><li>epoch_2_leaf_index 表 data sample $x$ 在 tree $f_2(x)$ 對應的 leaf node index</li></ul><h4 id="C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x"><a href="#C-Compute-Each-Leaf-Node’s-Output-of-New-Tree-f-2-x" class="headerlink" title="(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$"></a>(C) Compute Each Leaf Node’s Output of  New Tree $f_2(x)$</h4><p>決定 new tree $f_2(x)$ 每個 leaf node 的輸出值，對每個  leaf node 下的 data sample $x$ 取平均</p><p><img src="https://i.imgur.com/SM21atv.png" alt=""></p><ul><li>epoch_2_leaf_output 表 tree $f_2(x)$ 的輸出值。記住 ， $f_2(x)$ 擬合的是  residual epoch_2_residual</li></ul><h4 id="D-Update-F-2-x-with-New-Tree-f-2-x"><a href="#D-Update-F-2-x-with-New-Tree-f-2-x" class="headerlink" title="(D) Update $F_2(x)$  with New Tree $f_2(x)$"></a>(D) Update $F_2(x)$  with New Tree $f_2(x)$</h4><p>到目前為止我們建立了兩顆 $tree$  $f_1(x), f_2(x)$，假設  $\textit{learning rate = 0.1}$，則 $F_2(x)$ 為</p><script type="math/tex; mode=display">F_2(x) = F_0(x) + 0.1(f_1(x) + f_2(x))</script><p>每個  data sample  在 $F_2(x)$ 的 predict 值如下圖：</p><p><img src="https://i.imgur.com/wbbpfNc.png" alt=""> </p><ul><li>weight: out target value</li><li>epoch_0_prediction $F_0(x)$ 對每個 data sample $x$ 的輸出值</li><li>epoch_1_leaf_output: $f_1(x)$ epoch 1 的 tree 擬合出的 residual</li><li>epoch_2_leaf_output: $f_2(x)$ epoch 2 的 tree 擬合出的 residual</li><li>epoch_2_prediction:  $F_2(x)$ epoch 2 GDBT-regression tree 的整體輸出</li></ul><h2 id="Step-3-Output-GBDT-fitted-model"><a href="#Step-3-Output-GBDT-fitted-model" class="headerlink" title="Step 3 Output GBDT fitted model"></a>Step 3 Output GBDT fitted model</h2><blockquote><p> Output GBDT fitted model $F_M(x)$</p></blockquote><p>Step 3 輸出模型 $F_M(x)$，把 $\textit{epoch 1 to M}$ 建的 tree $f_1(x),f_2(x), …..f_M(x)$ 整合起來就是 $F_M(x)$</p><script type="math/tex; mode=display">F_M(x) = F_0(x) + \nu\sum^{M}_{m=1}f_m(x)</script><p><img src="https://i.imgur.com/ZPUv6Cr.png" alt=""></p><p>$F_M(x)$ 中的每棵樹 $f_m(x)$ 都去擬合 observed target $y$  與上一步 predicted value $\hat{y}$ $F_{m-1}(x)$ 的  $residual$</p><p><a name="math"></a></p><h1 id="GBDT-Regression-背後的數學"><a href="#GBDT-Regression-背後的數學" class="headerlink" title="GBDT Regression 背後的數學"></a>GBDT Regression 背後的數學</h1><h2 id="Q-Loss-function-為什麼用-mean-square-error"><a href="#Q-Loss-function-為什麼用-mean-square-error" class="headerlink" title="Q:  Loss function 為什麼用 mean square error ?"></a>Q:  Loss function 為什麼用 mean square error ?</h2><p>​    </p><p>選擇 $\cfrac{1}{2}(\textit{observed - predicted}) ^2$  當作 loss function  的好處是對 $F(X)$ 微分的形式簡潔</p><script type="math/tex; mode=display">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script><p>其意義就是找出一個 可以最小化 loss function 的 predicted value $F(X)$， 其值正好就是 $\textit{negative  residual}$</p><p>​    $-(y_i - F(X)) = \textit{-(observed - predicted) = negative residual }  $</p><p>而我們知道 $F(X)$ 在  loss function $L(y_i, F(X))$ 的 gradient 就是 $\textit{negative residual}$ 後，那麼梯度下降的方向即 $residual$</p><h2 id="Q：為什麼初始-F-0-X-直接對-targets-取平均？"><a href="#Q：為什麼初始-F-0-X-直接對-targets-取平均？" class="headerlink" title="Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？"></a>Q：為什麼初始 $F_0(X)$ 直接對 targets 取平均？</h2><p><a href="#step1">問題來自 step 1</a></p><p>我們要找的是能使 cost function $\sum^n_{i=1}L(y_i, \gamma)$ 最小的那個輸出值 $\gamma$ 做為  $F_0(X)$。</p><p>$F_0(x) = argmin_r \sum^n_{i=1} L(y_i,\gamma)$</p><ul><li><p>$F_0(x)$ 初始化的  function，其值是常數</p></li><li><p>$\gamma$  refers to the predicted values</p></li><li><p>$n$ 是 data sample 數</p></li></ul><p><em>Proof:</em></p><p>已知</p><p>​    <script type="math/tex">\cfrac{d \ L(y_i, F(x))}{d \ F(x)} =  \cfrac{d }{d \ F(X)} \ \cfrac{1}{2}(y_i - F(X))^2 = -( y_i - F(X))</script></p><p>所以 cost function 對 $\gamma $ 微分後，是所有 sample data 跟 $\gamma$ 的 negative residual 和 為 0</p><p>​    <script type="math/tex">\sum^n_{i=1}L(y_i, \gamma) = -(y_1 - \gamma) - (y_2 - \gamma)-  ... -(y_n - \gamma) = 0</script></p><p>移項得到 $\gamma$</p><p>​    <script type="math/tex">\gamma = \cfrac{y_1 + y_2 + .... + y_n}{n}</script></p><p>正是所有 target value 的 mean，故得證</p><p>​    <script type="math/tex">F_0(x) = \gamma = \textit{the average of targets value}</script> </p><h2 id="Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？"><a href="#Q：為什麼可以直接計算-residual，他跟-loss-function-甚麼關係？" class="headerlink" title="Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？"></a>Q：為什麼可以直接計算 residual，他跟 loss function 甚麼關係？</h2><p><a href="#step2A">問題來自 step 2A</a></p><p>在 $m$ 步的一開始還沒建 新 tree $f_m(x)$  時，整體的輸出是 $F_{m-1}(x)$，此時的 cost function 是 </p><script type="math/tex; mode=display">\sum^n_{i=1} L(y_i,F_{m-1}(x_i)) = \sum \cfrac{1}{2}(y_i^2 - F(x))^2</script><p>我們接下來想建一顆新 tree $f_m(x)$ 使得 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$ 的 cost function $\sum^n_{i=1} L(y_i,F_{m}(x_i))$ 進一步變小要怎麼做？</p><p>觀察 $F_m(x) = F_{m-1}(x) + \nu f_m(x)$，是不是很像 gradient decent update 的公式</p><script type="math/tex; mode=display">F_m(x) = F_{m-1}(x) + \nu f_m(x)\hAar F_m(x) = F_{m-1}(x) - \hat{\nu} \cfrac{d}{d \ F(x)} \ L(y, F(x))</script><p>讓 $f_m(x)$ 的方向與 gradient decent 方向一致，對應一下，新的 tree $f_m(x)$ 即是</p><script type="math/tex; mode=display">\begin{aligned}f_m(x) &= - \cfrac{d}{d \ F_{m-1}(x)} \ L(y, F_{m-1}(x)) \\ &= -(-(y - F_{m-1}(x))) \\ &= -(-(observed - predicted )) \\ &= - \textit{negative residual} \\ & = residual \end{aligned}</script><p><strong>新建的 tree $f_m(x)$ 要擬合的就是 gradient decent  的方向和值， 也就是 residual</strong> </p><p>​    $f_m(x)$  = $\textit{gradient decent}$   = $\textit{negative gradient}$  = $residual$</p><p><strong>GBDT 每輪迭代就是新建一顆新 tree $f(x)$ 去擬合 $\textit{gradient  decent}$  的方向得到新的 $F(x)$</strong></p><p><strong>這也正是為什麼叫做 gradient boost</strong> 。</p><p>by the way，step 2-(A) compute residuals:</p><script type="math/tex; mode=display">r_{im} = -[\cfrac{\partial L(y_i, F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)} \ \textit{for i = 1,....,n}</script><ul><li>$i$: sample number</li><li>$m$: the tree we are trying to build</li></ul><h2 id="Q：個別-leaf-node-的輸出為什麼是取平均-？"><a href="#Q：個別-leaf-node-的輸出為什麼是取平均-？" class="headerlink" title="Q：個別 leaf node 的輸出為什麼是取平均 ？"></a>Q：個別 leaf node 的輸出為什麼是取平均 ？</h2><p><a href="#step2C">問題來自 step 2C </a></p><p>在 new tree $f_m(x)$ 結構確定後，目標是在每個 leaf node $j$ 找一個 $\gamma$ 使 cost function  最小</p><p>$\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} L(y_i, F_{m-1}(x_i) + \gamma)$</p><ul><li>$\gamma_{j,m}$ $m$ 步 第 $j$ 個 leaf node 的輸出值</li><li>$R_{jm}$ 表 $m$ 步 第 $j$ 個 leaf node，包含的 data sample 集合</li><li>$F_m(x_i) = F_{m-1}(x_i) + f_m(x_i)$， $x_i$ 在 $f_m(x)$ 的 leaf node $j$  上的輸出值為 $\gamma_{j,m}$</li></ul><script type="math/tex; mode=display">\gamma_{j,m} = argmin_\gamma \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2</script><p>直接對  $\gamma$ 微分</p><script type="math/tex; mode=display">\begin{aligned}\cfrac{d}{d \gamma} \ \sum_{x_i \in R_{j,m}} \cfrac{1}{2}(y_i - F_{m-1}(x_i) - \gamma)^2 =  \ \sum_{x_i \in R_{j,m}}-(y_i - F_{m-1}(x_i) - \gamma) = 0\end{aligned}</script><p>移項</p><script type="math/tex; mode=display">\gamma = \cfrac{1}{n_{jm}} \sum_{x_i \in R_{j,m}}y_i - F_{m-1}(x_i)</script><ul><li>$n_{jm}$ is the number of data sample in leaf node $j$ at step $m$</li></ul><p>白話說就是，第 $m$ 步 第 $j$ 個 node 輸出 $\gamma_{jm}$ 為 node $j$ 下所有 data sample 的 residuals 的平均</p><p>事實上跟一開始 $F_0(x)$ 取平均的性質是一樣的，$F_0(x)$ 一棵樹 $f(x) $  都還沒建，可以視為所有 data sample 都在同一個 leaf node 下。</p><h2 id="Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？"><a href="#Q：-如何整合-tree-set-f-1-t-f-2-t-……f-m-t-？" class="headerlink" title="Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？"></a>Q： 如何整合 tree set $f_1(t), f_2(t),……f_m(t)$ ？</h2><p><a href="#step2D">問題來自 step 2D </a></p><script type="math/tex; mode=display">\begin{aligned}F_m(x) & = F_{m-1}(x) + \nu f_m(x) \\ &= F_{m-1}(x) + \nu \sum^{J_m}_{j=1}\gamma_{jm}I(x \in R_{jm})\end{aligned}</script><ul><li>$F_{m-1}(x) = \nu\sum^{m-1}_{i=1} f_i(x)$</li><li>$\nu$ learning rate</li><li>$J_m$ m 步 的 leaf node 總數</li><li>$\gamma_{jm}$ m 步 第 $j$ 個 node 的輸出</li></ul><p>$F_m(x)$ 展開來就是</p><script type="math/tex; mode=display">F_m(x) = F_0(x) + \nu(f_1(x) + f_2(x)+ ...+f_m(t))</script><h1 id="寫在最後"><a href="#寫在最後" class="headerlink" title="寫在最後"></a>寫在最後</h1><blockquote><p>seeing is believing</p></blockquote><p>太多次看了又忘的經驗表明，我們多數只是當下理解了，過了幾天、 幾個禮拜後又會一片白紙</p><p>人會遺忘，尤其是短期記憶，只有一遍遍不斷主動回顧，才能將知識變成長期記憶</p><blockquote><p>learning by doing it</p></blockquote><p>所以下次忘記時，不妨從這些簡單的 data sample 開始，跟著 algorithm 實作一遍，相信會更清楚理解的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = [</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">88</span>),</span><br><span class="line">    (<span class="number">1.6</span>, <span class="string">&#x27;Greem&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">76</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">56</span>),</span><br><span class="line">    (<span class="number">1.8</span>, <span class="string">&#x27;Red&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">73</span>),</span><br><span class="line">    (<span class="number">1.5</span>, <span class="string">&#x27;Green&#x27;</span>, <span class="string">&#x27;Male&#x27;</span>, <span class="number">77</span>),</span><br><span class="line">    (<span class="number">1.4</span>, <span class="string">&#x27;Blue&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="number">57</span>)   </span><br><span class="line">]</span><br><span class="line">columns = [<span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;favorite_color&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>]</span><br><span class="line">df = pd.DataFrame.from_records(data, index=<span class="literal">None</span>, columns=columns)</span><br></pre></td></tr></table></figure><blockquote><p>always get your hands dirty</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h2 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h2><p><a href="https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=75s">Gradient Boost Part 1 (of 4): Regression Main Ideas</a></p><p><a href="https://www.youtube.com/watch?v=2xudPOBz-vs">Gradient Boost Part 2 (of 4): Regression Details</a></p><p>ccd comment: 上面兩個必看</p><ul><li>A Step by Step Gradient Boosting Decision Tree Example <a href="https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/">https://sefiks.com/2018/10/04/a-step-by-step-gradient-boosting-decision-tree-example/</a><ul><li>真 step by step</li></ul></li><li>Gradient Boosting Decision Tree Algorithm Explained <a href="https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4">https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4</a><ul><li>including sklearn 實作</li></ul></li></ul><h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><ul><li>Gradient Boosting Decision Tree <a href="http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95">http://gitlinux.net/2019-06-11-gbdt-gradient-boosting-decision-tree/#1-gbdt-回归算法</a></li><li>提升树算法理论详解 <a href="https://hexinlin.top/2020/03/01/GBDT/">https://hexinlin.top/2020/03/01/GBDT/</a></li><li>梯度提升树(GBDT)原理小结 <a href="https://www.cnblogs.com/pinard/p/6140514.html">https://www.cnblogs.com/pinard/p/6140514.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="ckk9zuhsn0003k0e2aler1a0b/"/>
      <url>ckk9zuhsn0003k0e2aler1a0b/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
