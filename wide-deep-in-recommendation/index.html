<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>實作 wide &amp; deep 從訓練到推薦排序 - seed9D&#039;s blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="seed9D&#039;s blog"><meta name="msapplication-TileImage" content="/images/seed.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="seed9D&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Memorization &amp;amp; Generalization在推薦系統中，如果要選第一個深度排序模型從傳統機器學習接軌到 DNN，那首推 google 在 2016 年提出的  Wide &amp;amp; Deep ，Wide &amp;amp; Deep 名稱來自其由一個 shallow 的 model 與一個 deep  的  network 組合而成。 在進入到 DNN 前，我們手上肯定有個正在線上"><meta property="og:type" content="article"><meta property="og:title" content="實作 wide &amp; deep 從訓練到推薦排序"><meta property="og:url" content="https://seed9d.github.io/wide-deep-in-recommendation/"><meta property="og:site_name" content="seed9D&#039;s blog"><meta property="og:description" content="Memorization &amp;amp; Generalization在推薦系統中，如果要選第一個深度排序模型從傳統機器學習接軌到 DNN，那首推 google 在 2016 年提出的  Wide &amp;amp; Deep ，Wide &amp;amp; Deep 名稱來自其由一個 shallow 的 model 與一個 deep  的  network 組合而成。 在進入到 DNN 前，我們手上肯定有個正在線上"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://i.imgur.com/ZlRSMGf.png"><meta property="og:image" content="https://i.imgur.com/2jHjzDW.png"><meta property="article:published_time" content="2021-02-20T15:29:06.000Z"><meta property="article:modified_time" content="2021-02-20T15:49:28.000Z"><meta property="article:author" content="seed9D"><meta property="article:tag" content="recommendation system"><meta property="article:tag" content="pytorch"><meta property="article:tag" content="推薦系統"><meta property="article:tag" content="deep learning"><meta property="article:tag" content="ranking"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.imgur.com/ZlRSMGf.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://seed9d.github.io/wide-deep-in-recommendation/"},"headline":"seed9D's blog","image":["https://i.imgur.com/ZlRSMGf.png","https://i.imgur.com/2jHjzDW.png"],"datePublished":"2021-02-20T15:29:06.000Z","dateModified":"2021-02-20T15:49:28.000Z","author":{"@type":"Person","name":"seed9D"},"description":"Memorization &amp; Generalization在推薦系統中，如果要選第一個深度排序模型從傳統機器學習接軌到 DNN，那首推 google 在 2016 年提出的  Wide &amp; Deep ，Wide &amp; Deep 名稱來自其由一個 shallow 的 model 與一個 deep  的  network 組合而成。 在進入到 DNN 前，我們手上肯定有個正在線上"}</script><link rel="canonical" href="https://seed9d.github.io/wide-deep-in-recommendation/"><link rel="icon" href="/images/seed.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="seed9D's blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/logo.svg" alt="seed9D&#039;s blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>實作 wide &amp; deep 從訓練到推薦排序</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2021-02-20</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2021-02-20</time></span><span class="level-item"><a class="link-muted" href="/categories/recommendation-system/">recommendation system</a></span><span class="level-item">28 minutes read (About 4247 words)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h1 id="Memorization-amp-Generalization"><a href="#Memorization-amp-Generalization" class="headerlink" title="Memorization &amp; Generalization"></a>Memorization &amp; Generalization</h1><p>在推薦系統中，如果要選第一個深度排序模型從傳統機器學習接軌到 DNN，那首推 google 在 2016 年提出的  Wide &amp; Deep ，Wide &amp; Deep 名稱來自其由一個 shallow 的 model 與一個 deep  的  network 組合而成。</p>
<p>在進入到 DNN 前，我們手上肯定有個正在線上運行的傳統排序模型 ex: FM, GBDT …，還有積累了很久的有效特徵組合，如果直接進入 DNN，這些經驗和積累打水漂不說，還得花很長的時間 tune 模型。</p>
<p>有了 Wide &amp; Deep，我們只需將正在 serving 的那個排序模型放在 wide side 使用，特徵照舊使用; 另外在建一個  deep side 的深度模型，兩個模型 joint training  即可無縫接軌到 DNN。</p>
<p><img src="https://i.imgur.com/ZlRSMGf.png" alt=""></p>
<p>那 wide 跟  deep network 分別代表什麼呢 ？</p>
<p>Google 在 2016 的論文中總結出：Wide 負責 memorization , Deep 負責 Generalization , Wide &amp; Deep 是 Memorization &amp; Generalization 的體現，實際上也是推薦領域中經典的 Exploitation &amp; Exploration  問題。</p>
<a id="more"></a>
<h2 id="Wide"><a href="#Wide" class="headerlink" title="Wide"></a>Wide</h2><p>Wide side  利用 LR + cross product 構造非線性特徵。傳統淺層的模型，對於曾經出現過的 feature pairs 有很強的記憶性，很適合用來 exploit 已有的訊息。</p>
<p>ex: 用戶安裝了 app A ，此時曝光 app B，用戶安裝的可能性很大，wide 網路捕捉了 install A 跟 impression  B 的關係，加以利用。</p>
<h2 id="Deep"><a href="#Deep" class="headerlink" title="Deep"></a>Deep</h2><p>Deep side 利用 neural network 的 features extraction 的特性，自動找出  feature  之間的交叉關係，就算在訓練樣本中不曾出現過或者稀少的 pair，也能學出低維 embedding，達成 generalization，屬於 “exploration”。</p>
<h3 id="如何處理-sparse-id-features"><a href="#如何處理-sparse-id-features" class="headerlink" title="如何處理 sparse id features?"></a>如何處理 sparse id features?</h3><p>對於高維且稀疏的 id 特徵，deep side 採用  embedding 技術，將高維稀疏的 id 轉換成低維稠密的 embedding 輸入  feedforward dense layer</p>
<h1 id="Implement-Wide-amp-Deep-by-Pytorch"><a href="#Implement-Wide-amp-Deep-by-Pytorch" class="headerlink" title="Implement Wide &amp; Deep by Pytorch"></a>Implement Wide &amp; Deep by Pytorch</h1><p>dataset 用 movielen ml-1m ，可以在以下連結找到</p>
<p><a target="_blank" rel="noopener" href="https://grouplens.org/datasets/movielens/">MovieLens</a></p>
<p>在推薦領域中，原則上所有的特徵會被 id 化，連續特徵會被分桶 label 化，MovieLens dataset 中的特徵恰好都已經分桶過了，只需要對特徵給予 label 即可。</p>
<h2 id="wide-特徵處理"><a href="#wide-特徵處理" class="headerlink" title="wide 特徵處理"></a>wide 特徵處理</h2><figure class="highlight"><figcaption><span>CrossFeatures >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossFeatures</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cross_col_pairs: List[Tuple[<span class="built_in">str</span>, <span class="built_in">str</span>]]</span>):</span></span><br><span class="line">        self.cross_col_pairs = cross_col_pairs</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, df: pd.DataFrame, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.unique_columns_ = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> pair <span class="keyword">in</span> self.cross_col_pairs:</span><br><span class="line">            self.unique_columns_.update(<span class="built_in">list</span>(pair))</span><br><span class="line">        </span><br><span class="line">        self.crossed_colnamed_ = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> cols <span class="keyword">in</span> self.cross_col_pairs:</span><br><span class="line">            cols = <span class="built_in">list</span>(cols)</span><br><span class="line">            new_colname = <span class="string">&quot;_&quot;</span>.join(cols)</span><br><span class="line">            self.crossed_colnamed_.append(new_colname)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, df: pd.DataFrame</span>):</span></span><br><span class="line">            df_cross = df[self.unique_columns_].copy().astype(<span class="built_in">str</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> cols <span class="keyword">in</span> self.cross_col_pairs:</span><br><span class="line">                cols = <span class="built_in">list</span>(cols)</span><br><span class="line">                new_colname = <span class="string">&quot;_&quot;</span>.join(cols)</span><br><span class="line">                df_cross[new_colname] = df_cross[cols[<span class="number">0</span>]] + \</span><br><span class="line">                    <span class="string">&#x27;-&#x27;</span> + df_cross[cols[<span class="number">1</span>]]</span><br><span class="line">            <span class="keyword">return</span> df_cross[self.crossed_colnamed_]</span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure>
<ul>
<li>class CrossFeatures 將兩個 features  做交叉，即共現的 AND 關係</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wide_cols = [<span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;zipCode&#x27;</span>]</span><br><span class="line">crossed_cols = [(<span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;age&#x27;</span>), (<span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>), (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>)]</span><br><span class="line">wideGenerator = WideFeaturesGenerator(wide_cols, crossed_cols)</span><br><span class="line">x_wide = wideGenerator.fit_transform(X)</span><br></pre></td></tr></table></figure>
<ul>
<li>我們想捕捉 “gender AND age”, “gender AND occupation”, “age AND occupation” 的關係</li>
<li>wide 側不放 continuous 特徵</li>
</ul>
<p>看看維度 x_wide  的 dimension：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print(x_wide.shape)</span><br><span class="line">Out:</span><br><span class="line">	(<span class="number">1000209</span>, <span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>wide 側總共產生 7 個特徵。 符合預期，3 組交叉特徵，4個一階特徵</li>
</ul>
<p>但實際上，x_wide 的特徵是 label 化的，每一維僅存 label 值，看一下全局 label 值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print(wideGenerator.feature_dict_)</span><br><span class="line">Out:</span><br><span class="line">	&#123;<span class="string">&#x27;gender_F&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">	 <span class="string">&#x27;gender_M&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">	 <span class="string">&#x27;age_1&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">	 <span class="string">&#x27;age_56&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">	 <span class="string">&#x27;age_25&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">	 <span class="string">&#x27;age_50&#x27;</span>: <span class="number">6</span>,</span><br><span class="line">	 <span class="string">&#x27;age_18&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">	 <span class="string">&#x27;age_45&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">	 <span class="string">&#x27;age_35&#x27;</span>: <span class="number">9</span>,</span><br><span class="line">	 <span class="string">&#x27;occupation_10&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">	 <span class="string">&#x27;occupation_16&#x27;</span>: <span class="number">11</span>,</span><br><span class="line">	 <span class="string">&#x27;occupation_12&#x27;</span>: <span class="number">12</span>,</span><br><span class="line">	 <span class="string">&#x27;occupation_7&#x27;</span>: <span class="number">13</span>,</span><br><span class="line">	 <span class="string">&#x27;occupation_1&#x27;</span>: <span class="number">14</span>,</span><br><span class="line">	</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>
<ul>
<li>可以看出，wide 側的被全局  label 化了</li>
</ul>
<p>實際的 dimension 為</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print(<span class="built_in">len</span>(wideGenerator.feature_dict_))</span><br><span class="line">Out:</span><br><span class="line">	<span class="number">3659</span></span><br></pre></td></tr></table></figure>
<h2 id="deep-特徵處理"><a href="#deep-特徵處理" class="headerlink" title="deep 特徵處理"></a>deep 特徵處理</h2><figure class="highlight"><figcaption><span>LabelEncoder >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelEncoder</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, columns_to_encode: List[<span class="built_in">str</span>]</span>):</span></span><br><span class="line">        self.columns_to_encode = columns_to_encode</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, df: pd.DataFrame, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        df_ = df[self.columns_to_encode].copy()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> self.columns_to_encode:</span><br><span class="line">            df_[self.columns_to_encode] = df[self.columns_to_encode].astype(</span><br><span class="line">                <span class="string">&#x27;str&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        unique_column_vals = &#123;col: df_[col].unique()</span><br><span class="line">                              <span class="keyword">for</span> col <span class="keyword">in</span> self.columns_to_encode&#125;</span><br><span class="line"></span><br><span class="line">        self.encoding_dict_ = <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> unique_column_vals.items():</span><br><span class="line">            self.encoding_dict_[k] = &#123;val: idx <span class="keyword">for</span> idx, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(v)&#125;</span><br><span class="line">            self.encoding_dict_[k][<span class="string">&#x27;unseen&#x27;</span>] = <span class="built_in">len</span>(self.encoding_dict_[k])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, df: pd.DataFrame</span>):</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.encoding_dict_</span><br><span class="line">        <span class="keyword">except</span> AttributeError:</span><br><span class="line">            <span class="keyword">raise</span> NotFittedError(</span><br><span class="line">                <span class="string">&quot;This LabelEncoder instance is not fitted yet. &quot;</span></span><br><span class="line">                <span class="string">&quot;Call &#x27;fit&#x27; with appropriate arguments before using this LabelEncoder.&quot;</span></span><br><span class="line">            )</span><br><span class="line">        df_ = df.copy()</span><br><span class="line">        df_[self.columns_to_encode] = df_[self.columns_to_encode].astype(<span class="string">&#x27;str&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> col, encoding_map <span class="keyword">in</span> self.encoding_dict_.items():</span><br><span class="line">            original_value = [f <span class="keyword">for</span> f <span class="keyword">in</span> encoding_map.keys() <span class="keyword">if</span> f != <span class="string">&#x27;unseen&#x27;</span>]</span><br><span class="line">            df_[col] = np.where(df_[col].isin(</span><br><span class="line">                original_value), df_[col], <span class="string">&#x27;unseen&#x27;</span>)</span><br><span class="line">            df_[col] = df_[col].apply(<span class="keyword">lambda</span> x: encoding_map[x])</span><br><span class="line">        <span class="keyword">return</span> df_</span><br><span class="line"></span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure>
<ul>
<li>class LabelEncoder 會將 id 類特徵重新編碼，給予一個連續編號</li>
</ul>
<p>首先，觀察一下每個 id 類特徵的 distinct values</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	<span class="keyword">for</span> col <span class="keyword">in</span> X:</span><br><span class="line">	    print(col, <span class="built_in">len</span>(X[col].unique()))</span><br><span class="line">Out:</span><br><span class="line">	userId <span class="number">6040</span></span><br><span class="line">	movieId <span class="number">3706</span></span><br><span class="line">	gender <span class="number">2</span></span><br><span class="line">	age <span class="number">7</span></span><br><span class="line">	occupation <span class="number">21</span></span><br><span class="line">	zipCode <span class="number">3439</span></span><br></pre></td></tr></table></figure>
<p>這讓我們大概有個底，知道每個 id 特徵要 embedding  到多少 ，下面是每個 id 類特徵 embedding 的 dimension:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">category_embed_dim_mapping = &#123;</span><br><span class="line">    <span class="string">&#x27;userId&#x27;</span>: <span class="number">50</span>,</span><br><span class="line">    <span class="string">&#x27;movieId&#x27;</span>: <span class="number">50</span>,</span><br><span class="line">    <span class="string">&#x27;gender&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">2</span>,</span><br><span class="line">    <span class="string">&#x27;occupation&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">    <span class="string">&#x27;zipCode&#x27;</span>: <span class="number">20</span></span><br><span class="line">&#125;</span><br><span class="line">category_cols = <span class="built_in">list</span>(category_embed_dim_mapping.keys())</span><br><span class="line">continuous_cols = []</span><br></pre></td></tr></table></figure>
<ul>
<li>在推薦領域，通常連續特徵都可以被轉化成 id 類特徵 (分桶 + label)，所以 continuous_cols 為空很正常</li>
</ul>
<p>將原始特徵轉換成 deep side 的特徵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">deep_generator = DeepFeaturesGenerator(category_cols, continuous_cols)</span><br><span class="line">df_deep = X[category_cols + continuous_cols].copy()</span><br><span class="line">x_deep = deep_generator.fit_transform(X)</span><br></pre></td></tr></table></figure>
<p>看看 x_deep 的 dimension</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print(x_deep.shape)</span><br><span class="line">Out:</span><br><span class="line">	(<span class="number">1000209</span>, <span class="number">6</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>符合預期，總共 6 個特徵，都是 label  化後的 id 特徵</li>
</ul>
<p>來看看每個特徵的實際維度 (onehot encoding 展開後)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print(deep_generator.embed_cols_unique_labels_)</span><br><span class="line">Out:</span><br><span class="line">	&#123;<span class="string">&#x27;userId&#x27;</span>: <span class="number">6041</span>,</span><br><span class="line">	 <span class="string">&#x27;movieId&#x27;</span>: <span class="number">3707</span>,</span><br><span class="line">	 <span class="string">&#x27;gender&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">	 <span class="string">&#x27;age&#x27;</span>: <span class="number">8</span>,</span><br><span class="line">	 <span class="string">&#x27;occupation&#x27;</span>: <span class="number">22</span>,</span><br><span class="line">	 <span class="string">&#x27;zipCode&#x27;</span>: <span class="number">3440</span>&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>利用 label 化來代替 onehot encoding，可以減少大量儲存空間</li>
</ul>
<h2 id="Wide-amp-Deep-Model-Graph"><a href="#Wide-amp-Deep-Model-Graph" class="headerlink" title="Ｗide &amp; Deep Model Graph"></a>Ｗide &amp; Deep Model Graph</h2><h3 id="Wide-Model"><a href="#Wide-Model" class="headerlink" title="Wide Model"></a>Wide Model</h3><p>wide model 很簡單，就是簡單的淺層 feedforward network</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wide</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, wide_dim: <span class="built_in">int</span>, predict_dim: <span class="built_in">int</span>=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear = nn.Embedding(wide_dim + <span class="number">1</span>, predict_dim, padding_idx=<span class="number">0</span>) <span class="comment"># reserve 1 dim for unseen cross feature</span></span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(predict_dim))</span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span>(<span class="params">self</span>):</span></span><br><span class="line">        nn.init.kaiming_normal_(self.linear.weight, a=math.sqrt(<span class="number">5</span>))</span><br><span class="line">        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.linear.weight)</span><br><span class="line">        bound = <span class="number">1</span> / math.sqrt(fan_in)</span><br><span class="line">        nn.init.uniform_(self.bias, -bound, bound)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X: torch.Tensor</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># X [b_size, num_of_wide_features]</span></span><br><span class="line">        <span class="keyword">return</span> self.linear(X.long()).<span class="built_in">sum</span>(dim=<span class="number">1</span>) + self.bias <span class="comment"># [b_size, predict_dim]</span></span><br></pre></td></tr></table></figure>
<h3 id="Deep-Model"><a href="#Deep-Model" class="headerlink" title="Deep Model"></a>Deep Model</h3><p>deep network 比較複雜，對於每個 id 類特徵，都會保存專屬的  <code>nn.Embedding</code>  , 將高維希疏向量轉變成低維稠密 embeddings </p>
<figure class="highlight"><figcaption><span>Deep >folded</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Deep</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    EMBEDDING_LAYER_PREFIX = <span class="string">&quot;emb_layer&quot;</span></span><br><span class="line">    DENSE_LAYER_PREFIX = <span class="string">&quot;dense_layer&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 columns_index: Dict[<span class="built_in">str</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 embed_cols_info: List[Tuple[<span class="built_in">str</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]],  <span class="comment"># (col_name, label_size, embeding_dim)</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 continuous_cols: List[<span class="built_in">str</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_layer_neural: List[<span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_layer_dropout: List[<span class="built_in">float</span>],</span></span></span><br><span class="line"><span class="function"><span class="params">                 embed_col_dropout: <span class="built_in">float</span>=<span class="number">0.0</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.columns_index = columns_index</span><br><span class="line">        self.embed_cols_info = embed_cols_info</span><br><span class="line">        self.continuous_cols = continuous_cols</span><br><span class="line"></span><br><span class="line">        self.embed_layers = self._create_embed_layers(embed_cols_info)</span><br><span class="line">        self.embed_dropout_layer = nn.Dropout(embed_col_dropout)</span><br><span class="line"></span><br><span class="line">        self.hidden_layer_neural = self._update_hidden_layer_neural(</span><br><span class="line">            hidden_layer_neural)</span><br><span class="line"></span><br><span class="line">        self.dense_layer = self._create_dense_layer(hidden_layer_dropout)</span><br><span class="line">        self.output_dim = hidden_layer_neural[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_embed_layers</span>(<span class="params">self, embed_cols_info: List[Tuple[<span class="built_in">str</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]]</span>):</span></span><br><span class="line">        <span class="keyword">return</span> nn.ModuleDict(&#123;self.EMBEDDING_LAYER_PREFIX + <span class="string">&#x27;_&#x27;</span> + col_name.replace(<span class="string">&quot;.&quot;</span>, <span class="string">&#x27;_&#x27;</span>): nn.Embedding(num_label, dim) <span class="keyword">for</span> col_name, num_label, dim <span class="keyword">in</span> embed_cols_info&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_dense_layer</span>(<span class="params">self, hidden_layer_dropout</span>):</span></span><br><span class="line">        dense_dequential = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.hidden_layer_neural)):</span><br><span class="line">            dense_dequential.add_module(</span><br><span class="line">                <span class="string">&quot;&#123;&#125;_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.DENSE_LAYER_PREFIX, i - <span class="number">1</span>),</span><br><span class="line">                self._create_dense_component(</span><br><span class="line">                    self.hidden_layer_neural[i-<span class="number">1</span>], self.hidden_layer_neural[i], hidden_layer_dropout[i-<span class="number">1</span>], <span class="literal">True</span>)</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">return</span> dense_dequential</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_hidden_layer_neural</span>(<span class="params">self, hidden_layer_neurals: List[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">        embed_dim = <span class="built_in">sum</span>([embed[<span class="number">2</span>] <span class="keyword">for</span> embed <span class="keyword">in</span> self.embed_cols_info])</span><br><span class="line">        continuous_dim = <span class="built_in">len</span>(self.continuous_cols)</span><br><span class="line">        <span class="keyword">return</span> [embed_dim + continuous_dim] + hidden_layer_neurals</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_dense_component</span>(<span class="params">self, input_dim: <span class="built_in">int</span>, output_dim: <span class="built_in">int</span>, dropout_ratio: <span class="built_in">float</span>=<span class="number">0.0</span>, batch_norm=<span class="literal">False</span></span>):</span></span><br><span class="line">        layers = [</span><br><span class="line">            nn.Linear(input_dim, output_dim),</span><br><span class="line">            nn.LeakyReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">if</span> batch_norm:</span><br><span class="line">            layers.append(nn.BatchNorm1d(output_dim))</span><br><span class="line">        layers.append(nn.Dropout(dropout_ratio))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get_embeding_layer</span>(<span class="params">self, embed_col</span>):</span></span><br><span class="line">        embed_col = self.EMBEDDING_LAYER_PREFIX + <span class="string">&#x27;_&#x27;</span> + embed_col.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.embed_layers[embed_col]</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, deep_input_x: torch.Tensor</span>):</span></span><br><span class="line">        embed_x = [</span><br><span class="line">           self.__get_embeding_layer(col)(deep_input_x[:, self.columns_index[col]].long())</span><br><span class="line">            <span class="keyword">for</span> col, _, _ <span class="keyword">in</span> self.embed_cols_info</span><br><span class="line">        ]</span><br><span class="line">        embed_x = torch.cat(embed_x, <span class="number">1</span>)</span><br><span class="line">        continuous_cols_idx = [self.columns_index[col]</span><br><span class="line">                              <span class="keyword">for</span> col <span class="keyword">in</span> self.continuous_cols]</span><br><span class="line">        continuous_x = deep_input_x[:, continuous_cols_idx].<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">        x = torch.cat([embed_x, continuous_x], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dense_layer(x)  <span class="comment"># [b_size, hidden_layer_last_dim]</span></span><br></pre></td></tr></table></figure></span><br></pre></td></tr></table></figure>
<ul>
<li>self.embed_layers 存放每個 id 特徵的 embeddings 向量</li>
<li>所有的 id embedding 會在 <code>dim=1</code>  concatenate</li>
</ul>
<h3 id="Wide-amp-Deep"><a href="#Wide-amp-Deep" class="headerlink" title="Wide &amp; Deep"></a>Wide &amp; Deep</h3><p>建一個 nn.Module 管理 Wide and Deep</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WideDeep</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, wide: nn.Module, deep: nn.Module</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        deep = nn.Sequential(</span><br><span class="line">            deep,</span><br><span class="line">            nn.Linear(deep.output_dim, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.wide_deep = nn.ModuleDict(&#123;</span><br><span class="line">            <span class="string">&quot;wide&quot;</span>: wide,</span><br><span class="line">            <span class="string">&quot;deep&quot;</span>: deep</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x_wide: torch.Tensor, x_deep: torch.Tensor</span>):</span></span><br><span class="line">        wide_out = self.wide_deep[<span class="string">&#x27;wide&#x27;</span>](x_wide)  <span class="comment">#  [b_size, 1]</span></span><br><span class="line">        deep_out = self.wide_deep[<span class="string">&#x27;deep&#x27;</span>](x_deep)  <span class="comment"># [b_size, 1]</span></span><br><span class="line">        out = wide_out + deep_out <span class="comment"># # [b_size, 1]</span></span><br><span class="line">        <span class="keyword">return</span> out.view(-<span class="number">1</span>) <span class="comment"># [b_size]</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x_wide: torch.Tensor, x_deep: torch.Tensor, threshold: <span class="built_in">int</span>=<span class="number">0.5</span></span>):</span></span><br><span class="line">        logistic = self.predict_probs(x_wide, x_deep)</span><br><span class="line">        <span class="keyword">return</span> (logistic &gt; threshold).<span class="built_in">int</span>()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_probs</span>(<span class="params">self, x_wide: torch.Tensor, x_deep: torch.Tensor</span>):</span></span><br><span class="line">        out = self.forward(x_wide, x_deep)</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(out.view(-<span class="number">1</span>).<span class="built_in">float</span>())</span><br></pre></td></tr></table></figure>
<ul>
<li><p>wide output 跟 deep output 結合方式如下</p>
<p>$\text{w}^T_{wide}[x, \phi(x)] + \text{w}^T_{deep}a^{(l_f)} +b$</p>
<ul>
<li>$\text{w}^T_{deep}$  將 deep  network output 轉換成 1 維的 vector</li>
<li>$a^{(l_f)}$ 為 deep network 最後一層輸出</li>
</ul>
</li>
</ul>
<h2 id="Building-Model"><a href="#Building-Model" class="headerlink" title="Building Model"></a>Building Model</h2><p>實例化 wide deep 模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">embed_cols_info = [(col, deep_generator.embed_cols_unique_labels_[col], embed_dim)<span class="keyword">for</span> col, embed_dim <span class="keyword">in</span> category_embed_dim_mapping.items()]</span><br><span class="line">deep_column_idx = &#123;col: i <span class="keyword">for</span> i, col <span class="keyword">in</span> <span class="built_in">enumerate</span>(deep_generator.deep_cols)&#125;</span><br><span class="line">hidden_layers = [ <span class="number">512</span>, <span class="number">256</span>]</span><br><span class="line">drop_out = [<span class="number">0.25</span>, <span class="number">0.2</span>]</span><br><span class="line"></span><br><span class="line">wide = Wide(wide_dim=np.unique(x_wide).shape[<span class="number">0</span>], predict_dim=<span class="number">1</span>)</span><br><span class="line">deep = Deep(deep_column_idx, embed_cols_info, continuous_cols, hidden_layers, drop_out, <span class="number">0.2</span>)</span><br><span class="line">wide_deep = WideDeep(wide, deep)</span><br></pre></td></tr></table></figure>
<p>看看 <code>embed_cols_info</code> ，存放格式 <code>(&#39;columns_name&#39;, &#39;input_dims&#39;, &#39;embedding_dims&#39;)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;userId&#x27;</span>, <span class="number">6041</span>, <span class="number">50</span>),</span><br><span class="line"> (<span class="string">&#x27;movieId&#x27;</span>, <span class="number">3707</span>, <span class="number">50</span>),</span><br><span class="line"> (<span class="string">&#x27;gender&#x27;</span>, <span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line"> (<span class="string">&#x27;age&#x27;</span>, <span class="number">8</span>, <span class="number">2</span>),</span><br><span class="line"> (<span class="string">&#x27;occupation&#x27;</span>, <span class="number">22</span>, <span class="number">5</span>),</span><br><span class="line"> (<span class="string">&#x27;zipCode&#x27;</span>, <span class="number">3440</span>, <span class="number">20</span>)]</span><br></pre></td></tr></table></figure>
<ul>
<li>userId onehot encoding 後會有 6041 維，但 embedding 後降到 100 維</li>
</ul>
<h2 id="Training-Stage"><a href="#Training-Stage" class="headerlink" title="Training Stage"></a>Training Stage</h2><p>訓練過程省略，請參閱 </p>
<p><a target="_blank" rel="noopener" href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Recommendation">seed9D/hands-on-machine-learning</a></p>
<h1 id="Online-Ranking"><a href="#Online-Ranking" class="headerlink" title="Online Ranking"></a>Online Ranking</h1><p>wide &amp; deep  在推薦系統中做為排序模型使用，對一批召回物品打分決定順序。 參閱  <a href="/realtime-recommendataion-system-architecture/" title="實時推薦策略流程">實時推薦策略流程</a></p>
<p>通常訓練模型的平台跟模型 serving 的平台是不同語言開發的，例如用 python / spark 訓練模型， serving  用 java ，造成我們很難把模型上線。尤其是複雜的深度模型，要上線 serving 需要有強大的工程支援。</p>
<p>常見的思路是把模型的 embedding 跟 multi layer perceptron 分離，embedding 存放在 DB 透過 index  取回，線上只需運算簡單的 MLP，有效減少線上計算量。</p>
<p>以 google 的 wide &amp; deep  為例紅框內為線上 inference 部分:</p>
<p><img src="https://i.imgur.com/2jHjzDW.png" alt="Wide%20&amp;%20Deep%20pytorch%20%E5%AF%A6%E7%8F%BE%20c1021d1986c94b39855460ce279d378d/Untitled%201.png"></p>
<p>線上 embedding 跟 feedforward network 分離的好處就是 embedding 的訓練可以很複雜，inference 時只需訓練完的 embedding  作為輸入，模型即可運作。</p>
<p>演示的關係，下面都用 python 試做 online ranking，但要應用在生產得用另外語言自行開發。</p>
<p>在 online serving 階段，我們會需要三個 component</p>
<ol>
<li>feature provider<ul>
<li>提供 user or item 的 特徵</li>
</ul>
</li>
<li>embedding provider<ul>
<li>提供訓練好的 id embedding，抽象成度夠高的話，算法工程師是不需要關心背後的儲存源的</li>
</ul>
</li>
<li>ranker<ul>
<li>對召回物品打分</li>
</ul>
</li>
</ol>
<h2 id="Feature-Provider"><a href="#Feature-Provider" class="headerlink" title="Feature Provider"></a>Feature Provider</h2><p>feature provider 負責提供所有模型所需的原始特徵，取回特定特徵後得自行加工符合模型 input</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureProvider</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, df: pd.DataFrame, index_col_name</span>):</span></span><br><span class="line">        self.df = df.copy()</span><br><span class="line">        self.index_col_name = index_col_name</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">query_features</span>(<span class="params">self, index: List[<span class="built_in">int</span>], cols: List[<span class="built_in">str</span>]</span>)-&gt; pd.DataFrame:</span></span><br><span class="line">        df = self.df</span><br><span class="line">        <span class="keyword">return</span> df[df[self.index_col_name].isin(index)][cols].copy()</span><br></pre></td></tr></table></figure>
<p>我們產生 user_feature_provider 與 item_feature_provider 分別提供 user features 和 item features</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">user_df = pd.read_csv(user_data_path, sep=<span class="string">&quot;::&quot;</span>, header=<span class="literal">None</span>, engine=<span class="string">&quot;python&quot;</span>, names=user_columns)</span><br><span class="line">user_df = user_df.reset_index().rename(columns=&#123;<span class="string">&#x27;index&#x27;</span>: <span class="string">&#x27;userId&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">item_df = pd.read_csv(movie_data_path, sep=<span class="string">&quot;::&quot;</span>, header=<span class="literal">None</span>, engine=<span class="string">&quot;python&quot;</span>, names=movie_columns)</span><br><span class="line">item_df = item_df.reset_index().rename(columns=&#123;<span class="string">&#x27;index&#x27;</span>: <span class="string">&#x27;movieId&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">user_feature_provider = FeatureProvider(user_df, <span class="string">&#x27;userId&#x27;</span>)</span><br><span class="line">item_feature_provider = FeatureProvider(item_df, <span class="string">&#x27;movieId&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>試試取回 userId = [5, 10] 的 age 跟 gender</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	user_feature_provider.query_features(index=[<span class="number">5</span>, <span class="number">10</span>], cols=[<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>])</span><br><span class="line">Out:</span><br><span class="line">		age	gender</span><br><span class="line">		<span class="number">25</span>	M</span><br><span class="line">		<span class="number">35</span>	F</span><br></pre></td></tr></table></figure>
<h2 id="Embedding-Provider"><a href="#Embedding-Provider" class="headerlink" title="Embedding Provider"></a>Embedding Provider</h2><p>embedding provider 提供已經訓練完的 id embedding</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingProvider</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dict: nn.ModuleDict, prefix=<span class="string">&#x27;emb_layer&#x27;</span></span>):</span></span><br><span class="line">        self.prefix= prefix</span><br><span class="line">        self.embedding_dict = embedding_dict</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">query_embedding</span>(<span class="params">self, batch_labels:np.array, label_order:List[<span class="built_in">str</span>]</span>)-&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># batch_label: [b_size, num of labels]</span></span><br><span class="line">        label_order = [self.prefix + <span class="string">&#x27;_&#x27;</span> + <span class="built_in">str</span>(label) <span class="keyword">for</span> label <span class="keyword">in</span> label_order]</span><br><span class="line">        </span><br><span class="line">        batch_labels = torch.from_numpy(batch_labels).long()</span><br><span class="line">        embed_X = [</span><br><span class="line">           self.embedding_dict[label](batch_labels[:, idx]) <span class="keyword">for</span> idx, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_order)</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(embed_X, <span class="number">1</span>) <span class="comment"># [b_size, sum of all embedding dims]</span></span><br></pre></td></tr></table></figure>
<p>我們將wide &amp;deep 訓練完的 embedding 取出放進去，這一步模擬線上 embedding provider 的初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embedding_provider = EmbeddingProvider(wide_deep.wide_deep[<span class="string">&#x27;deep&#x27;</span>][<span class="number">0</span>].embed_layers, <span class="string">&#x27;emb_layer&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>print 看看有哪些 id embedding可用 :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print(embedding_provider.embedding_dict)</span><br><span class="line">Out:</span><br><span class="line">	ModuleDict(</span><br><span class="line">  (emb_layer_age): Embedding(<span class="number">8</span>, <span class="number">2</span>)</span><br><span class="line">  (emb_layer_gender): Embedding(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">  (emb_layer_movieId): Embedding(<span class="number">3707</span>, <span class="number">50</span>)</span><br><span class="line">  (emb_layer_occupation): Embedding(<span class="number">22</span>, <span class="number">5</span>)</span><br><span class="line">  (emb_layer_userId): Embedding(<span class="number">6041</span>, <span class="number">50</span>)</span><br><span class="line">  (emb_layer_zipCode): Embedding(<span class="number">3440</span>, <span class="number">20</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li>有 age , gender movieId, occupation , userId, zipCode 的 embedding</li>
</ul>
<h2 id="Ranker"><a href="#Ranker" class="headerlink" title="Ranker"></a>Ranker</h2><p>ranker 對召回物品打分，在這裡我們的 ranker  實作 wide &amp; deep  inference，更嚴格來說是 wide &amp; deep 的  MLP 部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WideDeepOnlineRanker</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, offlineWideDeep: nn.Module</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.deep_dense = self._fetch_deep_dense(offlineWideDeep)</span><br><span class="line">        self.wide_part = offlineWideDeep.wide_deep[<span class="string">&#x27;wide&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fetch_deep_dense</span>(<span class="params">self, offlineWideDeep</span>):</span></span><br><span class="line">        deep_dense = []</span><br><span class="line">        <span class="keyword">for</span> dense_layer <span class="keyword">in</span> offlineWideDeep.wide_deep[<span class="string">&#x27;deep&#x27;</span>][<span class="number">0</span>].dense_layer:</span><br><span class="line">            new_layer = nn.Sequential(*[d <span class="keyword">for</span> d <span class="keyword">in</span> dense_layer <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(d, nn.Dropout)])</span><br><span class="line">            deep_dense.append(new_layer)</span><br><span class="line">        deep_dense.append(offlineWideDeep.wide_deep[<span class="string">&#x27;deep&#x27;</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*deep_dense)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, wide_x: torch.Tensor, deep_x_embedding: torch.Tensor</span>):</span></span><br><span class="line">        wide_output = self.wide_part(wide_x.long())</span><br><span class="line">        deep_output = self.deep_dense(deep_x_embedding)</span><br><span class="line">        <span class="keyword">return</span> (wide_output + deep_output).view(-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scoring</span>(<span class="params">self, wide_x: torch.Tensor, deep_x_embedding: torch.Tensor</span>):</span></span><br><span class="line">        self.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">return</span> self.forward(wide_x, deep_x_embedding).view(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_probs</span>(<span class="params">self, wide_x: torch.Tensor, deep_x_embedding: torch.Tensor</span>):</span></span><br><span class="line">        self.<span class="built_in">eval</span>()</span><br><span class="line">        out = self.forward(wide_x, deep_x_embedding)</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(out.view(-<span class="number">1</span>)).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure>
<ul>
<li>ranker 在生產環境中開發比較複雜的，演示關係這邊直接用 pytorch nn.Module 模擬 ranker</li>
<li>只需要知道 ranker 的負責打分即可，內部實現因人而異</li>
</ul>
<p>初始化 ranker，將離線訓練好的 wide &amp; deep 輸入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ranker = WideDeepOnlineRanker(wide_deep)</span><br></pre></td></tr></table></figure>
<h2 id="Recommendation-Process"><a href="#Recommendation-Process" class="headerlink" title="Recommendation Process"></a>Recommendation Process</h2><p>有了 feature provider, embedding provider, ranker 三大 component，我們就可以對召回物品打分排序了</p>
<p>模擬 user_id=100 與 30個召回商品：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	user_id = <span class="number">100</span></span><br><span class="line">	match_itemIds = np.random.choice(item_df[<span class="string">&#x27;movieId&#x27;</span>], <span class="number">30</span>)</span><br><span class="line">	print(match_itemIds)</span><br><span class="line">Out:</span><br><span class="line">	array([<span class="number">1866</span>, <span class="number">2421</span>, <span class="number">2869</span>, <span class="number">1217</span>, <span class="number">3535</span>, <span class="number">2535</span>, <span class="number">1542</span>, <span class="number">3569</span>, <span class="number">1884</span>,  <span class="number">644</span>, <span class="number">1784</span>,</span><br><span class="line">       <span class="number">3506</span>, <span class="number">3907</span>, <span class="number">1522</span>,  <span class="number">803</span>, <span class="number">3148</span>, <span class="number">1715</span>, <span class="number">2055</span>, <span class="number">3442</span>,  <span class="number">296</span>, <span class="number">3588</span>, <span class="number">3871</span>,</span><br><span class="line">       <span class="number">1785</span>,  <span class="number">628</span>, <span class="number">1527</span>, <span class="number">3401</span>, <span class="number">1921</span>, <span class="number">3449</span>,  <span class="number">438</span>,  <span class="number">842</span>])</span><br></pre></td></tr></table></figure>
<p>分別取回  user_id, 與 match_itemIds 模型所需特徵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">query_user_features_cols = [<span class="string">&#x27;userId&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;zipCode&#x27;</span>]</span><br><span class="line">query_item_features_cols = [<span class="string">&#x27;movieId&#x27;</span>]</span><br><span class="line"></span><br><span class="line">user_primitive_feature = user_feature_provider.query_features([user_id], query_user_features_cols)</span><br><span class="line">item_primitive_feature = item_feature_provider.query_features(<span class="built_in">list</span>(match_itemIds), query_item_features_cols)</span><br></pre></td></tr></table></figure>
<p>print 數個 item_primitive_feature 看看</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">	  movieId</span><br><span class="line"><span class="number">293</span>	    <span class="number">296</span></span><br><span class="line"><span class="number">434</span>	    <span class="number">438</span></span><br><span class="line"><span class="number">623</span>	    <span class="number">628</span></span><br><span class="line"><span class="number">639</span>	    <span class="number">644</span></span><br><span class="line"><span class="number">793</span>	    <span class="number">803</span></span><br><span class="line"><span class="number">831</span>	    <span class="number">842</span></span><br><span class="line"><span class="number">1199</span>	  <span class="number">1217</span></span><br><span class="line"><span class="number">1486</span>	  <span class="number">1522</span></span><br><span class="line"><span class="number">1491</span>	  <span class="number">1527</span></span><br><span class="line"><span class="number">1503</span>	  <span class="number">1542</span></span><br></pre></td></tr></table></figure>
<ul>
<li>第一個 columns 是 dataframe 本身的 index…</li>
<li>可以看到  item 相關特徵只有用到 movieId ….，事實上 movieLen 中還有 title, genre 可以用<ul>
<li>title 特徵可以先過 word2vector 做 title embedding</li>
<li>genre 是 multi-label 的特徵，本質上是 multi-hot encoding ，涉及不定長度的 tensor input，處理起來稍微麻煩。 multi-label 的特徵在推薦系統中還滿常見的，演示的關係就 let it go 了</li>
</ul>
</li>
</ul>
<p>接下來要將原始特徵轉換成 wide &amp; deep 的 input，線上線下的特徵處理務求一致。演示關係，這邊假設我們已經 implement 好 wide feature generator 和 deep feature generator</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">user_primitive_feature[<span class="string">&#x27;join&#x27;</span>] = <span class="number">1</span></span><br><span class="line">item_primitive_feature[<span class="string">&#x27;join&#x27;</span>] = <span class="number">1</span></span><br><span class="line">primitive_features = user_primitive_feature.merge(item_primitive_feature, on = [<span class="string">&#x27;join&#x27;</span>]).drop(<span class="string">&#x27;join&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">processed_wide_features = wideGenerator.transform(primitive_features)</span><br><span class="line">processed_deep_features = deep_generator.transform(primitive_features)</span><br><span class="line"></span><br><span class="line">processed_wide_features = torch.from_numpy(processed_wide_features).long()</span><br></pre></td></tr></table></figure>
<p>print 看看 processed_wide_features 和 processed_deep_features 的維度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	processed_wide_features.shape, processed_deep_features.shape</span><br><span class="line">Out:</span><br><span class="line">	((<span class="number">30</span>, <span class="number">7</span>), (<span class="number">30</span>, <span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>wide 特徵用到 7 個; deep 特徵用到 6 個</li>
</ul>
<p>processed_deep_features 的 6 個特徵都是 embedding  特徵，我們透過 embedding provider 取回各自的 embedding</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">embedding_label_order = [<span class="string">&#x27;userId&#x27;</span>, <span class="string">&#x27;movieId&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;zipCode&#x27;</span>]</span><br><span class="line">embedding_features = embedding_provider.query_embedding(processed_deep_features, embedding_label_order)</span><br></pre></td></tr></table></figure>
<p>print embedding_features dimension</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	print(embedding_features.shape)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">	torch.Size([<span class="number">30</span>, <span class="number">129</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>embedding  provider 內部還貼心 concatenate 好了。<br>P.S. concatenate 功能應該要拆分出來，這不屬於 embedding provider 的職責</li>
</ul>
<p>經過前面一大段特徵處理，終於可以送進  ranker 打分了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores = ranker.predict_probs(processed_wide_features, embedding_features)</span><br></pre></td></tr></table></figure>
<p>將 score  由高到低排序並 print 出對應的 movieId 和 movie title</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">In:</span><br><span class="line">	sorted_index = np.argsort(scores.detach().numpy(), axis=<span class="number">0</span>)[::-<span class="number">1</span>]</span><br><span class="line">	<span class="keyword">for</span> idx <span class="keyword">in</span> sorted_index:</span><br><span class="line">    movieId = match_itemIds[idx]</span><br><span class="line">    title = item_feature_provider.query_features([movieId], [<span class="string">&#x27;title&#x27;</span>])[<span class="string">&#x27;title&#x27;</span>].item()</span><br><span class="line">    print(<span class="string">&quot;movieId:&#123;&#125; \t &#x27;&#123;&#125;&#x27; \t score:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(movieId, title,  <span class="built_in">round</span>(scores[idx].item(), <span class="number">3</span>)))</span><br><span class="line">Out:</span><br><span class="line">	movieId:<span class="number">1866</span> 	 <span class="string">&#x27;Big Hit, The (1998)&#x27;</span> 	 score:<span class="number">0.77</span></span><br><span class="line">	movieId:<span class="number">1542</span> 	 <span class="string">&#x27;Brassed Off (1996)&#x27;</span> 	 score:<span class="number">0.702</span></span><br><span class="line">	movieId:<span class="number">2869</span> 	 <span class="string">&#x27;Separation, The (La S�paration) (1994)&#x27;</span> 	 score:<span class="number">0.553</span></span><br><span class="line">	movieId:<span class="number">3506</span> 	 <span class="string">&#x27;North Dallas Forty (1979)&#x27;</span> 	 score:<span class="number">0.546</span></span><br><span class="line">	movieId:<span class="number">3588</span> 	 <span class="string">&#x27;King of Marvin Gardens, The (1972)&#x27;</span> 	 score:<span class="number">0.473</span></span><br><span class="line">	movieId:<span class="number">438</span> 	 <span class="string">&#x27;Cowboy Way, The (1994)&#x27;</span> 	 score:<span class="number">0.456</span></span><br><span class="line">	movieId:<span class="number">1921</span> 	 <span class="string">&#x27;Pi (1998)&#x27;</span> 	 score:<span class="number">0.453</span></span><br><span class="line">	movieId:<span class="number">3148</span> 	 <span class="string">&#x27;Cider House Rules, The (1999)&#x27;</span> 	 score:<span class="number">0.385</span></span><br><span class="line">	movieId:<span class="number">1527</span> 	 <span class="string">&#x27;Fifth Element, The (1997)&#x27;</span> 	 score:<span class="number">0.367</span></span><br><span class="line">	movieId:<span class="number">803</span> 	 <span class="string">&#x27;Walking and Talking (1996)&#x27;</span> 	 score:<span class="number">0.34</span></span><br><span class="line">	movieId:<span class="number">1884</span> 	 <span class="string">&#x27;Fear and Loathing in Las Vegas (1998)&#x27;</span> 	 score:<span class="number">0.315</span></span><br><span class="line">	movieId:<span class="number">3535</span> 	 <span class="string">&#x27;American Psycho (2000)&#x27;</span> 	 score:<span class="number">0.315</span></span><br><span class="line">	movieId:<span class="number">296</span> 	 <span class="string">&#x27;Pulp Fiction (1994)&#x27;</span> 	 score:<span class="number">0.313</span></span><br><span class="line">	movieId:<span class="number">842</span> 	 <span class="string">&#x27;Tales from the Crypt Presents: Bordello of Blood (1996)&#x27;</span> 	 score:<span class="number">0.289</span></span><br><span class="line">	movieId:<span class="number">3401</span> 	 <span class="string">&#x27;Baby... Secret of the Lost Legend (1985)&#x27;</span> 	 score:<span class="number">0.286</span></span><br><span class="line">	movieId:<span class="number">644</span> 	 <span class="string">&#x27;Happy Weekend (1996)&#x27;</span> 	 score:<span class="number">0.268</span></span><br><span class="line">	movieId:<span class="number">3907</span> 	 <span class="string">&#x27;Prince of Central Park, The (1999)&#x27;</span> 	 score:<span class="number">0.259</span></span><br><span class="line">	movieId:<span class="number">3449</span> 	 <span class="string">&#x27;Good Mother, The (1988)&#x27;</span> 	 score:<span class="number">0.226</span></span><br><span class="line">	movieId:<span class="number">1784</span> 	 <span class="string">&#x27;As Good As It Gets (1997)&#x27;</span> 	 score:<span class="number">0.162</span></span><br><span class="line">	movieId:<span class="number">1522</span> 	 <span class="string">&#x27;Ripe (1996)&#x27;</span> 	 score:<span class="number">0.115</span></span><br><span class="line">	movieId:<span class="number">3569</span> 	 <span class="string">&#x27;Idiots, The (Idioterne) (1998)&#x27;</span> 	 score:<span class="number">0.09</span></span><br><span class="line">	movieId:<span class="number">628</span> 	 <span class="string">&#x27;Primal Fear (1996)&#x27;</span> 	 score:<span class="number">0.09</span></span><br><span class="line">	movieId:<span class="number">2535</span> 	 <span class="string">&#x27;Earthquake (1974)&#x27;</span> 	 score:<span class="number">0.072</span></span><br><span class="line">	movieId:<span class="number">1217</span> 	 <span class="string">&#x27;Ran (1985)&#x27;</span> 	 score:<span class="number">0.07</span></span><br><span class="line">	movieId:<span class="number">3442</span> 	 <span class="string">&#x27;Band of the Hand (1986)&#x27;</span> 	 score:<span class="number">0.063</span></span><br><span class="line">	movieId:<span class="number">2421</span> 	 <span class="string">&#x27;Karate Kid, Part II, The (1986)&#x27;</span> 	 score:<span class="number">0.063</span></span><br><span class="line">	movieId:<span class="number">2055</span> 	 <span class="string">&#x27;Hot Lead and Cold Feet (1978)&#x27;</span> 	 score:<span class="number">0.062</span></span><br><span class="line">	movieId:<span class="number">1715</span> 	 <span class="string">&#x27;Office Killer (1997)&#x27;</span> 	 score:<span class="number">0.055</span></span><br><span class="line">	movieId:<span class="number">1785</span> 	 <span class="string">&#x27;King of New York (1990)&#x27;</span> 	 score:<span class="number">0.034</span></span><br><span class="line">	movieId:<span class="number">3871</span> 	 <span class="string">&#x27;Shane (1953)&#x27;</span> 	 score:<span class="number">0.02</span></span><br></pre></td></tr></table></figure>
<p>到此，我們完成了 online ranking 流程</p>
<h1 id="Last-but-not-Least"><a href="#Last-but-not-Least" class="headerlink" title="Last but not Least"></a>Last but not Least</h1><ul>
<li>在 google 論文中為了讓樣本的 features 非常稀疏， optimizer 採用 FTRL with L1 regularization</li>
<li>此篇實作中的 features 展開來也才一萬出頭，所以使用 FTRL 優化效果不好。</li>
</ul>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li>Karatzoglou, A., &amp; Hidasi, B. (2017). Deep learning for recommender systems. RecSys 2017 - Proceedings of the 11th ACM Conference on Recommender Systems, 396–397. <a target="_blank" rel="noopener" href="https://doi.org/10.1145/3109859.3109933">https://doi.org/10.1145/3109859.3109933</a></li>
<li>详解 Wide &amp; Deep 结构背后的动机 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53361519">https://zhuanlan.zhihu.com/p/53361519</a></li>
<li>TensorFlow Wide &amp; Deep Learning Tutorial <a target="_blank" rel="noopener" href="https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/tutorials/wide_and_deep/index.md">https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/tutorials/wide_and_deep/index.md</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jrzaurin/pytorch-widedeep">https://github.com/jrzaurin/pytorch-widedeep</a></li>
<li>wide &amp; deep jupyter notebook <a target="_blank" rel="noopener" href="https://github.com/seed9D/hands-on-machine-learning/tree/main/Recommendation">seed9D/hands-on-machine-learning</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>實作 wide &amp; deep 從訓練到推薦排序</p><p><a href="https://seed9d.github.io/wide-deep-in-recommendation/">https://seed9d.github.io/wide-deep-in-recommendation/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>seed9D</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-02-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-02-20</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/recommendation-system/">recommendation system, </a><a class="link-muted" rel="tag" href="/tags/pytorch/">pytorch, </a><a class="link-muted" rel="tag" href="/tags/%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1/">推薦系統, </a><a class="link-muted" rel="tag" href="/tags/deep-learning/">deep learning, </a><a class="link-muted" rel="tag" href="/tags/ranking/">ranking </a></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/realtime-recommendataion-system-architecture/"><span class="level-item">實時推薦策略流程</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.6.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "ceba2f041af2fc272391c2f27194957e",
            repo: "seed9D.github.io",
            owner: "seed9D",
            clientID: "eb1cbacea1411b9a4729",
            clientSecret: "13dcde9fed4b916ec81748c4c29e8047dc4861e7",
            admin: ["seed9D"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Memorization-amp-Generalization"><span class="level-left"><span class="level-item">Memorization &amp; Generalization</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Wide"><span class="level-left"><span class="level-item">Wide</span></span></a></li><li><a class="level is-mobile" href="#Deep"><span class="level-left"><span class="level-item">Deep</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#如何處理-sparse-id-features"><span class="level-left"><span class="level-item">如何處理 sparse id features?</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Implement-Wide-amp-Deep-by-Pytorch"><span class="level-left"><span class="level-item">Implement Wide &amp; Deep by Pytorch</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#wide-特徵處理"><span class="level-left"><span class="level-item">wide 特徵處理</span></span></a></li><li><a class="level is-mobile" href="#deep-特徵處理"><span class="level-left"><span class="level-item">deep 特徵處理</span></span></a></li><li><a class="level is-mobile" href="#Wide-amp-Deep-Model-Graph"><span class="level-left"><span class="level-item">Ｗide &amp; Deep Model Graph</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Wide-Model"><span class="level-left"><span class="level-item">Wide Model</span></span></a></li><li><a class="level is-mobile" href="#Deep-Model"><span class="level-left"><span class="level-item">Deep Model</span></span></a></li><li><a class="level is-mobile" href="#Wide-amp-Deep"><span class="level-left"><span class="level-item">Wide &amp; Deep</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Building-Model"><span class="level-left"><span class="level-item">Building Model</span></span></a></li><li><a class="level is-mobile" href="#Training-Stage"><span class="level-left"><span class="level-item">Training Stage</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Online-Ranking"><span class="level-left"><span class="level-item">Online Ranking</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Feature-Provider"><span class="level-left"><span class="level-item">Feature Provider</span></span></a></li><li><a class="level is-mobile" href="#Embedding-Provider"><span class="level-left"><span class="level-item">Embedding Provider</span></span></a></li><li><a class="level is-mobile" href="#Ranker"><span class="level-left"><span class="level-item">Ranker</span></span></a></li><li><a class="level is-mobile" href="#Recommendation-Process"><span class="level-left"><span class="level-item">Recommendation Process</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Last-but-not-Least"><span class="level-left"><span class="level-item">Last but not Least</span></span></a></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">Reference</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-20T15:29:06.000Z">2021-02-20</time></p><p class="title"><a href="/wide-deep-in-recommendation/">實作 wide &amp; deep 從訓練到推薦排序</a></p><p class="categories"><a href="/categories/recommendation-system/">recommendation system</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-19T15:52:12.000Z">2021-02-19</time></p><p class="title"><a href="/realtime-recommendataion-system-architecture/">實時推薦策略流程</a></p><p class="categories"><a href="/categories/recommendation-system/">recommendation system</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-18T16:00:42.000Z">2021-02-19</time></p><p class="title"><a href="/pratical-FM-model/">推薦系統中的瑞士刀 Factorization Machine</a></p><p class="categories"><a href="/categories/recommendation-system/">recommendation system</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-17T15:20:35.000Z">2021-02-17</time></p><p class="title"><a href="/what-make-XGBoost-so-effective/">透視 XGBoost(0) 總結篇</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-17T02:00:16.000Z">2021-02-17</time></p><p class="title"><a href="/XGBoost-cool-optimization/">透視 XGBoost(4) 神奇 optimization 在哪裡？</a></p><p class="categories"><a href="/categories/Machine-Learning/">Machine Learning</a></p></div></article></div></div><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/images/GG.jpg" alt="seed9D"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">seed9D</p><p class="is-size-6 is-block">這一生志願平凡快樂</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>左岸</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">8</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/seed9D" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/seed9D"><i class="fab fa-github"></i></a></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/logo.svg" alt="seed9D&#039;s blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 seed9D</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener external nofollow">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener external nofollow">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>